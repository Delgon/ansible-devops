{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MAS Devops Ansible Collection \uf0c1 MAS Compatability \uf0c1 The collection supports IBM Maximo Application Suite version 8.6 and above. Usage \uf0c1 Run a playbook \uf0c1 The collection includes a number of playbooks that string together multiple roles, you can directly invoke them after installing the collection: ansible-playbook ibm.mas_devops.lite_core_roks Run a role \uf0c1 If you only want to perform a single action, you can directly invoke one of our roles from the command line without the need to build a playbook: ansible localhost -m include_role -a name=ibm.mas_devops.ocp_verify You can also use the run_role playbook: ROLE_NAME=cert_manager ansible-playbook ibm.mas_devops.run_role Running in Docker \uf0c1 The easiest way to use this collection is to take advantage of the ibmmas/cli container image, this negates the need to install anything on your local machine (other than docker - or podman if you prefer). docker run -ti --rm quay.io/ibmmas/cli Local Install \uf0c1 Install Python & Ansible \uf0c1 Python 3.9 is recommended as it is the most widely used version of Python within our development team, but any in-support 3.x version of Python should work fine. python3 --version python3 -m pip install ansible junit_xml pymongo xmljson jmespath kubernetes==12.0.1 openshift==0.12.1 ansible --version ansible-playbook --version We recommend using the latest version of ansible-core at all times (at time of writing this was v2.12.3) and the collection has a minimum supported version of ansible-core v2.10.3 which is enforced by the ibm.mas_devops.ansible_version_check role. Install OpenShift CLI \uf0c1 If you do not already have the oc command line tool, you can download it as below: wget -q https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/4.8.35/openshift-client-linux.tar.gz tar -zxf openshift-client-linux.tar.gz mv oc kubectl /usr/local/bin/ rm -rf openshift-client-linux.tar.gz oc version Install IBM Cloud CLI \uf0c1 If you are using this collection to manage an OpenShift cluster in IBM Cloud RedHat OpenShift Kubernetes Service (ROKS), then you must also install the IBM Cloud CLI: curl -sL https://raw.githubusercontent.com/IBM-Cloud/ibm-cloud-developer-tools/master/linux-installer/idt-installer | bash ibmcloud version` Install the Ansible Collection \uf0c1 Install the collection direct from Ansible Galaxy ansible-galaxy collection install ibm.mas_devops Optionally, you can also pin the version of the collection that you install, allowing you to control exactly what version of the collection is in use in your automation: ansible-galaxy collection install ibm.mas_devops:10.6.2 Support \uf0c1 This Ansible collection is developed by the IBM Maximo Application Suite development team, customers may raise support tickets via the same routes they would an issue with the product itself, or raise an issue directly in the GitHub repository .","title":"Home"},{"location":"#mas-devops-ansible-collection","text":"","title":"MAS Devops Ansible Collection"},{"location":"#mas-compatability","text":"The collection supports IBM Maximo Application Suite version 8.6 and above.","title":"MAS Compatability"},{"location":"#usage","text":"","title":"Usage"},{"location":"#run-a-playbook","text":"The collection includes a number of playbooks that string together multiple roles, you can directly invoke them after installing the collection: ansible-playbook ibm.mas_devops.lite_core_roks","title":"Run a playbook"},{"location":"#run-a-role","text":"If you only want to perform a single action, you can directly invoke one of our roles from the command line without the need to build a playbook: ansible localhost -m include_role -a name=ibm.mas_devops.ocp_verify You can also use the run_role playbook: ROLE_NAME=cert_manager ansible-playbook ibm.mas_devops.run_role","title":"Run a role"},{"location":"#running-in-docker","text":"The easiest way to use this collection is to take advantage of the ibmmas/cli container image, this negates the need to install anything on your local machine (other than docker - or podman if you prefer). docker run -ti --rm quay.io/ibmmas/cli","title":"Running in Docker"},{"location":"#local-install","text":"","title":"Local Install"},{"location":"#install-python-ansible","text":"Python 3.9 is recommended as it is the most widely used version of Python within our development team, but any in-support 3.x version of Python should work fine. python3 --version python3 -m pip install ansible junit_xml pymongo xmljson jmespath kubernetes==12.0.1 openshift==0.12.1 ansible --version ansible-playbook --version We recommend using the latest version of ansible-core at all times (at time of writing this was v2.12.3) and the collection has a minimum supported version of ansible-core v2.10.3 which is enforced by the ibm.mas_devops.ansible_version_check role.","title":"Install Python &amp; Ansible"},{"location":"#install-openshift-cli","text":"If you do not already have the oc command line tool, you can download it as below: wget -q https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/4.8.35/openshift-client-linux.tar.gz tar -zxf openshift-client-linux.tar.gz mv oc kubectl /usr/local/bin/ rm -rf openshift-client-linux.tar.gz oc version","title":"Install OpenShift CLI"},{"location":"#install-ibm-cloud-cli","text":"If you are using this collection to manage an OpenShift cluster in IBM Cloud RedHat OpenShift Kubernetes Service (ROKS), then you must also install the IBM Cloud CLI: curl -sL https://raw.githubusercontent.com/IBM-Cloud/ibm-cloud-developer-tools/master/linux-installer/idt-installer | bash ibmcloud version`","title":"Install IBM Cloud CLI"},{"location":"#install-the-ansible-collection","text":"Install the collection direct from Ansible Galaxy ansible-galaxy collection install ibm.mas_devops Optionally, you can also pin the version of the collection that you install, allowing you to control exactly what version of the collection is in use in your automation: ansible-galaxy collection install ibm.mas_devops:10.6.2","title":"Install the Ansible Collection"},{"location":"#support","text":"This Ansible collection is developed by the IBM Maximo Application Suite development team, customers may raise support tickets via the same routes they would an issue with the product itself, or raise an issue directly in the GitHub repository .","title":"Support"},{"location":"changes/","text":"Changes \uf0c1 Note that links to pull requests prior to public release of the code (4.0) direct to IBM GitHub Enterprise, and will only be accessible to IBM employees. 11.4 Multiple Updates: Add support for Sep 2022 catalog ( #476 ) Support CloudPak for Data 4.5.x install/upgrade ( #466 ) Add One-click install playbook for Predict ( #454 ) Support Manage attachments and BIM configuration ( #420 ) 11.3 Multiple Updates: AMQ Streams Kafka enhancements ( #451 ) Configure EFS as RWX Storage Class on ROSA ( #432 ) Add Oneclick MVI Playbook ( #434 ) Add Oneclick optimizer Playbook ( #456 ) 11.2 Add Oneclick Update and Upgrade Playbooks ( #444 ) 11.1 Multiple Updates: Catalog update for IoT airgap support fix ( #435 ) Add Single Node OpenShift on AWS support ( #440 ) Add support for OCP IPI installation on AWS ( #439 ) 11.0 Multiple Updates: Support for MAS 8.8 & Maximo Curated Operator Catalog ( #413 ) Support OCP Power on DevIT FYRE ( #423 ) 10.6 Analytics Project configuration in HPUtilities ( #367 ) 10.5 Support use of product_group quota in FYRE ( #373 ) 10.4 Multiple Updates: Support OCP on ROSA ( #360 ) Support Watson Discovery Instance Creation ( #359 ) 10.3 Multiple Updates: Add support for Cloudflare DNS integration ( #349 ) Support creation of Db2 LDAP user ( #330 ) 10.2 Deploy Grafana in cluster_monitoring role ( #336 ) 10.1 Multiple Updates: Update suite application roles to support Optimizer application ( #309 ) Support AIO configuration in Manage ( #316 ) Consolidate entitlement management ( #323 ) Add support for source & channel selection to mas-devops-sls Tekton task ( #324 ) 10.0 One-click installer support ( #285 , #296 , #299 ) 9.0 Multiple Updates: Added ability to set annotations onto suite CR ( #269 ) Add Assist install playbooks ( #271 ) Added gencfg-sls and gencfg-uds playbooks for using existing SLS and UDS ( #275 ) Add new required var CPD_METADB_BLOCK_STORAGE_CLASS for CP4D 4.0 ( #273 ) 8.0 Multiple Updates: SMTP email support for Azure and other changes ( #265 ) Configure azurefiles storage class for multi-cloud ( #260 ) Create secure route to CP4D web client when setting up DNS using CIS and suite_dns role ( #251 ) Add cp4d_wds role for discovery instance provison ( #234 ) Change MAS_APPWS_COMPONENTS variable format ( #267 ) 7.0 Mulitple Updates: Consolidate cluster specific provision/deprovision playbooks ( #236 ) Fix SLS bootstrap and update docs ( #242 ) Support GPU node in OCP on ROKS ( #224 ) Azure support for OCP cluster deployment ( #243 ) Add UDSCfg (BASCfg) generator ( #210 ) 6.5 Multiple Updates: Add JDBCCfg generator role ( #188 ) Add mustgather clusterTask to pipeline and new mustgather_download playbook ( #222 ) 6.4 Add support for MVI deployment ( #196 ) 6.3 Allow ocp_server and ocp_token to be used for ocp_login ( #211 ) 6.2 Multiple Updates: Support manual upgrade approvals ( #205 ) Add support for Db2u operator ( #203 ) Add Workspace config generator ( #189 ) 6.1 Create WSL project and enable HPU deploy ( #201 ) 6.0 Multiple Updates: Upgrade to kubernetes.core Ansible module ( #194 ) Remove BAS support (replaced by UDS) ( #194 ) 5.3 Multiple Updates: Add support for db2wh backup & restore ( #133 ) Add support for appConnect ( #170 ) Switch BAS from FullDeployment to AnalyticsProxy ( #178 ) 5.2 Multiple Updates: Support MongoDb CPU and memory configuration ( #158 ) Separate CIS_APIKEY support for MAS Installation ( #156 ) Support configurable prometheus storage & retention policy ( #151 ) Support configurable application spec ( #160 ) 5.1 Multiple Updates: Add support for Cloud Object Storage setup ( #122 ) Conditional application deployment in Tekton pipelines ( #118 ) Add support for CP4D v4 alongside existing support for v3.5 ( #93 ) 5.0 Multiple Updates: Add support for AI Applications' must-gather tooling ( #91 ) Migrate airgap support into ibm.mas_airgap collection ( #38 ) Support for Assist application ( #76 ) Significant refactoring for CP4D support ( #68 ) Migrate build system to GitHub Actions ( #68 ) 4.5 Add support for Manage ( #61 ) 4.4 Add CP4D and DB2W playbooks ( #51 ) 4.3 Add support for playbook junit result generation ( #39 ) 4.2 Add support for Tekton pipelines ( #34 ) 4.1 Add ocp_verify role and associated playbook ( #20 ) 4.0 Initial Public Release on ibm.mas_devops ( #5 ) 3.3 Support configurable SLS settings ( #53 ) 3.2 Add support for BAS ( #44 ) 3.1 Add support for SLS ( #35 ) 3.0 Switch to config dir instead of config file list ( #36 ) 2.7 Support AirGap install of MAS ( #28 ) 2.6 Add support for Gen2 application mgmt (install and configure) ( #24 ) 2.5 Add support for Watson Studio ( #16 ) 2.4 Add support for MongoDb Community Edition ( #25 ) 2.3 Add support for IBM Cloud resource groups ( #20 ) 2.2 Support DNS and certificate mgmt with CIS & LetsEncrypt ( #10 ) 2.1 Add support for AMQ Streams (Kafka) ( #19 ) 2.0 Major refactor of the roles and playbooks ( #17 ) 1.2 Add initial Spark support (incomplete) ( #15 ) 1.1 Enable db2wh SSL and generate jdbccfg for MAS ( #9 ) 1.0 Initial release","title":"Changes"},{"location":"changes/#changes","text":"Note that links to pull requests prior to public release of the code (4.0) direct to IBM GitHub Enterprise, and will only be accessible to IBM employees. 11.4 Multiple Updates: Add support for Sep 2022 catalog ( #476 ) Support CloudPak for Data 4.5.x install/upgrade ( #466 ) Add One-click install playbook for Predict ( #454 ) Support Manage attachments and BIM configuration ( #420 ) 11.3 Multiple Updates: AMQ Streams Kafka enhancements ( #451 ) Configure EFS as RWX Storage Class on ROSA ( #432 ) Add Oneclick MVI Playbook ( #434 ) Add Oneclick optimizer Playbook ( #456 ) 11.2 Add Oneclick Update and Upgrade Playbooks ( #444 ) 11.1 Multiple Updates: Catalog update for IoT airgap support fix ( #435 ) Add Single Node OpenShift on AWS support ( #440 ) Add support for OCP IPI installation on AWS ( #439 ) 11.0 Multiple Updates: Support for MAS 8.8 & Maximo Curated Operator Catalog ( #413 ) Support OCP Power on DevIT FYRE ( #423 ) 10.6 Analytics Project configuration in HPUtilities ( #367 ) 10.5 Support use of product_group quota in FYRE ( #373 ) 10.4 Multiple Updates: Support OCP on ROSA ( #360 ) Support Watson Discovery Instance Creation ( #359 ) 10.3 Multiple Updates: Add support for Cloudflare DNS integration ( #349 ) Support creation of Db2 LDAP user ( #330 ) 10.2 Deploy Grafana in cluster_monitoring role ( #336 ) 10.1 Multiple Updates: Update suite application roles to support Optimizer application ( #309 ) Support AIO configuration in Manage ( #316 ) Consolidate entitlement management ( #323 ) Add support for source & channel selection to mas-devops-sls Tekton task ( #324 ) 10.0 One-click installer support ( #285 , #296 , #299 ) 9.0 Multiple Updates: Added ability to set annotations onto suite CR ( #269 ) Add Assist install playbooks ( #271 ) Added gencfg-sls and gencfg-uds playbooks for using existing SLS and UDS ( #275 ) Add new required var CPD_METADB_BLOCK_STORAGE_CLASS for CP4D 4.0 ( #273 ) 8.0 Multiple Updates: SMTP email support for Azure and other changes ( #265 ) Configure azurefiles storage class for multi-cloud ( #260 ) Create secure route to CP4D web client when setting up DNS using CIS and suite_dns role ( #251 ) Add cp4d_wds role for discovery instance provison ( #234 ) Change MAS_APPWS_COMPONENTS variable format ( #267 ) 7.0 Mulitple Updates: Consolidate cluster specific provision/deprovision playbooks ( #236 ) Fix SLS bootstrap and update docs ( #242 ) Support GPU node in OCP on ROKS ( #224 ) Azure support for OCP cluster deployment ( #243 ) Add UDSCfg (BASCfg) generator ( #210 ) 6.5 Multiple Updates: Add JDBCCfg generator role ( #188 ) Add mustgather clusterTask to pipeline and new mustgather_download playbook ( #222 ) 6.4 Add support for MVI deployment ( #196 ) 6.3 Allow ocp_server and ocp_token to be used for ocp_login ( #211 ) 6.2 Multiple Updates: Support manual upgrade approvals ( #205 ) Add support for Db2u operator ( #203 ) Add Workspace config generator ( #189 ) 6.1 Create WSL project and enable HPU deploy ( #201 ) 6.0 Multiple Updates: Upgrade to kubernetes.core Ansible module ( #194 ) Remove BAS support (replaced by UDS) ( #194 ) 5.3 Multiple Updates: Add support for db2wh backup & restore ( #133 ) Add support for appConnect ( #170 ) Switch BAS from FullDeployment to AnalyticsProxy ( #178 ) 5.2 Multiple Updates: Support MongoDb CPU and memory configuration ( #158 ) Separate CIS_APIKEY support for MAS Installation ( #156 ) Support configurable prometheus storage & retention policy ( #151 ) Support configurable application spec ( #160 ) 5.1 Multiple Updates: Add support for Cloud Object Storage setup ( #122 ) Conditional application deployment in Tekton pipelines ( #118 ) Add support for CP4D v4 alongside existing support for v3.5 ( #93 ) 5.0 Multiple Updates: Add support for AI Applications' must-gather tooling ( #91 ) Migrate airgap support into ibm.mas_airgap collection ( #38 ) Support for Assist application ( #76 ) Significant refactoring for CP4D support ( #68 ) Migrate build system to GitHub Actions ( #68 ) 4.5 Add support for Manage ( #61 ) 4.4 Add CP4D and DB2W playbooks ( #51 ) 4.3 Add support for playbook junit result generation ( #39 ) 4.2 Add support for Tekton pipelines ( #34 ) 4.1 Add ocp_verify role and associated playbook ( #20 ) 4.0 Initial Public Release on ibm.mas_devops ( #5 ) 3.3 Support configurable SLS settings ( #53 ) 3.2 Add support for BAS ( #44 ) 3.1 Add support for SLS ( #35 ) 3.0 Switch to config dir instead of config file list ( #36 ) 2.7 Support AirGap install of MAS ( #28 ) 2.6 Add support for Gen2 application mgmt (install and configure) ( #24 ) 2.5 Add support for Watson Studio ( #16 ) 2.4 Add support for MongoDb Community Edition ( #25 ) 2.3 Add support for IBM Cloud resource groups ( #20 ) 2.2 Support DNS and certificate mgmt with CIS & LetsEncrypt ( #10 ) 2.1 Add support for AMQ Streams (Kafka) ( #19 ) 2.0 Major refactor of the roles and playbooks ( #17 ) 1.2 Add initial Spark support (incomplete) ( #15 ) 1.1 Enable db2wh SSL and generate jdbccfg for MAS ( #9 ) 1.0 Initial release","title":"Changes"},{"location":"playbooks/ocp/","text":"OCP Playbooks \uf0c1 Provision \uf0c1 Refer to the ocp_provision role documentation for more information. Provision on AWS ROSA \uf0c1 This playbook uses your ROSA API Token to provision a brand new OCP cluster, provision an instance of EFS and set up the cluster with a ReadWriteMany storage class named efs utilizing that instance. To obtain your API token login to the OpenShift cluster manager . export AWS_ACCESS_KEY_ID=xxx export AWS_SECRET_ACCESS_KEY=xxx export ROSA_TOKEN=xxx export CLUSTER_NAME=masonrosa export OCP_VERSION=4.10 export ROSA_COMPUTE_NODES=5 export ROSA_CLUSTER_ADMIN_PASSWORD=xxx ansible-playbook ibm.mas_devops.ocp_rosa_provision Provision on IBMCloud ROKS \uf0c1 This playbook uses your IBMCloud API key to provision a brand new OCP cluster. The playbook supports installing an IBM entitlement key as a cluster-wide image pull secret and reboot all worker nodes, which is required for IBM Cloud Pak for Data v4; this can be enabled by setting REBOOT_WORKER_NODES to true and providing the entitlement key with CPD_ENTITLEMENT_KEY . This also supports upgrading the storage volume used for the cluster's internal image registry from 100Gb to 400Gb, this must be enabled by setting UPGRADE_IMAGE_REGISTRY_STORAGE to true . This option is stringly recommended if you intend to install the Watson services from Cloud Pak for Data as the default volume size is too small. export CLUSTER_NAME=masinst1 export OCP_VERSION=4.8_openshift export IBMCLOUD_APIKEY=xxx export REBOOT_WORKER_NODES=true export CPD_ENTITLEMENT_KEY=xxx export UPGRADE_IMAGE_REGISTRY_STORAGE=true ansible-playbook ibm.mas_devops.ocp_roks_provision Provision on IBM DevIT Fyre \uf0c1 This playbook will provision a QuickBurn OCP cluster in IBM DevIT Fyre service, QuickBurn clusters will be automatically deprovisioned after 36 hours and are only suitable for small scale deployments for local development and demostration systems. export CLUSTER_NAME=masinst1 export OCP_VERSION=4.8 export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx export FYRE_PRODUCT_ID=xxx ansible-playbook ibm.mas_devops.ocp_fyre_provision Deprovision \uf0c1 Refer to the ocp_deprovision role documentation for more information. Deprovision on IBMCloud ROKS \uf0c1 export CLUSTER_NAME=masinst1 export IBMCLOUD_APIKEY=xxx ansible-playbook ibm.mas_devops.ocp_roks_deprovision Deprovision on IBM DevIT Fyre \uf0c1 export CLUSTER_NAME=masinst1 export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx ansible-playbook ibm.mas_devops.ocp_fyre_deprovision","title":"OCP"},{"location":"playbooks/ocp/#ocp-playbooks","text":"","title":"OCP Playbooks"},{"location":"playbooks/ocp/#provision","text":"Refer to the ocp_provision role documentation for more information.","title":"Provision"},{"location":"playbooks/ocp/#provision-on-aws-rosa","text":"This playbook uses your ROSA API Token to provision a brand new OCP cluster, provision an instance of EFS and set up the cluster with a ReadWriteMany storage class named efs utilizing that instance. To obtain your API token login to the OpenShift cluster manager . export AWS_ACCESS_KEY_ID=xxx export AWS_SECRET_ACCESS_KEY=xxx export ROSA_TOKEN=xxx export CLUSTER_NAME=masonrosa export OCP_VERSION=4.10 export ROSA_COMPUTE_NODES=5 export ROSA_CLUSTER_ADMIN_PASSWORD=xxx ansible-playbook ibm.mas_devops.ocp_rosa_provision","title":"Provision on AWS ROSA"},{"location":"playbooks/ocp/#provision-on-ibmcloud-roks","text":"This playbook uses your IBMCloud API key to provision a brand new OCP cluster. The playbook supports installing an IBM entitlement key as a cluster-wide image pull secret and reboot all worker nodes, which is required for IBM Cloud Pak for Data v4; this can be enabled by setting REBOOT_WORKER_NODES to true and providing the entitlement key with CPD_ENTITLEMENT_KEY . This also supports upgrading the storage volume used for the cluster's internal image registry from 100Gb to 400Gb, this must be enabled by setting UPGRADE_IMAGE_REGISTRY_STORAGE to true . This option is stringly recommended if you intend to install the Watson services from Cloud Pak for Data as the default volume size is too small. export CLUSTER_NAME=masinst1 export OCP_VERSION=4.8_openshift export IBMCLOUD_APIKEY=xxx export REBOOT_WORKER_NODES=true export CPD_ENTITLEMENT_KEY=xxx export UPGRADE_IMAGE_REGISTRY_STORAGE=true ansible-playbook ibm.mas_devops.ocp_roks_provision","title":"Provision on IBMCloud ROKS"},{"location":"playbooks/ocp/#provision-on-ibm-devit-fyre","text":"This playbook will provision a QuickBurn OCP cluster in IBM DevIT Fyre service, QuickBurn clusters will be automatically deprovisioned after 36 hours and are only suitable for small scale deployments for local development and demostration systems. export CLUSTER_NAME=masinst1 export OCP_VERSION=4.8 export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx export FYRE_PRODUCT_ID=xxx ansible-playbook ibm.mas_devops.ocp_fyre_provision","title":"Provision on IBM DevIT Fyre"},{"location":"playbooks/ocp/#deprovision","text":"Refer to the ocp_deprovision role documentation for more information.","title":"Deprovision"},{"location":"playbooks/ocp/#deprovision-on-ibmcloud-roks","text":"export CLUSTER_NAME=masinst1 export IBMCLOUD_APIKEY=xxx ansible-playbook ibm.mas_devops.ocp_roks_deprovision","title":"Deprovision on IBMCloud ROKS"},{"location":"playbooks/ocp/#deprovision-on-ibm-devit-fyre","text":"export CLUSTER_NAME=masinst1 export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx ansible-playbook ibm.mas_devops.ocp_fyre_deprovision","title":"Deprovision on IBM DevIT Fyre"},{"location":"playbooks/oneclick-core/","text":"OneClick Install for MAS Core \uf0c1 This playbook will install and configure IBM Maximo Application Suite Core along with all necessary dependencies. This can be ran against any OCP cluster regardless of it's type, whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. It will take approximately 90 minutes to set up MAS core services and all of it's dependencies, at the end of the process you will be able to login to the MAS admin dashboard to install any applications that you wish to use, or you can use our other playbooks to automate the installation of those applications (including any additional dependencies) Playbook Content \uf0c1 Install IBM Operator Catalogs (1 minute) Install IBM Common Services (3 minutes) Install Certificate Manager Operator (3 minutes) Install Service Binding Operator (2 minutes) Configure Cluster Monitoring (1 minute) Install Mongodb Operator and Create a Cluster (10 minutes) Install and bootstrap IBM Suite License Service (10 minutes) Install IBM User Data Services (30 minutes) Generate a MAS Workspace Configuration (1 minute) Configure Cloud Internet Services Integration for Maximo Application Suite (Optional, 1 minute) Install Maximo Application Suite Core Services (1 minute) Configure Maximo Application Suite (1 minute) Verify the Install and Configuration of Maximo Application Suite (25 minutes) All timings are estimates, see the individual pages for each of these roles for more information and full details of all configuration options available in this playbook. Preparation \uf0c1 1. IBM Entitlement key \uf0c1 Access Container Software Library using your IBMId to access your entitlement key 2. MAS License File \uf0c1 Access IBM License Key Center , on the Get Keys menu select IBM AppPoint Suites . Select IBM MAXIMO APPLICATION SUITE AppPOINT LIC and on the next page fill in the information as below: Field Content Number of Keys How many AppPoints to assign to the license file Host ID Type Set to Ethernet Address Host ID Enter any 12 digit hexadecimal string Hostname Set to the hostname of your OCP instance Port Set to 27000 The other values can be left at their defaults. Finally, click Generate and download the license file to your home directory as entitlement.lic , set SLS_LICENSE_FILE to point to this location. Usage \uf0c1 Required environment variables \uf0c1 IBM_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) SLS_LICENSE_ID The license ID must match the license file available in SLS_LICENSE_FILE SLS_LICENSE_FILE The path to the location of the license file. UDS_CONTACT_EMAIL Defines the email for person to contact for UDS UDS_CONTACT_FIRSTNAME Defines the first name of the person to contact for UDS UDS_CONTACT_LASTNAME Defines the last name of the person to contact for UDS Storage Class Configuraton \uf0c1 Storage class configuration is built into the collection and the playbook will auto-select the appropriate storage classes when it detects the presence of certain storage classes in your cluster (IBM Cloud Storage or OpenShift Container Storage). If you are running the install on a cluster that does not have these storage classes then you will also must configure the following environment variables: ReadWriteMany Access Mode \uf0c1 Usually fulfilled by block storage classes: PROMETHEUS_ALERTMGR_STORAGE_CLASS ReadWriteOnce Access Mode \uf0c1 Usually fulfilled by file storage classes: PROMETHEUS_STORAGE_CLASS PROMETHEUS_USERWORKLOAD_STORAGE_CLASS GRAFANA_INSTANCE_STORAGE_CLASS MONGODB_STORAGE_CLASS UDS_STORAGE_CLASS Examples \uf0c1 Release build \uf0c1 The simplest configuration to deploy a release build of IBM Maximo Application Suite (core only) with dependencies is: export IBM_ENTITLEMENT_KEY=xxx export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export SLS_LICENSE_ID=xxx export SLS_LICENSE_FILE=/path/to/entitlement.lic export UDS_CONTACT_EMAIL=xxx@xxx.com export UDS_CONTACT_FIRSTNAME=xxx export UDS_CONTACT_LASTNAME=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_core Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti quay.io/ibmmas/ansible-devops:latest bash Pre-release build \uf0c1 To deploy a pre-release build of IBM Maximo Application Suite (core only) with dependencies a number of additional parameters are required, note that pre-release builds are only available to IBM employees: export IBM_ENTITLEMENT_KEY=xxx export ARTIFACTORY_USERNAME=$W3_USERNAME_LOWERCASE export ARTIFACTORY_APIKEY=xxx export MAS_ICR_CP=wiotp-docker-local.artifactory.swg-devops.com export MAS_ICR_CPOPEN=wiotp-docker-local.artifactory.swg-devops.com export MAS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export MAS_ENTITLEMENT_KEY=$ARTIFACTORY_APIKEY export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export MAS_CATALOG_SOURCE=ibm-operator-catalog export MAS_CHANNEL=rp1dev88 export SLS_LICENSE_ID=xxx export SLS_LICENSE_FILE=/path/to/entitlement.lic export UDS_CONTACT_EMAIL=xxx@xxx.com export UDS_CONTACT_FIRSTNAME=xxx export UDS_CONTACT_LASTNAME=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_core Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti quay.io/ibmmas/ansible-devops:latest bash","title":"Install Core"},{"location":"playbooks/oneclick-core/#oneclick-install-for-mas-core","text":"This playbook will install and configure IBM Maximo Application Suite Core along with all necessary dependencies. This can be ran against any OCP cluster regardless of it's type, whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. It will take approximately 90 minutes to set up MAS core services and all of it's dependencies, at the end of the process you will be able to login to the MAS admin dashboard to install any applications that you wish to use, or you can use our other playbooks to automate the installation of those applications (including any additional dependencies)","title":"OneClick Install for MAS Core"},{"location":"playbooks/oneclick-core/#playbook-content","text":"Install IBM Operator Catalogs (1 minute) Install IBM Common Services (3 minutes) Install Certificate Manager Operator (3 minutes) Install Service Binding Operator (2 minutes) Configure Cluster Monitoring (1 minute) Install Mongodb Operator and Create a Cluster (10 minutes) Install and bootstrap IBM Suite License Service (10 minutes) Install IBM User Data Services (30 minutes) Generate a MAS Workspace Configuration (1 minute) Configure Cloud Internet Services Integration for Maximo Application Suite (Optional, 1 minute) Install Maximo Application Suite Core Services (1 minute) Configure Maximo Application Suite (1 minute) Verify the Install and Configuration of Maximo Application Suite (25 minutes) All timings are estimates, see the individual pages for each of these roles for more information and full details of all configuration options available in this playbook.","title":"Playbook Content"},{"location":"playbooks/oneclick-core/#preparation","text":"","title":"Preparation"},{"location":"playbooks/oneclick-core/#1-ibm-entitlement-key","text":"Access Container Software Library using your IBMId to access your entitlement key","title":"1. IBM Entitlement key"},{"location":"playbooks/oneclick-core/#2-mas-license-file","text":"Access IBM License Key Center , on the Get Keys menu select IBM AppPoint Suites . Select IBM MAXIMO APPLICATION SUITE AppPOINT LIC and on the next page fill in the information as below: Field Content Number of Keys How many AppPoints to assign to the license file Host ID Type Set to Ethernet Address Host ID Enter any 12 digit hexadecimal string Hostname Set to the hostname of your OCP instance Port Set to 27000 The other values can be left at their defaults. Finally, click Generate and download the license file to your home directory as entitlement.lic , set SLS_LICENSE_FILE to point to this location.","title":"2. MAS License File"},{"location":"playbooks/oneclick-core/#usage","text":"","title":"Usage"},{"location":"playbooks/oneclick-core/#required-environment-variables","text":"IBM_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) SLS_LICENSE_ID The license ID must match the license file available in SLS_LICENSE_FILE SLS_LICENSE_FILE The path to the location of the license file. UDS_CONTACT_EMAIL Defines the email for person to contact for UDS UDS_CONTACT_FIRSTNAME Defines the first name of the person to contact for UDS UDS_CONTACT_LASTNAME Defines the last name of the person to contact for UDS","title":"Required environment variables"},{"location":"playbooks/oneclick-core/#storage-class-configuraton","text":"Storage class configuration is built into the collection and the playbook will auto-select the appropriate storage classes when it detects the presence of certain storage classes in your cluster (IBM Cloud Storage or OpenShift Container Storage). If you are running the install on a cluster that does not have these storage classes then you will also must configure the following environment variables:","title":"Storage Class Configuraton"},{"location":"playbooks/oneclick-core/#readwritemany-access-mode","text":"Usually fulfilled by block storage classes: PROMETHEUS_ALERTMGR_STORAGE_CLASS","title":"ReadWriteMany Access Mode"},{"location":"playbooks/oneclick-core/#readwriteonce-access-mode","text":"Usually fulfilled by file storage classes: PROMETHEUS_STORAGE_CLASS PROMETHEUS_USERWORKLOAD_STORAGE_CLASS GRAFANA_INSTANCE_STORAGE_CLASS MONGODB_STORAGE_CLASS UDS_STORAGE_CLASS","title":"ReadWriteOnce Access Mode"},{"location":"playbooks/oneclick-core/#examples","text":"","title":"Examples"},{"location":"playbooks/oneclick-core/#release-build","text":"The simplest configuration to deploy a release build of IBM Maximo Application Suite (core only) with dependencies is: export IBM_ENTITLEMENT_KEY=xxx export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export SLS_LICENSE_ID=xxx export SLS_LICENSE_FILE=/path/to/entitlement.lic export UDS_CONTACT_EMAIL=xxx@xxx.com export UDS_CONTACT_FIRSTNAME=xxx export UDS_CONTACT_LASTNAME=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_core Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti quay.io/ibmmas/ansible-devops:latest bash","title":"Release build"},{"location":"playbooks/oneclick-core/#pre-release-build","text":"To deploy a pre-release build of IBM Maximo Application Suite (core only) with dependencies a number of additional parameters are required, note that pre-release builds are only available to IBM employees: export IBM_ENTITLEMENT_KEY=xxx export ARTIFACTORY_USERNAME=$W3_USERNAME_LOWERCASE export ARTIFACTORY_APIKEY=xxx export MAS_ICR_CP=wiotp-docker-local.artifactory.swg-devops.com export MAS_ICR_CPOPEN=wiotp-docker-local.artifactory.swg-devops.com export MAS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export MAS_ENTITLEMENT_KEY=$ARTIFACTORY_APIKEY export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export MAS_CATALOG_SOURCE=ibm-operator-catalog export MAS_CHANNEL=rp1dev88 export SLS_LICENSE_ID=xxx export SLS_LICENSE_FILE=/path/to/entitlement.lic export UDS_CONTACT_EMAIL=xxx@xxx.com export UDS_CONTACT_FIRSTNAME=xxx export UDS_CONTACT_LASTNAME=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_core Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti quay.io/ibmmas/ansible-devops:latest bash","title":"Pre-release build"},{"location":"playbooks/oneclick-iot/","text":"Install IoT Application \uf0c1 Prerequisites \uf0c1 You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 already be installed, the oneclick-core playbook can be used to set this up. Overview \uf0c1 This playbook will add Maximo IoT v8.4 to an existing IBM Maximo Application Suite Core installation. It will also creatie an in-cluster Db2 instance and Kafka cluster, both of which will be automatically set up as system-level configurations in MAS. IoT will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of it's type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install IBM Db2 Universal Operator (2 minutes) Create Db2 Warehouse Instance (45 minutes) Install RedHat AMQ Streams Operator (2 minutes) Create Apache Kafka Cluster (15 minutes) Configure Maximo Application Suite: Set up Db2 instance as the system-level JDBC datasource Set up Kafka cluster as the system-level Kafka Install Maximo IoT application: Install application (90 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Required environment variables \uf0c1 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) MAS_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Usage \uf0c1 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export MAS_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_iot Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti quay.io/ibmmas/ansible-devops:latest bash","title":"Add IoT"},{"location":"playbooks/oneclick-iot/#install-iot-application","text":"","title":"Install IoT Application"},{"location":"playbooks/oneclick-iot/#prerequisites","text":"You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 already be installed, the oneclick-core playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/oneclick-iot/#overview","text":"This playbook will add Maximo IoT v8.4 to an existing IBM Maximo Application Suite Core installation. It will also creatie an in-cluster Db2 instance and Kafka cluster, both of which will be automatically set up as system-level configurations in MAS. IoT will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of it's type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install IBM Db2 Universal Operator (2 minutes) Create Db2 Warehouse Instance (45 minutes) Install RedHat AMQ Streams Operator (2 minutes) Create Apache Kafka Cluster (15 minutes) Configure Maximo Application Suite: Set up Db2 instance as the system-level JDBC datasource Set up Kafka cluster as the system-level Kafka Install Maximo IoT application: Install application (90 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/oneclick-iot/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) MAS_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry","title":"Required environment variables"},{"location":"playbooks/oneclick-iot/#usage","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export MAS_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_iot Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti quay.io/ibmmas/ansible-devops:latest bash","title":"Usage"},{"location":"playbooks/oneclick-manage/","text":"Install Manage Application \uf0c1 Prerequisites \uf0c1 You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 already be installed, the oneclick-core playbook can be used to set this up. Overview \uf0c1 This playbook will add Maximo Manage v8.3 to an existing IBM Maximo Application Suite Core installation. It will also creatie an in-cluster Db2 instance, which will be automatically set up as the system-level JDBC configuration in MAS. Manage will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OCP cluster regardless of it's type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install IBM Db2 Universal Operator (2 minutes) Create Db2 Warehouse Instance (45 minutes) Additional Db2 configuration for Manage (5 minutes) Configure Maximo Application Suite: Set up Db2 instance as the system-level JDBC datasource Install Maximo Manage application: Install application (10 minutes) Configure workspace (2 hours) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Required environment variables \uf0c1 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) MAS_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry MAS_APP_ID Declare app_id as either manage or health Tip Manage requires the user to select one or more application components to enable in the workspace. By default the base component at the latest version will be installed if no MAS_APPWS_COMPONENTS is set. To customise the components that are enabled use the MAS_APPWS_COMPONENTS environment variable, for example to enable Manage(base) and Health set it to the following: export MAS_APPWS_COMPONENTS=\"base=latest,health=latest\" To enable Asset Investment Optimizer, optional feature of health. Set MANAGE_AIO_FLAG to true . By default this flag is set to false . This featue is only avalaible on Manage with health as a addon or on Health as a Standalone install. export MANAGE_AIO_FLAG=true To install Health as a Standalone with a specified version, set MAS_APP_ID to health and set MAS_APPWS_COMPONENTS to health=x.x.x . By default health standalone will be installed using health=latest export MAS_APP_ID=health export MAS_APPWS_COMPONENTS=\"health=latest\" Usage \uf0c1 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export MAS_ENTITLEMENT_KEY=xxx export MAS_APP_ID=manage oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_manage Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti quay.io/ibmmas/ansible-devops:latest bash","title":"Add Manage"},{"location":"playbooks/oneclick-manage/#install-manage-application","text":"","title":"Install Manage Application"},{"location":"playbooks/oneclick-manage/#prerequisites","text":"You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 already be installed, the oneclick-core playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/oneclick-manage/#overview","text":"This playbook will add Maximo Manage v8.3 to an existing IBM Maximo Application Suite Core installation. It will also creatie an in-cluster Db2 instance, which will be automatically set up as the system-level JDBC configuration in MAS. Manage will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OCP cluster regardless of it's type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install IBM Db2 Universal Operator (2 minutes) Create Db2 Warehouse Instance (45 minutes) Additional Db2 configuration for Manage (5 minutes) Configure Maximo Application Suite: Set up Db2 instance as the system-level JDBC datasource Install Maximo Manage application: Install application (10 minutes) Configure workspace (2 hours) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/oneclick-manage/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) MAS_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry MAS_APP_ID Declare app_id as either manage or health Tip Manage requires the user to select one or more application components to enable in the workspace. By default the base component at the latest version will be installed if no MAS_APPWS_COMPONENTS is set. To customise the components that are enabled use the MAS_APPWS_COMPONENTS environment variable, for example to enable Manage(base) and Health set it to the following: export MAS_APPWS_COMPONENTS=\"base=latest,health=latest\" To enable Asset Investment Optimizer, optional feature of health. Set MANAGE_AIO_FLAG to true . By default this flag is set to false . This featue is only avalaible on Manage with health as a addon or on Health as a Standalone install. export MANAGE_AIO_FLAG=true To install Health as a Standalone with a specified version, set MAS_APP_ID to health and set MAS_APPWS_COMPONENTS to health=x.x.x . By default health standalone will be installed using health=latest export MAS_APP_ID=health export MAS_APPWS_COMPONENTS=\"health=latest\"","title":"Required environment variables"},{"location":"playbooks/oneclick-manage/#usage","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export MAS_ENTITLEMENT_KEY=xxx export MAS_APP_ID=manage oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_manage Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti quay.io/ibmmas/ansible-devops:latest bash","title":"Usage"},{"location":"playbooks/oneclick-monitor/","text":"Install Monitor Application \uf0c1 Prerequisites \uf0c1 You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 and Maximo IoT v8.4 already be installed, the oneclick-core and oneclick-iot playbooks can be used to set this up. Overview \uf0c1 This playbook will add Maximo Monitor v8.7 to an existing IBM Maximo Application Suite Core installation. Monitor will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of it's type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install Maximo Monitor application: Install application (60 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Required environment variables \uf0c1 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) MAS_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Usage \uf0c1 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export MAS_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_monitor Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti quay.io/ibmmas/ansible-devops:latest bash","title":"Add Monitor"},{"location":"playbooks/oneclick-monitor/#install-monitor-application","text":"","title":"Install Monitor Application"},{"location":"playbooks/oneclick-monitor/#prerequisites","text":"You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 and Maximo IoT v8.4 already be installed, the oneclick-core and oneclick-iot playbooks can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/oneclick-monitor/#overview","text":"This playbook will add Maximo Monitor v8.7 to an existing IBM Maximo Application Suite Core installation. Monitor will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of it's type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install Maximo Monitor application: Install application (60 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/oneclick-monitor/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) MAS_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry","title":"Required environment variables"},{"location":"playbooks/oneclick-monitor/#usage","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export MAS_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_monitor Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti quay.io/ibmmas/ansible-devops:latest bash","title":"Usage"},{"location":"playbooks/oneclick-optimizer/","text":"Install Optimizer Application \uf0c1 Prerequisites \uf0c1 You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.8. The [oneclick-core] (oneclick-core.md) playbook can be used to set this up. Overview \uf0c1 This playbook will add Maximo Optimizer v8.2 to an existing IBM Maximo Application Suite Core installation. Optimizer will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of it's type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install Maximo Optimizer application: Install application (10 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Required environment variables \uf0c1 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) MAS_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Usage \uf0c1 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export MAS_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_optimizer Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti quay.io/ibmmas/ansible-devops:latest bash","title":"Add Optimizer"},{"location":"playbooks/oneclick-optimizer/#install-optimizer-application","text":"","title":"Install Optimizer Application"},{"location":"playbooks/oneclick-optimizer/#prerequisites","text":"You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.8. The [oneclick-core] (oneclick-core.md) playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/oneclick-optimizer/#overview","text":"This playbook will add Maximo Optimizer v8.2 to an existing IBM Maximo Application Suite Core installation. Optimizer will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of it's type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install Maximo Optimizer application: Install application (10 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/oneclick-optimizer/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) MAS_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry","title":"Required environment variables"},{"location":"playbooks/oneclick-optimizer/#usage","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export MAS_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_optimizer Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti quay.io/ibmmas/ansible-devops:latest bash","title":"Usage"},{"location":"playbooks/oneclick-predict/","text":"Install Predict Application \uf0c1 Prerequisites \uf0c1 You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 already be installed, the oneclick-core playbook can be used to set this up. Overview \uf0c1 This playbook will add Predict v8.6 to an existing IBM Maximo Application Suite Core installation. It will also install CloudPak for Data + CP4D services. This playbook can be ran against any OCP cluster regardless of it's type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install CP4D (~1 1/2 hours) Install Watson Studio (~3 hours) Install Watson Machine Learning (~2 1/2 hours) Install Spark (~30 minutes) Install Openscale (~1 hour) Install Predict application: Install application (~15 Minutes) Configure workspace (~30 Minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Required environment variables \uf0c1 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) MAS_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry These variables are required only if you set CP4D_INSTALL_WSL to false in optional varibles, otherwise don't set it. \uf0c1 CPD_ADMIN_USERNAME CP4D Username CPD_ADMIN_PASSWORD CP4D Password CPD_URL CP4D Base URL Warning Be sure that your current instance of CP4D has all the dependencies required by Predict: - Install CP4D - Install Watson Studio - Install Watson Machine Learning - Install Spark - Install Openscale Optional environment variables \uf0c1 CP4D_INSTALL_PLATFORM True/False - If you HAVE CP4D already installed in your cluster, then set it to \"false\" CP4D_INSTALL_WSL True/False - If you HAVE Watson Studio already installed in your cluster, then set it to \"false\" CP4D_INSTALL_WML True/False - If you HAVE Watson Machine Learning already installed in your cluster, then set it to \"false\" CP4D_INSTALL_SPARK True/False - If you HAVE Spark already installed in your cluster, then set it to \"false\" CP4D_INSTALL_OPENSCALE True/False - If you HAVE Openscale already installed in your cluster, then set it to \"false\" Usage when you already HAVE CP4D installed \uf0c1 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export MAS_ENTITLEMENT_KEY=xxx export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_URL=\"https://mycp4durl\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_predict Usage when you DON'T HAVE CP4D installed \uf0c1 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export MAS_ENTITLEMENT_KEY=xxx export CP4D_INSTALL_PLATFORM=\"true\" export CP4D_INSTALL_WSL=\"true\" export CP4D_INSTALL_WML=\"true\" export CP4D_INSTALL_SPARK=\"true\" export CP4D_INSTALL_OPENSCALE=\"true\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_predict Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti quay.io/ibmmas/ansible-devops:latest bash","title":"Install Predict Application"},{"location":"playbooks/oneclick-predict/#install-predict-application","text":"","title":"Install Predict Application"},{"location":"playbooks/oneclick-predict/#prerequisites","text":"You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 already be installed, the oneclick-core playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/oneclick-predict/#overview","text":"This playbook will add Predict v8.6 to an existing IBM Maximo Application Suite Core installation. It will also install CloudPak for Data + CP4D services. This playbook can be ran against any OCP cluster regardless of it's type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install CP4D (~1 1/2 hours) Install Watson Studio (~3 hours) Install Watson Machine Learning (~2 1/2 hours) Install Spark (~30 minutes) Install Openscale (~1 hour) Install Predict application: Install application (~15 Minutes) Configure workspace (~30 Minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/oneclick-predict/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) MAS_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry","title":"Required environment variables"},{"location":"playbooks/oneclick-predict/#these-variables-are-required-only-if-you-set-cp4d_install_wsl-to-false-in-optional-varibles-otherwise-dont-set-it","text":"CPD_ADMIN_USERNAME CP4D Username CPD_ADMIN_PASSWORD CP4D Password CPD_URL CP4D Base URL Warning Be sure that your current instance of CP4D has all the dependencies required by Predict: - Install CP4D - Install Watson Studio - Install Watson Machine Learning - Install Spark - Install Openscale","title":"These variables are required only if you set CP4D_INSTALL_WSL to false in optional varibles, otherwise don't set it."},{"location":"playbooks/oneclick-predict/#optional-environment-variables","text":"CP4D_INSTALL_PLATFORM True/False - If you HAVE CP4D already installed in your cluster, then set it to \"false\" CP4D_INSTALL_WSL True/False - If you HAVE Watson Studio already installed in your cluster, then set it to \"false\" CP4D_INSTALL_WML True/False - If you HAVE Watson Machine Learning already installed in your cluster, then set it to \"false\" CP4D_INSTALL_SPARK True/False - If you HAVE Spark already installed in your cluster, then set it to \"false\" CP4D_INSTALL_OPENSCALE True/False - If you HAVE Openscale already installed in your cluster, then set it to \"false\"","title":"Optional environment variables"},{"location":"playbooks/oneclick-predict/#usage-when-you-already-have-cp4d-installed","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export MAS_ENTITLEMENT_KEY=xxx export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_URL=\"https://mycp4durl\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_predict","title":"Usage when you already HAVE CP4D installed"},{"location":"playbooks/oneclick-predict/#usage-when-you-dont-have-cp4d-installed","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export MAS_ENTITLEMENT_KEY=xxx export CP4D_INSTALL_PLATFORM=\"true\" export CP4D_INSTALL_WSL=\"true\" export CP4D_INSTALL_WML=\"true\" export CP4D_INSTALL_SPARK=\"true\" export CP4D_INSTALL_OPENSCALE=\"true\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_predict Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti quay.io/ibmmas/ansible-devops:latest bash","title":"Usage when you DON'T HAVE CP4D installed"},{"location":"playbooks/oneclick-update/","text":"OneClick Update \uf0c1 This playbook will update the IBM Maximo Operator Catalog on your OpenShift cluster. This will make available new operator updates, which will be automatically applied across the cluster. These updates will not change the functionality of the software in your cluster, they will only carry fixes for security vulnerabilities and bugs. Note If you are using the dynamic catalog ( ibm-maximo-operator-catalog:v8 ) this playbook can be ignored, as you will recieve catalog updates in your cluster as soon as they are released. This playbook is specifically for customers who choose to use static catalogs to control the consumption of updates in their cluster. This is distinct from an upgrade , which will modify the operator subscriptions on your cluster to deliver new features. Performing an updating may make new upgrades available in the cluster, but it will never initiate the upgrade, you must choose when to upgrade. Playbook Content \uf0c1 Install IBM Operator Catalog (1 minute) Preparation \uf0c1 You will need to determine the version of the IBM Maximo Operator Catalog that you wish to update to. Generally speaking, you should update the most recent catalog available. The following catalogs are available at time of writing: v8-220805-amd64 v8-220717-amd64 Important If you are using a private/mirror registry it is critical that you mirror the images from the updated catalog before you run this playbook, otherwise you will see numerous containers in ImagePullBackoff as the updates are rolled out automatically after the catalog has been updated. You do not need to worry about translating the image tags to digests to make these catalogs compatible with image mirroring on OpenShift, the role will automatically usse the image digest when it installs any static operator catalog. Usage \uf0c1 Required environment variables \uf0c1 MAS_CATALOG_VERSION Example \uf0c1 Only one parameter is required, the new tag of the IBM Maximo Operator Catalog that you wish to use: export MAS_CATALOG_VERSION=v8-220805-amd64 oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_update Tip If you do not want to set up all the dependencies on your local system, you can run the update from inside our container image: docker run -ti --rm quay.io/ibmmas/cli:latest","title":"Update"},{"location":"playbooks/oneclick-update/#oneclick-update","text":"This playbook will update the IBM Maximo Operator Catalog on your OpenShift cluster. This will make available new operator updates, which will be automatically applied across the cluster. These updates will not change the functionality of the software in your cluster, they will only carry fixes for security vulnerabilities and bugs. Note If you are using the dynamic catalog ( ibm-maximo-operator-catalog:v8 ) this playbook can be ignored, as you will recieve catalog updates in your cluster as soon as they are released. This playbook is specifically for customers who choose to use static catalogs to control the consumption of updates in their cluster. This is distinct from an upgrade , which will modify the operator subscriptions on your cluster to deliver new features. Performing an updating may make new upgrades available in the cluster, but it will never initiate the upgrade, you must choose when to upgrade.","title":"OneClick Update"},{"location":"playbooks/oneclick-update/#playbook-content","text":"Install IBM Operator Catalog (1 minute)","title":"Playbook Content"},{"location":"playbooks/oneclick-update/#preparation","text":"You will need to determine the version of the IBM Maximo Operator Catalog that you wish to update to. Generally speaking, you should update the most recent catalog available. The following catalogs are available at time of writing: v8-220805-amd64 v8-220717-amd64 Important If you are using a private/mirror registry it is critical that you mirror the images from the updated catalog before you run this playbook, otherwise you will see numerous containers in ImagePullBackoff as the updates are rolled out automatically after the catalog has been updated. You do not need to worry about translating the image tags to digests to make these catalogs compatible with image mirroring on OpenShift, the role will automatically usse the image digest when it installs any static operator catalog.","title":"Preparation"},{"location":"playbooks/oneclick-update/#usage","text":"","title":"Usage"},{"location":"playbooks/oneclick-update/#required-environment-variables","text":"MAS_CATALOG_VERSION","title":"Required environment variables"},{"location":"playbooks/oneclick-update/#example","text":"Only one parameter is required, the new tag of the IBM Maximo Operator Catalog that you wish to use: export MAS_CATALOG_VERSION=v8-220805-amd64 oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_update Tip If you do not want to set up all the dependencies on your local system, you can run the update from inside our container image: docker run -ti --rm quay.io/ibmmas/cli:latest","title":"Example"},{"location":"playbooks/oneclick-upgrade/","text":"OneClick Upgrade \uf0c1 This playbook will upgrade the channel subscriptions for IBM Maximo Application Suite on your OpenShift cluster. Upgrades can only be performed to releases available in the version of the IBM Maximo Pperator Catalog that is installed in your cluster. To update to a newer version of the operator catalog refer to the oneclick-update.md playbook documentation. The playbook will attempt to upgrade MAS Core and all installed applications. Note If you are using the dynamic catalog ( ibm-maximo-operator-catalog:v8 ) you will always have access to the latest MAS releases, as you will recieve catalog updates in your cluster as soon as they are released. Customers using the static catalogs to control the consumption of updates in their cluster will need to ensure that the version of the catalog they have installed supports the version of MAS that they wish to upgrade to. Playbook Content \uf0c1 Upgrade MAS Core Verify MAS Core Upgrade MAS Application (Assist) Upgrade MAS Application (HP Utilities) Upgrade MAS Application (IoT) Upgrade MAS Application (Manage) Upgrade MAS Application (Monitor) Upgrade MAS Application (Optimizer) Upgrade MAS Application (Predict) Upgrade MAS Application (Safety) Upgrade MAS Application (Visual Inspection) Preparation \uf0c1 If you are using a private/mirror registry it is critical that you mirror the images for the new release before you run this playbook, otherwise you will see numerous containers in ImagePullBackoff as the updates are rolled out automatically after the subscription has been changed, if you have not mirrored the new images the subscription change itself may fail if the operator bundle is not on your private registry. Usage \uf0c1 Required Parameters \uf0c1 MAS_INSTANCE_ID Set the instance ID of the MAS installation to upgrade Optional Parameters \uf0c1 If you provide no values for MAS Core or the individual applications, the roles will attempt to upgrade to the next level of MAS and upgrade applications to the latest version supported by the installed version of MAS Core (after upgrading MAS Core). MAS_CHANNEL Set the target subscription channel for MAS Core MAS_APP_CHANNEL_ASSIST Set the target subscription channel for Assist MAS_APP_CHANNEL_HPUTILITIES Set the target subscription channel for Health & Predict Utilities MAS_APP_CHANNEL_IOT Set the target subscription channel for IoT MAS_APP_CHANNEL_MONITOR Set the target subscription channel for Monitor MAS_APP_CHANNEL_OPTIMIZER Set the target subscription channel for Optimizer MAS_APP_CHANNEL_PREDICT Set the target subscription channel for Predict MAS_APP_CHANNEL_SAFETY Set the target subscription channel for Safety MAS_APP_CHANNEL_VISUALINSPECTION Set the target subscription channel for Visual Inspection Example \uf0c1 The simplest way to upgrade MAS is to provide only the instance ID that you wish to upgrade, allowing the roles to determine the correct target version each application. export MAS_INSTANCE_ID=instance1 oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_upgrade You can also explicitly specify the target upgrade: export MAS_INSTANCE_ID=instance1 export MAS_CHANNEL=8.8.x export MAS_APP_CHANNEL_IOT=8.5.x oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_upgrade Tip If you do not want to set up all the dependencies on your local system, you can run the update from inside our container image: docker run -ti --rm quay.io/ibmmas/cli:latest","title":"Upgrade"},{"location":"playbooks/oneclick-upgrade/#oneclick-upgrade","text":"This playbook will upgrade the channel subscriptions for IBM Maximo Application Suite on your OpenShift cluster. Upgrades can only be performed to releases available in the version of the IBM Maximo Pperator Catalog that is installed in your cluster. To update to a newer version of the operator catalog refer to the oneclick-update.md playbook documentation. The playbook will attempt to upgrade MAS Core and all installed applications. Note If you are using the dynamic catalog ( ibm-maximo-operator-catalog:v8 ) you will always have access to the latest MAS releases, as you will recieve catalog updates in your cluster as soon as they are released. Customers using the static catalogs to control the consumption of updates in their cluster will need to ensure that the version of the catalog they have installed supports the version of MAS that they wish to upgrade to.","title":"OneClick Upgrade"},{"location":"playbooks/oneclick-upgrade/#playbook-content","text":"Upgrade MAS Core Verify MAS Core Upgrade MAS Application (Assist) Upgrade MAS Application (HP Utilities) Upgrade MAS Application (IoT) Upgrade MAS Application (Manage) Upgrade MAS Application (Monitor) Upgrade MAS Application (Optimizer) Upgrade MAS Application (Predict) Upgrade MAS Application (Safety) Upgrade MAS Application (Visual Inspection)","title":"Playbook Content"},{"location":"playbooks/oneclick-upgrade/#preparation","text":"If you are using a private/mirror registry it is critical that you mirror the images for the new release before you run this playbook, otherwise you will see numerous containers in ImagePullBackoff as the updates are rolled out automatically after the subscription has been changed, if you have not mirrored the new images the subscription change itself may fail if the operator bundle is not on your private registry.","title":"Preparation"},{"location":"playbooks/oneclick-upgrade/#usage","text":"","title":"Usage"},{"location":"playbooks/oneclick-upgrade/#required-parameters","text":"MAS_INSTANCE_ID Set the instance ID of the MAS installation to upgrade","title":"Required Parameters"},{"location":"playbooks/oneclick-upgrade/#optional-parameters","text":"If you provide no values for MAS Core or the individual applications, the roles will attempt to upgrade to the next level of MAS and upgrade applications to the latest version supported by the installed version of MAS Core (after upgrading MAS Core). MAS_CHANNEL Set the target subscription channel for MAS Core MAS_APP_CHANNEL_ASSIST Set the target subscription channel for Assist MAS_APP_CHANNEL_HPUTILITIES Set the target subscription channel for Health & Predict Utilities MAS_APP_CHANNEL_IOT Set the target subscription channel for IoT MAS_APP_CHANNEL_MONITOR Set the target subscription channel for Monitor MAS_APP_CHANNEL_OPTIMIZER Set the target subscription channel for Optimizer MAS_APP_CHANNEL_PREDICT Set the target subscription channel for Predict MAS_APP_CHANNEL_SAFETY Set the target subscription channel for Safety MAS_APP_CHANNEL_VISUALINSPECTION Set the target subscription channel for Visual Inspection","title":"Optional Parameters"},{"location":"playbooks/oneclick-upgrade/#example","text":"The simplest way to upgrade MAS is to provide only the instance ID that you wish to upgrade, allowing the roles to determine the correct target version each application. export MAS_INSTANCE_ID=instance1 oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_upgrade You can also explicitly specify the target upgrade: export MAS_INSTANCE_ID=instance1 export MAS_CHANNEL=8.8.x export MAS_APP_CHANNEL_IOT=8.5.x oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_upgrade Tip If you do not want to set up all the dependencies on your local system, you can run the update from inside our container image: docker run -ti --rm quay.io/ibmmas/cli:latest","title":"Example"},{"location":"playbooks/oneclick-visualinspection/","text":"Install Visual Inspection Application \uf0c1 Prerequisites \uf0c1 You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.8. The [oneclick-core] (oneclick-core.md) playbook can be used to set this up. Overview \uf0c1 This playbook will add Maximo Visual Inspection v8.5 to an existing IBM Maximo Application Suite Core installation. MVI will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of it's type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install NVIDIA Graphical Processing Unit (GPU) (10 minutes) Install Maximo Visual Inspection application: Install application (15 minutes) Configure workspace (10 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Required environment variables \uf0c1 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) MAS_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Usage \uf0c1 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export MAS_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_optimizer Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti quay.io/ibmmas/ansible-devops:latest bash","title":"Add Visual Inspection"},{"location":"playbooks/oneclick-visualinspection/#install-visual-inspection-application","text":"","title":"Install Visual Inspection Application"},{"location":"playbooks/oneclick-visualinspection/#prerequisites","text":"You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.8. The [oneclick-core] (oneclick-core.md) playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/oneclick-visualinspection/#overview","text":"This playbook will add Maximo Visual Inspection v8.5 to an existing IBM Maximo Application Suite Core installation. MVI will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of it's type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install NVIDIA Graphical Processing Unit (GPU) (10 minutes) Install Maximo Visual Inspection application: Install application (15 minutes) Configure workspace (10 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/oneclick-visualinspection/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) MAS_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry","title":"Required environment variables"},{"location":"playbooks/oneclick-visualinspection/#usage","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export MAS_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_optimizer Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti quay.io/ibmmas/ansible-devops:latest bash","title":"Usage"},{"location":"roles/ansible_version_check/","text":"ansible_version_check \uf0c1 Internal-use role that all other roles in the collection declare a dependency upon to ensure that the minimum supported level of Ansible is used. License \uf0c1 EPL-2.0","title":"ansible_version_check"},{"location":"roles/ansible_version_check/#ansible_version_check","text":"Internal-use role that all other roles in the collection declare a dependency upon to ensure that the minimum supported level of Ansible is used.","title":"ansible_version_check"},{"location":"roles/ansible_version_check/#license","text":"EPL-2.0","title":"License"},{"location":"roles/appconnect/","text":"appconnect \uf0c1 Installs IBM AppConnect and generates configuration that can be directly applied to IBM Maximo Application Suite. This dependency is required by the Health and Predict Utilities application: HP Utilities v8.4 requires support for 12.0.4.0-r2 AppConnect dashboards (License ID L-APEH-C9NCK6 ) HP Utilities v8.3 requires support for 12.0.2.0-r2 AppConnect dashboards (License ID L-KSBM-C87FU2 ) HP Utilities v8.2 requires support for 12.0.1.0-r2 AppConnect dashboards (license ID L-KSBM-C37J2R ) HP Utilities AppConnect License Dashboard Versions v8.4 v4.1 - v5.2 L-APEH-C9NCK6 12.0.4.0-r1, 12.0.4.0-r2 v8.3 v3.0 - v4.2 L-KSBM-C87FU2 12.0.2.0-r2 v8.2 v1.5 - v3.1 L-KSBM-C37J2R 12.0.1.0-r1, 12.0.1.0-r2 For more information review the licensing reference for IBM App Connect Operator . Important All defaults in this role are currently set for compatability with HP Utilities version 8.4. If you are installing App Connect for use with older release of HP Utilities then you must set the appconnect_channel and appconnect_license_id variables (and it would be sensible to customize appconnect_dashboard_name as well). Role Variables - Installation \uf0c1 ibm_entitlement_key \uf0c1 Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None appconnect_entitlement_username \uf0c1 An IBM entitlement key specific for AppConnect installation, primarily used to override ibm_entitlement_key in development. Optional Environment Variable: APPCONNECT_ENTITLEMENT_USERNAME Default: None appconnect_namespace \uf0c1 Defines the targetted cluster namespace/project where AppConnect will be installed. If not provided, default AppConnect namespace will be ibm-app-connect . Optional Environment Variable: APPCONNECT_NAMESPACE Default Value: ibm-app-connect appconnect_channel \uf0c1 Subscription channel, this must align with the version of HP Utilities (see table above). Optional Environment Variable: APPCONNECT_CHANNEL Default Value: v5.2 Role Variables - Configuration \uf0c1 appconnect_storage_class \uf0c1 Storage class where AppConnect will be installed - for IBM Cloud clusters, ibmc-file-gold-gid must be used as per documentation . Required Environment Variable: APPCONNECT_STORAGE_CLASS Default Value: None appconnect_dashboard_name \uf0c1 AppConnect dashboard instance name. Defaults to dashboard-12040r2 as a reference to AppConnect Dashboard version 12.0.4.0-r2 that is compatible with the default subscription channel and license ID. Optional Environment Variable: APPCONNECT_DASHBOARD_NAME Default Value: dashboard-12040r2 appconnect_dashboard_version \uf0c1 AppConnect dashboard version, this must align with the License ID used. Optional Environment Variable: APPCONNECT_DASHBOARD_VERSION Default Value: 12.0.4.0-r2 appconnect_license_id \uf0c1 AppConnect license ID. Optional Environment Variable: APPCONNECT_LICENSE_ID Default Value: L-KSBM-C37J2R Role Variables - MAS Configuration \uf0c1 mas_instance_id \uf0c1 The instance ID of Maximo Application Suite that the AppConnect configuration will target. If this or mas_config_dir are not set then the role will not generate an AppConnect template. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \uf0c1 Local directory to save the generated AppConnect resource definition. This can be used to manually configure a MAS instance to connect to AppConnect instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate an AppConnect template. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None Example Playbooks \uf0c1 Install IBM App Connect for the latest release of HP Utilties (v8.4) \uf0c1 - hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxx roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.appconnect Install IBM App Connect for HP Utilties v8.3 \uf0c1 - hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxx appconnect_channel: v4.2 appconnect_license_id: L-KSBM-C87FU2 appconnect_dashboard_name: dashboard-12020r2 roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.appconnect License \uf0c1 EPL-2.0","title":"appconnect"},{"location":"roles/appconnect/#appconnect","text":"Installs IBM AppConnect and generates configuration that can be directly applied to IBM Maximo Application Suite. This dependency is required by the Health and Predict Utilities application: HP Utilities v8.4 requires support for 12.0.4.0-r2 AppConnect dashboards (License ID L-APEH-C9NCK6 ) HP Utilities v8.3 requires support for 12.0.2.0-r2 AppConnect dashboards (License ID L-KSBM-C87FU2 ) HP Utilities v8.2 requires support for 12.0.1.0-r2 AppConnect dashboards (license ID L-KSBM-C37J2R ) HP Utilities AppConnect License Dashboard Versions v8.4 v4.1 - v5.2 L-APEH-C9NCK6 12.0.4.0-r1, 12.0.4.0-r2 v8.3 v3.0 - v4.2 L-KSBM-C87FU2 12.0.2.0-r2 v8.2 v1.5 - v3.1 L-KSBM-C37J2R 12.0.1.0-r1, 12.0.1.0-r2 For more information review the licensing reference for IBM App Connect Operator . Important All defaults in this role are currently set for compatability with HP Utilities version 8.4. If you are installing App Connect for use with older release of HP Utilities then you must set the appconnect_channel and appconnect_license_id variables (and it would be sensible to customize appconnect_dashboard_name as well).","title":"appconnect"},{"location":"roles/appconnect/#role-variables-installation","text":"","title":"Role Variables - Installation"},{"location":"roles/appconnect/#ibm_entitlement_key","text":"Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None","title":"ibm_entitlement_key"},{"location":"roles/appconnect/#appconnect_entitlement_username","text":"An IBM entitlement key specific for AppConnect installation, primarily used to override ibm_entitlement_key in development. Optional Environment Variable: APPCONNECT_ENTITLEMENT_USERNAME Default: None","title":"appconnect_entitlement_username"},{"location":"roles/appconnect/#appconnect_namespace","text":"Defines the targetted cluster namespace/project where AppConnect will be installed. If not provided, default AppConnect namespace will be ibm-app-connect . Optional Environment Variable: APPCONNECT_NAMESPACE Default Value: ibm-app-connect","title":"appconnect_namespace"},{"location":"roles/appconnect/#appconnect_channel","text":"Subscription channel, this must align with the version of HP Utilities (see table above). Optional Environment Variable: APPCONNECT_CHANNEL Default Value: v5.2","title":"appconnect_channel"},{"location":"roles/appconnect/#role-variables-configuration","text":"","title":"Role Variables - Configuration"},{"location":"roles/appconnect/#appconnect_storage_class","text":"Storage class where AppConnect will be installed - for IBM Cloud clusters, ibmc-file-gold-gid must be used as per documentation . Required Environment Variable: APPCONNECT_STORAGE_CLASS Default Value: None","title":"appconnect_storage_class"},{"location":"roles/appconnect/#appconnect_dashboard_name","text":"AppConnect dashboard instance name. Defaults to dashboard-12040r2 as a reference to AppConnect Dashboard version 12.0.4.0-r2 that is compatible with the default subscription channel and license ID. Optional Environment Variable: APPCONNECT_DASHBOARD_NAME Default Value: dashboard-12040r2","title":"appconnect_dashboard_name"},{"location":"roles/appconnect/#appconnect_dashboard_version","text":"AppConnect dashboard version, this must align with the License ID used. Optional Environment Variable: APPCONNECT_DASHBOARD_VERSION Default Value: 12.0.4.0-r2","title":"appconnect_dashboard_version"},{"location":"roles/appconnect/#appconnect_license_id","text":"AppConnect license ID. Optional Environment Variable: APPCONNECT_LICENSE_ID Default Value: L-KSBM-C37J2R","title":"appconnect_license_id"},{"location":"roles/appconnect/#role-variables-mas-configuration","text":"","title":"Role Variables - MAS Configuration"},{"location":"roles/appconnect/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the AppConnect configuration will target. If this or mas_config_dir are not set then the role will not generate an AppConnect template. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/appconnect/#mas_config_dir","text":"Local directory to save the generated AppConnect resource definition. This can be used to manually configure a MAS instance to connect to AppConnect instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate an AppConnect template. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/appconnect/#example-playbooks","text":"","title":"Example Playbooks"},{"location":"roles/appconnect/#install-ibm-app-connect-for-the-latest-release-of-hp-utilties-v84","text":"- hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxx roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.appconnect","title":"Install IBM App Connect for the latest release of HP Utilties (v8.4)"},{"location":"roles/appconnect/#install-ibm-app-connect-for-hp-utilties-v83","text":"- hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxx appconnect_channel: v4.2 appconnect_license_id: L-KSBM-C87FU2 appconnect_dashboard_name: dashboard-12020r2 roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.appconnect","title":"Install IBM App Connect for HP Utilties v8.3"},{"location":"roles/appconnect/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cert_manager/","text":"cert_manager \uf0c1 Deploy the IBM Certificate Manager Operator into the target OCP cluster in the ibm-common-services namespace. Prerequisites \uf0c1 To run this role successfully you must have already installed a CatalogSource that contains IBM Certificate Manager and installed the IBM Cloud Pak Foundational Services Operator . These tasks can be achieved using the ibm_catalogs and common_services roles in this collection. Role Variables \uf0c1 None Example Playbook \uf0c1 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.common_services - ibm.mas_devops.cert_manager Run Role Playbook \uf0c1 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ROLE_NAME=cert_manager ansible-playbook ibm.mas_devops.run_role License \uf0c1 EPL-2.0","title":"cert_manager"},{"location":"roles/cert_manager/#cert_manager","text":"Deploy the IBM Certificate Manager Operator into the target OCP cluster in the ibm-common-services namespace.","title":"cert_manager"},{"location":"roles/cert_manager/#prerequisites","text":"To run this role successfully you must have already installed a CatalogSource that contains IBM Certificate Manager and installed the IBM Cloud Pak Foundational Services Operator . These tasks can be achieved using the ibm_catalogs and common_services roles in this collection.","title":"Prerequisites"},{"location":"roles/cert_manager/#role-variables","text":"None","title":"Role Variables"},{"location":"roles/cert_manager/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.common_services - ibm.mas_devops.cert_manager","title":"Example Playbook"},{"location":"roles/cert_manager/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ROLE_NAME=cert_manager ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/cert_manager/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cert_manager_upgrade/","text":"cert_manager_upgrade \uf0c1 This role will migrate the cert-manager resources used by the existing MAS 8.6 instance from JetStack cert-manager (under cert-manager namespace) to IBM Certificate Manager (under ibm-common-services namespace) used in MAS 8.7+ When running this role, note this will affect all of the applications in your Openshift Cluster that use cluster issuers to sign for certificates. Here are the list of events that will happen as part of this role: Cert-manager Jetstack deployments (under cert-manager namespace) will be deleted and IBM Certificate Manager (under ibm-common-services namespace) will be installed. Only one Certificate Manager must be running in the cluster to avoid conflicts. Note: It won't fully uninstall existing cert-manager nor remove its namespace/resources, however when this step is done, existing cert-manager won't be running anymore as it will be replaced by the new IBM Certificate Manager, therefore before running this role, be sure you know what you are doing to avoid disruptions in other services deployed in your cluster. Cert-manager resources for your targeted MAS instance such as Issuers, Certificates and Secrets will be moved from cert-manager namespace to the ibm-common-services namespace, that is needed so the new IBM Certificate Manager can continue to manage your MAS certificates after 8.7 upgrade. Cert-manager resources that will be moved to the new IBM Certificate Manager namespace ( cert-public resources will only exist if MAS is using self-signed certificates): secret/{{ mas_instance_id }}-cert-internal-ca secret/{{ mas_instance_id }}-cert-public-ca (only exists if MAS is using self-signed certificates) issuer.cert-manager.io/{{ mas_instance_id }}-core-internal-ca-issuer issuer.cert-manager.io/{{ mas_instance_id }}-core-public-ca-issuer (only exists if MAS is using self-signed certificates) certificate.cert-manager.io/{{ mas_instance_id }}-cert-internal-ca certificate.cert-manager.io/{{ mas_instance_id }}-cert-public-ca (only exists if MAS is using self-signed certificates) For more information, please refer to Upgrading Maximo Application Suite documentation. Role Variables \uf0c1 mas_instance_id \uf0c1 Required - Defines the instance id that is used in the existing MAS installation, will be used to lookup the existing MAS subscription and targeted Certificates, Issuers and Secrets to be migrated. Environment Variable: MAS_INSTANCE_ID Default Value: None custom_cluster_issuer \uf0c1 Optional - Only required if the targeted MAS 8.6 instance is configured with a custom cluster issuer (i.e Let's Encrypt). Having a cluster issuer or not will determine which resources will be migrated to the new IBM Certificate Manager namespace. Not required if your MAS instance is installed and configured using self-signed certificates. Environment Variable: MAS_CUSTOM_CLUSTER_ISSUER Default Value: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.suite_upgrade_check - ibm.mas_devops.cert_manager_upgrade If you have cluster issuer, and have setup CIS webhook in your existing instance, you will need also to reinstall CIS webhook in ibm-common-services namespace. Therefore, you might want to use the following playbook sample to accomplish this. - hosts: localhost any_errors_fatal: true vars: is_suite_upgrade: true # this will tell 'ibm.mas_devops.suite_dns' to reinstall cis webhook under 'ibm-common-services' ns if 'MAS_CUSTOM_CLUSTER_ISSUER' is set roles: - ibm.mas_devops.suite_upgrade_check - ibm.mas_devops.cert_manager_upgrade - ibm.mas_devops.suite_dns","title":"cert_manager_upgrade"},{"location":"roles/cert_manager_upgrade/#cert_manager_upgrade","text":"This role will migrate the cert-manager resources used by the existing MAS 8.6 instance from JetStack cert-manager (under cert-manager namespace) to IBM Certificate Manager (under ibm-common-services namespace) used in MAS 8.7+ When running this role, note this will affect all of the applications in your Openshift Cluster that use cluster issuers to sign for certificates. Here are the list of events that will happen as part of this role: Cert-manager Jetstack deployments (under cert-manager namespace) will be deleted and IBM Certificate Manager (under ibm-common-services namespace) will be installed. Only one Certificate Manager must be running in the cluster to avoid conflicts. Note: It won't fully uninstall existing cert-manager nor remove its namespace/resources, however when this step is done, existing cert-manager won't be running anymore as it will be replaced by the new IBM Certificate Manager, therefore before running this role, be sure you know what you are doing to avoid disruptions in other services deployed in your cluster. Cert-manager resources for your targeted MAS instance such as Issuers, Certificates and Secrets will be moved from cert-manager namespace to the ibm-common-services namespace, that is needed so the new IBM Certificate Manager can continue to manage your MAS certificates after 8.7 upgrade. Cert-manager resources that will be moved to the new IBM Certificate Manager namespace ( cert-public resources will only exist if MAS is using self-signed certificates): secret/{{ mas_instance_id }}-cert-internal-ca secret/{{ mas_instance_id }}-cert-public-ca (only exists if MAS is using self-signed certificates) issuer.cert-manager.io/{{ mas_instance_id }}-core-internal-ca-issuer issuer.cert-manager.io/{{ mas_instance_id }}-core-public-ca-issuer (only exists if MAS is using self-signed certificates) certificate.cert-manager.io/{{ mas_instance_id }}-cert-internal-ca certificate.cert-manager.io/{{ mas_instance_id }}-cert-public-ca (only exists if MAS is using self-signed certificates) For more information, please refer to Upgrading Maximo Application Suite documentation.","title":"cert_manager_upgrade"},{"location":"roles/cert_manager_upgrade/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cert_manager_upgrade/#mas_instance_id","text":"Required - Defines the instance id that is used in the existing MAS installation, will be used to lookup the existing MAS subscription and targeted Certificates, Issuers and Secrets to be migrated. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/cert_manager_upgrade/#custom_cluster_issuer","text":"Optional - Only required if the targeted MAS 8.6 instance is configured with a custom cluster issuer (i.e Let's Encrypt). Having a cluster issuer or not will determine which resources will be migrated to the new IBM Certificate Manager namespace. Not required if your MAS instance is installed and configured using self-signed certificates. Environment Variable: MAS_CUSTOM_CLUSTER_ISSUER Default Value: None","title":"custom_cluster_issuer"},{"location":"roles/cert_manager_upgrade/#example-playbook","text":"- hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.suite_upgrade_check - ibm.mas_devops.cert_manager_upgrade If you have cluster issuer, and have setup CIS webhook in your existing instance, you will need also to reinstall CIS webhook in ibm-common-services namespace. Therefore, you might want to use the following playbook sample to accomplish this. - hosts: localhost any_errors_fatal: true vars: is_suite_upgrade: true # this will tell 'ibm.mas_devops.suite_dns' to reinstall cis webhook under 'ibm-common-services' ns if 'MAS_CUSTOM_CLUSTER_ISSUER' is set roles: - ibm.mas_devops.suite_upgrade_check - ibm.mas_devops.cert_manager_upgrade - ibm.mas_devops.suite_dns","title":"Example Playbook"},{"location":"roles/cert_manager_upgrade_check/","text":"cert_manager_upgrade_check \uf0c1 This role requires cert_manager_upgrade role to have been executed prior. This will check and validate that MAS 8.6 instance continues to be in a fully working state after the cert-manager resources are migrated to new IBM Certificate Manager version. Here are the list of events that will happen as part of this role: It will update your targeted MAS 8.6 instance Suite CR to use ibm-common-services namespace to lookup for IBM Certificate Manager, instead of cert-manager namespace, then wait for MAS 8.6 operator to reconcile this change. It will make necessary assertions to ensure the new IBM Certificate Manager is able to handle certificate management against the targeted MAS instance. If all assertions are satisfied, then it will delete and clean up the old and not used cert-manager resources under cert-manager namespace: secret/{{ mas_instance_id }}-cert-internal-ca secret/{{ mas_instance_id }}-cert-public-ca (only exists if MAS is using self-signed certificates) issuer.cert-manager.io/{{ mas_instance_id }}-core-internal-ca-issuer issuer.cert-manager.io/{{ mas_instance_id }}-core-public-ca-issuer (only exists if MAS is using self-signed certificates) certificate.cert-manager.io/{{ mas_instance_id }}-cert-internal-ca certificate.cert-manager.io/{{ mas_instance_id }}-cert-public-ca (only exists if MAS is using self-signed certificates) For more information, please refer to Upgrading Maximo Application Suite documentation. Role Variables \uf0c1 mas_instance_id \uf0c1 Required - Defines the instance id that is used in the existing MAS installation, will be used to lookup the existing MAS subscription and targeted Certificates, Issuers and Secrets to be migrated. Environment Variable: MAS_INSTANCE_ID Default Value: None custom_cluster_issuer \uf0c1 Optional - Only required if the targeted MAS 8.6 instance is configured with a custom cluster issuer (i.e Let's Encrypt). Having a cluster issuer or not will determine which resources will be migrated and cleaned up from the old and existing cert-manager namespace. Not required if your MAS instance is installed and configured using self-signed certificates. Environment Variable: MAS_CUSTOM_CLUSTER_ISSUER Default Value: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.suite_upgrade_check - ibm.mas_devops.cert_manager_upgrade - ibm.mas_devops.cert_manager_upgrade_check If you have cluster issuer, and have setup CIS webhook in your existing instance, you will need also to reinstall CIS webhook in ibm-common-services namespace. Therefore, you might want to use the following playbook sample to accomplish this. - hosts: localhost any_errors_fatal: true vars: is_suite_upgrade: true # this will tell 'ibm.mas_devops.suite_dns' to reinstall cis webhook under 'ibm-common-services' ns if 'MAS_CUSTOM_CLUSTER_ISSUER' is set roles: - ibm.mas_devops.suite_upgrade_check - ibm.mas_devops.cert_manager_upgrade - ibm.mas_devops.suite_dns - ibm.mas_devops.cert_manager_upgrade_check","title":"cert_manager_upgrade_check"},{"location":"roles/cert_manager_upgrade_check/#cert_manager_upgrade_check","text":"This role requires cert_manager_upgrade role to have been executed prior. This will check and validate that MAS 8.6 instance continues to be in a fully working state after the cert-manager resources are migrated to new IBM Certificate Manager version. Here are the list of events that will happen as part of this role: It will update your targeted MAS 8.6 instance Suite CR to use ibm-common-services namespace to lookup for IBM Certificate Manager, instead of cert-manager namespace, then wait for MAS 8.6 operator to reconcile this change. It will make necessary assertions to ensure the new IBM Certificate Manager is able to handle certificate management against the targeted MAS instance. If all assertions are satisfied, then it will delete and clean up the old and not used cert-manager resources under cert-manager namespace: secret/{{ mas_instance_id }}-cert-internal-ca secret/{{ mas_instance_id }}-cert-public-ca (only exists if MAS is using self-signed certificates) issuer.cert-manager.io/{{ mas_instance_id }}-core-internal-ca-issuer issuer.cert-manager.io/{{ mas_instance_id }}-core-public-ca-issuer (only exists if MAS is using self-signed certificates) certificate.cert-manager.io/{{ mas_instance_id }}-cert-internal-ca certificate.cert-manager.io/{{ mas_instance_id }}-cert-public-ca (only exists if MAS is using self-signed certificates) For more information, please refer to Upgrading Maximo Application Suite documentation.","title":"cert_manager_upgrade_check"},{"location":"roles/cert_manager_upgrade_check/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cert_manager_upgrade_check/#mas_instance_id","text":"Required - Defines the instance id that is used in the existing MAS installation, will be used to lookup the existing MAS subscription and targeted Certificates, Issuers and Secrets to be migrated. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/cert_manager_upgrade_check/#custom_cluster_issuer","text":"Optional - Only required if the targeted MAS 8.6 instance is configured with a custom cluster issuer (i.e Let's Encrypt). Having a cluster issuer or not will determine which resources will be migrated and cleaned up from the old and existing cert-manager namespace. Not required if your MAS instance is installed and configured using self-signed certificates. Environment Variable: MAS_CUSTOM_CLUSTER_ISSUER Default Value: None","title":"custom_cluster_issuer"},{"location":"roles/cert_manager_upgrade_check/#example-playbook","text":"- hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.suite_upgrade_check - ibm.mas_devops.cert_manager_upgrade - ibm.mas_devops.cert_manager_upgrade_check If you have cluster issuer, and have setup CIS webhook in your existing instance, you will need also to reinstall CIS webhook in ibm-common-services namespace. Therefore, you might want to use the following playbook sample to accomplish this. - hosts: localhost any_errors_fatal: true vars: is_suite_upgrade: true # this will tell 'ibm.mas_devops.suite_dns' to reinstall cis webhook under 'ibm-common-services' ns if 'MAS_CUSTOM_CLUSTER_ISSUER' is set roles: - ibm.mas_devops.suite_upgrade_check - ibm.mas_devops.cert_manager_upgrade - ibm.mas_devops.suite_dns - ibm.mas_devops.cert_manager_upgrade_check","title":"Example Playbook"},{"location":"roles/cluster_monitoring/","text":"cluster_monitoring \uf0c1 Configure both prometheus cluster monitoring and prometheus user workload cluster monitoring with persistant storage. Also configures the community grafana operator v4 and deploys a grafana instance along with a datasource to prometheus. The grafana operator will scan for dashboards across the whole cluster so that it can import any dashbaords from Maximo Application Suite. The namespace grafana is installed to defaults to grafana but can be changed using the role variables below. The credentials for the grafana admin user are stored in grafana-admin-credentials secret in the grafana namespace. A route is created in the grafana namespace to allow access to the grafana UI. Role Variables \uf0c1 prometheus_retention_period \uf0c1 Adjust the retention period for Prometheus metrics, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Optional Environment Variable: PROMETHEUS_RETENTION_PERIOD Default Value: 15d prometheus_storage_class \uf0c1 Declare the storage class for Prometheus' metrics data persistent volume. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: PROMETHEUS_STORAGE_CLASS Default Value: ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium (if available) prometheus_storage_size \uf0c1 Adjust the size of the volume used to store metrics, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Optional Environment Variable: PROMETHEUS_STORAGE_SIZE Default Value: 300Gi prometheus_alertmgr_storage_class \uf0c1 Declare the storage class for AlertManager's persistent volume. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_CLASS Default Value: ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium (if available) prometheus_alertmgr_storage_size \uf0c1 Adjust the size of the volume used by AlertManager, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Optional Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_SIZE Default Value: 20Gi prometheus_userworkload_retention_period \uf0c1 Adjust the retention period for User Workload Prometheus metrics, this parameter applies only to the User Workload Prometheus instance. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_RETENTION_PERIOD Default Value: 15d prometheus_userworkload_storage_class \uf0c1 Declare the storage class for User Workload Prometheus' metrics data persistent volume. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_CLASS Default Value: PROMETHEUS_STORAGE_CLASS prometheus_userworkload_storage_size \uf0c1 Adjust the size of the volume used to store User Workload metrics. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_SIZE Default Value: 300Gi grafana_instance_storage_class \uf0c1 Declare the storage class for Grafana Instance user data persistent volume. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: GRAFANA_INSTANCE_STORAGE_CLASS Default Value: ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium (if available) grafana_instance_storage_size \uf0c1 Adjust the size of the volume used to store Grafana user data. Optional Environment Variable: GRAFANA_INSTANCE_STORAGE_SIZE Default Value: 10Gi grafana_namespace \uf0c1 Sets the namespace to install the grafana operator and grafana instance Optional Environment Variable: GRAFANA_NAMESPACE Default Value: grafana Example Playbook \uf0c1 - hosts: localhost vars: prometheus_storage_class: \"ibmc-block-gold\" prometheus_alertmgr_storage_class: \"ibmc-file-gold-gid\" roles: - ibm.mas_devops.cluster_monitoring Tekton Task \uf0c1 Start a run of the mas-devops-cluster-monitoring Task as below, you must have already prepared the namespace: cat <<EOF | oc create -f - apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: generateName: mas-devops-cluster-monitoring- spec: taskRef: kind: Task name: mas-devops-cluster-monitoring params: - name: prometheus_storage_class value: \"ibmc-block-gold\" - name: prometheus_alertmgr_storage_class value: \"ibmc-file-gold-gid\" - name: prometheus_userworkload_storage_class value: \"ibmc-block-gold\" resources: {} serviceAccountName: pipeline timeout: 24h0m0s EOF License \uf0c1 EPL-2.0","title":"cluster_monitoring"},{"location":"roles/cluster_monitoring/#cluster_monitoring","text":"Configure both prometheus cluster monitoring and prometheus user workload cluster monitoring with persistant storage. Also configures the community grafana operator v4 and deploys a grafana instance along with a datasource to prometheus. The grafana operator will scan for dashboards across the whole cluster so that it can import any dashbaords from Maximo Application Suite. The namespace grafana is installed to defaults to grafana but can be changed using the role variables below. The credentials for the grafana admin user are stored in grafana-admin-credentials secret in the grafana namespace. A route is created in the grafana namespace to allow access to the grafana UI.","title":"cluster_monitoring"},{"location":"roles/cluster_monitoring/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cluster_monitoring/#prometheus_retention_period","text":"Adjust the retention period for Prometheus metrics, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Optional Environment Variable: PROMETHEUS_RETENTION_PERIOD Default Value: 15d","title":"prometheus_retention_period"},{"location":"roles/cluster_monitoring/#prometheus_storage_class","text":"Declare the storage class for Prometheus' metrics data persistent volume. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: PROMETHEUS_STORAGE_CLASS Default Value: ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium (if available)","title":"prometheus_storage_class"},{"location":"roles/cluster_monitoring/#prometheus_storage_size","text":"Adjust the size of the volume used to store metrics, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Optional Environment Variable: PROMETHEUS_STORAGE_SIZE Default Value: 300Gi","title":"prometheus_storage_size"},{"location":"roles/cluster_monitoring/#prometheus_alertmgr_storage_class","text":"Declare the storage class for AlertManager's persistent volume. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_CLASS Default Value: ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium (if available)","title":"prometheus_alertmgr_storage_class"},{"location":"roles/cluster_monitoring/#prometheus_alertmgr_storage_size","text":"Adjust the size of the volume used by AlertManager, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Optional Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_SIZE Default Value: 20Gi","title":"prometheus_alertmgr_storage_size"},{"location":"roles/cluster_monitoring/#prometheus_userworkload_retention_period","text":"Adjust the retention period for User Workload Prometheus metrics, this parameter applies only to the User Workload Prometheus instance. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_RETENTION_PERIOD Default Value: 15d","title":"prometheus_userworkload_retention_period"},{"location":"roles/cluster_monitoring/#prometheus_userworkload_storage_class","text":"Declare the storage class for User Workload Prometheus' metrics data persistent volume. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_CLASS Default Value: PROMETHEUS_STORAGE_CLASS","title":"prometheus_userworkload_storage_class"},{"location":"roles/cluster_monitoring/#prometheus_userworkload_storage_size","text":"Adjust the size of the volume used to store User Workload metrics. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_SIZE Default Value: 300Gi","title":"prometheus_userworkload_storage_size"},{"location":"roles/cluster_monitoring/#grafana_instance_storage_class","text":"Declare the storage class for Grafana Instance user data persistent volume. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: GRAFANA_INSTANCE_STORAGE_CLASS Default Value: ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium (if available)","title":"grafana_instance_storage_class"},{"location":"roles/cluster_monitoring/#grafana_instance_storage_size","text":"Adjust the size of the volume used to store Grafana user data. Optional Environment Variable: GRAFANA_INSTANCE_STORAGE_SIZE Default Value: 10Gi","title":"grafana_instance_storage_size"},{"location":"roles/cluster_monitoring/#grafana_namespace","text":"Sets the namespace to install the grafana operator and grafana instance Optional Environment Variable: GRAFANA_NAMESPACE Default Value: grafana","title":"grafana_namespace"},{"location":"roles/cluster_monitoring/#example-playbook","text":"- hosts: localhost vars: prometheus_storage_class: \"ibmc-block-gold\" prometheus_alertmgr_storage_class: \"ibmc-file-gold-gid\" roles: - ibm.mas_devops.cluster_monitoring","title":"Example Playbook"},{"location":"roles/cluster_monitoring/#tekton-task","text":"Start a run of the mas-devops-cluster-monitoring Task as below, you must have already prepared the namespace: cat <<EOF | oc create -f - apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: generateName: mas-devops-cluster-monitoring- spec: taskRef: kind: Task name: mas-devops-cluster-monitoring params: - name: prometheus_storage_class value: \"ibmc-block-gold\" - name: prometheus_alertmgr_storage_class value: \"ibmc-file-gold-gid\" - name: prometheus_userworkload_storage_class value: \"ibmc-block-gold\" resources: {} serviceAccountName: pipeline timeout: 24h0m0s EOF","title":"Tekton Task"},{"location":"roles/cluster_monitoring/#license","text":"EPL-2.0","title":"License"},{"location":"roles/common_services/","text":"common_services \uf0c1 This role will install the following operators into the ibm-common-services namespace of the target cluster: IBM Cloud Pak Foundational Services IBM NamespaceScope Operator Operand Deployment Lifecycle Manager Also, an operator group will be created in the namespace if one does not already exist. Prerequisites \uf0c1 To run this role successfully you must have already installed a CatalogSource that contains IBM Cloud Pak Foundational Services, this can be achieved using the ibm_catalogs role in this collection. By default a catalog source of ibm-operator-catalog will be expected, but this can be customized using the common_services_catalog_source variable. Role Variables \uf0c1 common_services_catalog_source \uf0c1 Used to override the operator catalog source used when creating the ibm-common-service-operator subscription. Optional Environment Variable: COMMON_SERVICES_CATALOG_SOURCE Default Value: ibm-operator-catalog Example Playbook \uf0c1 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.common_services Run Role Playbook \uf0c1 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ROLE_NAME=common_services ansible-playbook ibm.mas_devops.run_role License \uf0c1 EPL-2.0","title":"common_services"},{"location":"roles/common_services/#common_services","text":"This role will install the following operators into the ibm-common-services namespace of the target cluster: IBM Cloud Pak Foundational Services IBM NamespaceScope Operator Operand Deployment Lifecycle Manager Also, an operator group will be created in the namespace if one does not already exist.","title":"common_services"},{"location":"roles/common_services/#prerequisites","text":"To run this role successfully you must have already installed a CatalogSource that contains IBM Cloud Pak Foundational Services, this can be achieved using the ibm_catalogs role in this collection. By default a catalog source of ibm-operator-catalog will be expected, but this can be customized using the common_services_catalog_source variable.","title":"Prerequisites"},{"location":"roles/common_services/#role-variables","text":"","title":"Role Variables"},{"location":"roles/common_services/#common_services_catalog_source","text":"Used to override the operator catalog source used when creating the ibm-common-service-operator subscription. Optional Environment Variable: COMMON_SERVICES_CATALOG_SOURCE Default Value: ibm-operator-catalog","title":"common_services_catalog_source"},{"location":"roles/common_services/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.common_services","title":"Example Playbook"},{"location":"roles/common_services/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ROLE_NAME=common_services ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/common_services/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cos/","text":"cos \uf0c1 This role provides support for Configuring Cloud Object Storage in MAS. It currently supports two providers: In-cluster Ceph Object Storage leveraging OpenShift Container Storage IBM Cloud Object Storage Currently this role only supports generating a system-scoped ObjectStorageCfg resource, but the generated file can be modified if you wish to use other scopes. Role Variables \uf0c1 cos_type \uf0c1 Required. Which COS provider to use; can be set to either ibm for IBM Cloud Object Storage or ocs for OpenShift Container Storage Environment Variable: COS_TYPE Default Value: None cos_instance_name \uf0c1 Provide an optional name for the Object Storage instance. This is only used when cos_type is set to ibm for IBM Cloud Object Storage. Environment Variable: COS_INSTANCE_NAME Default Value: Object Storage for MAS , if mas_instance_id is set the MAS instance ID will be appended to this name. ibmcloud_apikey \uf0c1 Required if cos_type is set to ibm . Provide your IBM Cloud API Key. Environment Variable: IBMCLOUD_APIKEY Default Value: None ibmcloud_resourcegroup \uf0c1 Only used when cos_type is set to ibm . Provide the name of the resource group which will own the COS instance. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default mas_instance_id \uf0c1 The instance ID of Maximo Application Suite that the ObjectStorageCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a ObjectStorageCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \uf0c1 Local directory to save the generated ObjectStorageCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a ObjectStorageCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None cluster ingres tls secret name \uf0c1 Specify the name of the cluster's ingres tls secret which contains the default router certificate. Optional Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default Value: router-certs-default Example Playbook \uf0c1 Create the Ceph Object store on the existing OCS cluster and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ocs mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos_setup Create the IBM Cloud Object storage Instance and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ibm # MAS instance and config dir mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos_setup License \uf0c1 EPL-2.0","title":"cos"},{"location":"roles/cos/#cos","text":"This role provides support for Configuring Cloud Object Storage in MAS. It currently supports two providers: In-cluster Ceph Object Storage leveraging OpenShift Container Storage IBM Cloud Object Storage Currently this role only supports generating a system-scoped ObjectStorageCfg resource, but the generated file can be modified if you wish to use other scopes.","title":"cos"},{"location":"roles/cos/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cos/#cos_type","text":"Required. Which COS provider to use; can be set to either ibm for IBM Cloud Object Storage or ocs for OpenShift Container Storage Environment Variable: COS_TYPE Default Value: None","title":"cos_type"},{"location":"roles/cos/#cos_instance_name","text":"Provide an optional name for the Object Storage instance. This is only used when cos_type is set to ibm for IBM Cloud Object Storage. Environment Variable: COS_INSTANCE_NAME Default Value: Object Storage for MAS , if mas_instance_id is set the MAS instance ID will be appended to this name.","title":"cos_instance_name"},{"location":"roles/cos/#ibmcloud_apikey","text":"Required if cos_type is set to ibm . Provide your IBM Cloud API Key. Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/cos/#ibmcloud_resourcegroup","text":"Only used when cos_type is set to ibm . Provide the name of the resource group which will own the COS instance. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default","title":"ibmcloud_resourcegroup"},{"location":"roles/cos/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the ObjectStorageCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a ObjectStorageCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/cos/#mas_config_dir","text":"Local directory to save the generated ObjectStorageCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a ObjectStorageCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/cos/#cluster-ingres-tls-secret-name","text":"Specify the name of the cluster's ingres tls secret which contains the default router certificate. Optional Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default Value: router-certs-default","title":"cluster ingres tls secret name"},{"location":"roles/cos/#example-playbook","text":"Create the Ceph Object store on the existing OCS cluster and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ocs mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos_setup Create the IBM Cloud Object storage Instance and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ibm # MAS instance and config dir mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos_setup","title":"Example Playbook"},{"location":"roles/cos/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cp4d/","text":"cp4d \uf0c1 This role installs or upgrades IBM Cloud Pak for Data Operator in the target cluster. Currently supported Cloud Pak for Data release versions are: 4.0.9 4.5.0 4.5.1 4.5.2 The role will automatically install or upgrade (if targeted to an existing CPD deployment) the corresponding Zen version associated to the chosen Cloud Pak for Data release, for example: Cloud Pak for Data release version 4.0.9 installs Zen/Control Plane version 4.4.4 . Cloud Pak for Data release version 4.5.0 installs Zen/Control Plane version 4.5.0 . Cloud Pak for Data release version 4.5.1 installs Zen/Control Plane version 4.5.0 . Cloud Pak for Data release version 4.5.2 installs Zen/Control Plane version 4.7.0 . For more information about CPD versioning, see IBM Cloud Pak for Data Operator and operand versions Upgrade \uf0c1 This role also supports seamlessly CPD control plane (or also called Zen service) minor version upgrades (CPD 4.0.9 -> CPD 4.5.0), and patch version upgrades (CPD 4.5.0 -> CPD 4.5.x). All you need to do is to define cpd_product_version variable to the version you target to upgrade and run this role against an existing CPD instance. For more information about IBM Cloud Pak for Data upgrade process, refer to the CPD official documentation . The role assumes that you have already installed the IBM Operator Catalog and configured IBM Cloud Pak Foundational services in the target cluster. These actions are performed by the ibm_catalogs common_services roles in this collection. Cloud Pak for Data will be configured as a specialized installation Info A specialized installation allows a user with project administrator permissions to install the software after a cluster administrator completes the initial cluster setup. A specialized installation also facilitates strict division between Red Hat OpenShift Container Platform projects (Kubernetes namespaces). In a specialized installation, the IBM Cloud Pak foundational services operators are installed in the ibm-common-services project and the Cloud Pak for Data operators are installed in a separate project (typically cpd-operators). Each project has a dedicated: Operator group, which specifies the OwnNamespace installation mode NamespaceScope Operator, which allows the operators in the project to manage operators and service workloads in specific projects In this way, you can specify different settings for the IBM Cloud Pak foundational services and for the Cloud Pak for Data operators. Cloud Pak for Data is made up of many moving parts across multiple namespaces. In the ibm-common-services namespace: oc -n ibm-common-services get deployments NAME READY UP-TO-DATE AVAILABLE AGE cert-manager-cainjector 1/1 1 1 85m cert-manager-controller 1/1 1 1 85m cert-manager-webhook 1/1 1 1 85m configmap-watcher 1/1 1 1 85m ibm-cert-manager-operator 1/1 1 1 87m ibm-common-service-operator 1/1 1 1 92m ibm-common-service-webhook 1/1 1 1 91m ibm-namespace-scope-operator 1/1 1 1 91m ibm-zen-operator 1/1 1 1 87m meta-api-deploy 1/1 1 1 86m operand-deployment-lifecycle-manager 1/1 1 1 90m secretshare 1/1 1 1 91m In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 87m ibm-common-service-operator 1/1 1 1 87m ibm-namespace-scope-operator 1/1 1 1 87m In the ibm-cpd namespace: oc -n ibm-cpd get zenservice,ibmcpd,deployments,sts NAME AGE zenservice.zen.cpd.ibm.com/lite-cr 81m NAME AGE ibmcpd.cpd.ibm.com/ibmcpd 85m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ibm-nginx 3/3 3 3 62m deployment.apps/usermgmt 3/3 3 3 64m deployment.apps/zen-audit 1/1 1 1 56m deployment.apps/zen-core 3/3 3 3 55m deployment.apps/zen-core-api 3/3 3 3 55m deployment.apps/zen-data-sorcerer 2/2 2 2 48m deployment.apps/zen-watchdog 1/1 1 1 48m deployment.apps/zen-watcher 1/1 1 1 55m NAME READY AGE statefulset.apps/dsx-influxdb 1/1 51m statefulset.apps/zen-metastoredb 3/3 68m Tip You can retrieve the Cloud Pak for Data password from the admin-user-details secret: oc -n ibm-cpd get secret admin-user-details -o jsonpath=\"{.data.initial_admin_password}\" | base64 -d Role Variables \uf0c1 cpd_product_version \uf0c1 Defines the IBM Cloud Pak for Data release version to be installed. Required Environment Variable: CPD_PRODUCT_VERSION Default: 4.0.9 ibm_entitlement_key \uf0c1 Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None cpd_entitlement_key \uf0c1 An IBM entitlement key specific for Cloud Pak for Data installation, primarily used to override ibm_entitlement_key in development. Optional Environment Variable: CPD_ENTITLEMENT_KEY Default: None cpd_primary_storage_class \uf0c1 Primary storage class for Cloud Pak for Data. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: CPD_PRIMARY_STORAGE_CLASS Default Value: ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium (if available) cpd_metadata_storage_class \uf0c1 Storage class for the Cloud Pak for Data Zen meta database. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: CPD_METADATA_STORAGE_CLASS Default Value: ibmc-block-gold , ocs-storagecluster-ceph-rbd , or managed-premium (if available) cpd_operators_namespace \uf0c1 Namespace where Cloud Pak for Data operators will be installed. Optional Environment Variable: CPD_OPERATORS_NAMESPACE Default Value: ibm-cpd-operators cpd_instance_namespace \uf0c1 Namespace that the Cloud Pak for Data operators will be configured to watch. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default Value: ibm-cpd Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: cpd_product_version: 4.5.0 cpd_primary_storage_class: ibmc-file-gold-gid cpd_metadata_storage_class: ibmc-block-gold roles: - ibm.mas_devops.cp4d License \uf0c1 EPL-2.0","title":"cp4d"},{"location":"roles/cp4d/#cp4d","text":"This role installs or upgrades IBM Cloud Pak for Data Operator in the target cluster. Currently supported Cloud Pak for Data release versions are: 4.0.9 4.5.0 4.5.1 4.5.2 The role will automatically install or upgrade (if targeted to an existing CPD deployment) the corresponding Zen version associated to the chosen Cloud Pak for Data release, for example: Cloud Pak for Data release version 4.0.9 installs Zen/Control Plane version 4.4.4 . Cloud Pak for Data release version 4.5.0 installs Zen/Control Plane version 4.5.0 . Cloud Pak for Data release version 4.5.1 installs Zen/Control Plane version 4.5.0 . Cloud Pak for Data release version 4.5.2 installs Zen/Control Plane version 4.7.0 . For more information about CPD versioning, see IBM Cloud Pak for Data Operator and operand versions","title":"cp4d"},{"location":"roles/cp4d/#upgrade","text":"This role also supports seamlessly CPD control plane (or also called Zen service) minor version upgrades (CPD 4.0.9 -> CPD 4.5.0), and patch version upgrades (CPD 4.5.0 -> CPD 4.5.x). All you need to do is to define cpd_product_version variable to the version you target to upgrade and run this role against an existing CPD instance. For more information about IBM Cloud Pak for Data upgrade process, refer to the CPD official documentation . The role assumes that you have already installed the IBM Operator Catalog and configured IBM Cloud Pak Foundational services in the target cluster. These actions are performed by the ibm_catalogs common_services roles in this collection. Cloud Pak for Data will be configured as a specialized installation Info A specialized installation allows a user with project administrator permissions to install the software after a cluster administrator completes the initial cluster setup. A specialized installation also facilitates strict division between Red Hat OpenShift Container Platform projects (Kubernetes namespaces). In a specialized installation, the IBM Cloud Pak foundational services operators are installed in the ibm-common-services project and the Cloud Pak for Data operators are installed in a separate project (typically cpd-operators). Each project has a dedicated: Operator group, which specifies the OwnNamespace installation mode NamespaceScope Operator, which allows the operators in the project to manage operators and service workloads in specific projects In this way, you can specify different settings for the IBM Cloud Pak foundational services and for the Cloud Pak for Data operators. Cloud Pak for Data is made up of many moving parts across multiple namespaces. In the ibm-common-services namespace: oc -n ibm-common-services get deployments NAME READY UP-TO-DATE AVAILABLE AGE cert-manager-cainjector 1/1 1 1 85m cert-manager-controller 1/1 1 1 85m cert-manager-webhook 1/1 1 1 85m configmap-watcher 1/1 1 1 85m ibm-cert-manager-operator 1/1 1 1 87m ibm-common-service-operator 1/1 1 1 92m ibm-common-service-webhook 1/1 1 1 91m ibm-namespace-scope-operator 1/1 1 1 91m ibm-zen-operator 1/1 1 1 87m meta-api-deploy 1/1 1 1 86m operand-deployment-lifecycle-manager 1/1 1 1 90m secretshare 1/1 1 1 91m In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 87m ibm-common-service-operator 1/1 1 1 87m ibm-namespace-scope-operator 1/1 1 1 87m In the ibm-cpd namespace: oc -n ibm-cpd get zenservice,ibmcpd,deployments,sts NAME AGE zenservice.zen.cpd.ibm.com/lite-cr 81m NAME AGE ibmcpd.cpd.ibm.com/ibmcpd 85m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ibm-nginx 3/3 3 3 62m deployment.apps/usermgmt 3/3 3 3 64m deployment.apps/zen-audit 1/1 1 1 56m deployment.apps/zen-core 3/3 3 3 55m deployment.apps/zen-core-api 3/3 3 3 55m deployment.apps/zen-data-sorcerer 2/2 2 2 48m deployment.apps/zen-watchdog 1/1 1 1 48m deployment.apps/zen-watcher 1/1 1 1 55m NAME READY AGE statefulset.apps/dsx-influxdb 1/1 51m statefulset.apps/zen-metastoredb 3/3 68m Tip You can retrieve the Cloud Pak for Data password from the admin-user-details secret: oc -n ibm-cpd get secret admin-user-details -o jsonpath=\"{.data.initial_admin_password}\" | base64 -d","title":"Upgrade"},{"location":"roles/cp4d/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cp4d/#cpd_product_version","text":"Defines the IBM Cloud Pak for Data release version to be installed. Required Environment Variable: CPD_PRODUCT_VERSION Default: 4.0.9","title":"cpd_product_version"},{"location":"roles/cp4d/#ibm_entitlement_key","text":"Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None","title":"ibm_entitlement_key"},{"location":"roles/cp4d/#cpd_entitlement_key","text":"An IBM entitlement key specific for Cloud Pak for Data installation, primarily used to override ibm_entitlement_key in development. Optional Environment Variable: CPD_ENTITLEMENT_KEY Default: None","title":"cpd_entitlement_key"},{"location":"roles/cp4d/#cpd_primary_storage_class","text":"Primary storage class for Cloud Pak for Data. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: CPD_PRIMARY_STORAGE_CLASS Default Value: ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium (if available)","title":"cpd_primary_storage_class"},{"location":"roles/cp4d/#cpd_metadata_storage_class","text":"Storage class for the Cloud Pak for Data Zen meta database. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: CPD_METADATA_STORAGE_CLASS Default Value: ibmc-block-gold , ocs-storagecluster-ceph-rbd , or managed-premium (if available)","title":"cpd_metadata_storage_class"},{"location":"roles/cp4d/#cpd_operators_namespace","text":"Namespace where Cloud Pak for Data operators will be installed. Optional Environment Variable: CPD_OPERATORS_NAMESPACE Default Value: ibm-cpd-operators","title":"cpd_operators_namespace"},{"location":"roles/cp4d/#cpd_instance_namespace","text":"Namespace that the Cloud Pak for Data operators will be configured to watch. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default Value: ibm-cpd","title":"cpd_instance_namespace"},{"location":"roles/cp4d/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: cpd_product_version: 4.5.0 cpd_primary_storage_class: ibmc-file-gold-gid cpd_metadata_storage_class: ibmc-block-gold roles: - ibm.mas_devops.cp4d","title":"Example Playbook"},{"location":"roles/cp4d/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cp4d_hack_worker_nodes/","text":"cp4d_hack_worker_nodes \uf0c1 This should be executed as part of cluster prepararion if you want to use CP4D v4. It will reboot all worker nodes, causing disruption to the entire cluster and everything running on it we do not include this as part of the normal flow because, well it shouldn't be necessary to reboot worker nodes to install containerized software. Hopefully this is just an example of poor documentation and there's a simple alternative that we can implement to remove this role. For more information, refer to https://cloud.ibm.com/docs/openshift?topic=openshift-registry#cluster_global_pull_secret Role Variables \uf0c1 cluster_type \uf0c1 Required. Note that only supported value at present is roks . Environment Variable: CLUSTER_TYPE Default Value: None cluster_name \uf0c1 Required. The name of the ROKS cluster that we are going to apply the CP4D hack to. Environment Variable: CLUSTER_NAME Default Value: None ibmcloud_apikey \uf0c1 Required. Provide your IBM Cloud API Key, this will be used to query the status of the cluster and issue the node restart commands. Environment Variable: IBMCLOUD_APIKEY Default Value: None cpd_entitlement_key \uf0c1 Required. Provide your IBM Entitlement Key. Environment Variable: CPD_ENTITLEMENT_KEY Default Value: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" cpd_entitlement_key: \"{{ lookup('env', 'CPD_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.cp4d_hack_worker_nodes License \uf0c1 EPL-2.0","title":"cp4d_hack_worker_nodes"},{"location":"roles/cp4d_hack_worker_nodes/#cp4d_hack_worker_nodes","text":"This should be executed as part of cluster prepararion if you want to use CP4D v4. It will reboot all worker nodes, causing disruption to the entire cluster and everything running on it we do not include this as part of the normal flow because, well it shouldn't be necessary to reboot worker nodes to install containerized software. Hopefully this is just an example of poor documentation and there's a simple alternative that we can implement to remove this role. For more information, refer to https://cloud.ibm.com/docs/openshift?topic=openshift-registry#cluster_global_pull_secret","title":"cp4d_hack_worker_nodes"},{"location":"roles/cp4d_hack_worker_nodes/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cp4d_hack_worker_nodes/#cluster_type","text":"Required. Note that only supported value at present is roks . Environment Variable: CLUSTER_TYPE Default Value: None","title":"cluster_type"},{"location":"roles/cp4d_hack_worker_nodes/#cluster_name","text":"Required. The name of the ROKS cluster that we are going to apply the CP4D hack to. Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/cp4d_hack_worker_nodes/#ibmcloud_apikey","text":"Required. Provide your IBM Cloud API Key, this will be used to query the status of the cluster and issue the node restart commands. Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/cp4d_hack_worker_nodes/#cpd_entitlement_key","text":"Required. Provide your IBM Entitlement Key. Environment Variable: CPD_ENTITLEMENT_KEY Default Value: None","title":"cpd_entitlement_key"},{"location":"roles/cp4d_hack_worker_nodes/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" cpd_entitlement_key: \"{{ lookup('env', 'CPD_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.cp4d_hack_worker_nodes","title":"Example Playbook"},{"location":"roles/cp4d_hack_worker_nodes/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cp4d_service/","text":"cp4d_service \uf0c1 Install or upgrade a chosen CloudPak for Data service. Currently supported Cloud Pak for Data release versions supported are: 4.0.9 4.5.0 4.5.1 4.5.2 The role will automatically install the corresponding CPD service operator channel and custom resource version associated to the chosen Cloud Pak for Data release version. For more information about the specific CPD services channels and versions associated to a particular Cloud Pak for Data release can be found here . Services Supported \uf0c1 These services can be deployed and configured using this role: Watson Studio required by Predict and Health & Predict Utilities Watson Machine Learning required by Predict Analytics Service (Apache Spark) required by Predict Watson OpenScale an optional dependency for Predict Watson Discovery required by Assist Upgrade \uf0c1 This role also supports seamlessly CPD services minor version upgrades (CPD 4.0.9 -> CPD 4.5.0), and patch version upgrades (CPD 4.5.0 -> CPD 4.5.x). All you need to do is to define cpd_product_version variable to the version you target to upgrade and run this role for a particular CPD service. It's important that before you upgrade CPD services, the CPD Control Plane/Zen is also upgraded to the same release version. For more information about IBM Cloud Pak for Data upgrade process, refer to the CPD official documentation . Application Support For more information on how Predict and HP Utilities make use of Watson Studio, refer to Predict/HP Utilities documentation Warning The reconcile of many CP4D resources will be marked as Failed multiple times during initial installation, these are misleading status updates , the install is just really slow and the operators can not properly handle this. For example, if you are watching the install of CCS you will see that each rabbitmq-ha pod takes 10-15 minutes to start up and it looks like there is a problem because the pod log will just stop at a certain point. If you see something like this as the last message in the pod log WAL: ra_log_wal init, open tbls: ra_log_open_mem_tables, closed tbls: ra_log_closed_mem_tables be assured that there's nothing wrong, it's just there's a long delay between that message and the next ( starting system coordination ) being logged. Watson Studio \uf0c1 Subscriptions related to Watson Studio: cpd-platform-operator ibm-cpd-wsl ibm-cpd-ccs ibm-cpd-datarefinery ibm-cpd-ws-runtimes Watson Studio is made up of many moving parts across multiple namespaces. In the ibm-common-services namespace: 15 workloads / 12 pods 0.126 CPU usage / 1.11 CPU requests / 3.57 CPU limit (11% utilization) 773.8 MiB memory usage, 2.27 GiB memory requests / 5.72 GiB memory limit (33% utilization) oc -n ibm-common-services get deployments NAME READY UP-TO-DATE AVAILABLE AGE cert-manager-cainjector 1/1 1 1 4h57m cert-manager-controller 1/1 1 1 4h57m cert-manager-webhook 1/1 1 1 4h57m configmap-watcher 1/1 1 1 4h57m ibm-cert-manager-operator 1/1 1 1 4h58m ibm-common-service-operator 1/1 1 1 5h3m ibm-common-service-webhook 1/1 1 1 5h2m ibm-namespace-scope-operator 1/1 1 1 5h3m ibm-zen-operator 1/1 1 1 4h58m meta-api-deploy 1/1 1 1 4h57m operand-deployment-lifecycle-manager 1/1 1 1 5h2m secretshare 1/1 1 1 5h2m In the ibm-cpd-operators namespace: 7 workloads / 7 pods 0.007 CPU usage / 0.7 CPU requests / 3.75 CPU limit (1% utilization) 263.4 MiB memory usage, 1.64 GiB memory requests /6.5 GiB memory limit (15% utilization) oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 5h ibm-common-service-operator 1/1 1 1 5h ibm-cpd-ccs-operator 1/1 1 1 3h43m ibm-cpd-datarefinery-operator 1/1 1 1 134m ibm-cpd-ws-operator 1/1 1 1 3h45m ibm-cpd-ws-runtimes-operator 1/1 1 1 118m ibm-namespace-scope-operator 1/1 1 1 5h In the ibm-cpd namespace: 51 workloads / 101 pods 1.1 CPU usage / 18.72 CPU requests / 86.7 CPU limit (5% utilization) 16.46 GiB memory usage, 33.04 GiB memory requests / 175.7 GiB memory limit (50% utilization) oc -n ibm-cpd get ccs,ws,datarefinery,notebookruntimes,deployments,sts NAME VERSION RECONCILED STATUS AGE ccs.ccs.cpd.ibm.com/ccs-cr 4.0.9 4.0.9 Completed 3h42m NAME VERSION RECONCILED STATUS AGE ws.ws.cpd.ibm.com/ws-cr 4.0.9 4.0.9 Completed 3h45m NAME VERSION STATUS AGE datarefinery.datarefinery.cpd.ibm.com/datarefinery-sample 4.0.9 Completed 133m NAME VERSION RECONCILED STATUS AGE notebookruntime.ws.cpd.ibm.com/ibm-cpd-ws-runtime-py39 4.0.9 Completed 116m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/asset-files-api 1/1 1 1 159m deployment.apps/ax-cdsx-jupyter-notebooks-converter-deploy 1/1 1 1 111m deployment.apps/ax-cdsx-notebooks-job-manager-deploy 1/1 1 1 111m deployment.apps/ax-environments-api-deploy 1/1 1 1 144m deployment.apps/ax-environments-ui-deploy 1/1 1 1 144m deployment.apps/ax-wdp-notebooks-api-deploy 1/1 1 1 111m deployment.apps/ax-ws-notebooks-ui-deploy 1/1 1 1 111m deployment.apps/catalog-api 2/2 2 2 167m deployment.apps/dap-dashboards-api 1/1 1 1 159m deployment.apps/dataview-api-service 1/1 1 1 138m deployment.apps/dc-main 1/1 1 1 167m deployment.apps/event-logger-api 1/1 1 1 159m deployment.apps/ibm-0100-model-viewer-prod 1/1 1 1 110m deployment.apps/ibm-nginx 3/3 3 3 4h37m deployment.apps/jobs-api 1/1 1 1 155m deployment.apps/jobs-ui 1/1 1 1 155m deployment.apps/ngp-projects-api 1/1 1 1 159m deployment.apps/portal-catalog 1/1 1 1 166m deployment.apps/portal-common-api 1/1 1 1 159m deployment.apps/portal-dashboards 1/1 1 1 159m deployment.apps/portal-job-manager 1/1 1 1 159m deployment.apps/portal-main 1/1 1 1 159m deployment.apps/portal-ml-dl 1/1 1 1 111m deployment.apps/portal-notifications 1/1 1 1 159m deployment.apps/portal-projects 1/1 1 1 159m deployment.apps/redis-ha-haproxy 1/1 1 1 174m deployment.apps/runtime-assemblies-operator 1/1 1 1 151m deployment.apps/runtime-manager-api 1/1 1 1 147m deployment.apps/spaces 1/1 1 1 142m deployment.apps/spawner-api 1/1 1 1 146m deployment.apps/usermgmt 3/3 3 3 4h39m deployment.apps/wdp-connect-connection 1/1 1 1 167m deployment.apps/wdp-connect-connector 1/1 1 1 167m deployment.apps/wdp-connect-flight 1/1 1 1 167m deployment.apps/wdp-dataprep 1/1 1 1 126m deployment.apps/wdp-dataview 1/1 1 1 138m deployment.apps/wdp-shaper 1/1 1 1 128m deployment.apps/wkc-search 1/1 1 1 167m deployment.apps/wml-main 1/1 1 1 143m deployment.apps/zen-audit 1/1 1 1 4h33m deployment.apps/zen-core 3/3 3 3 4h32m deployment.apps/zen-core-api 3/3 3 3 4h32m deployment.apps/zen-data-sorcerer 2/2 2 2 4h25m deployment.apps/zen-watchdog 1/1 1 1 4h25m deployment.apps/zen-watcher 1/1 1 1 4h32m NAME READY AGE statefulset.apps/dsx-influxdb 1/1 4h28m statefulset.apps/elasticsearch-master 3/3 171m statefulset.apps/rabbitmq-ha 3/3 3h35m statefulset.apps/redis-ha-server 3/3 3h1m statefulset.apps/wdp-couchdb 3/3 3h38m statefulset.apps/zen-metastoredb 3/3 4h43m Watson Machine Learning \uf0c1 Subscriptions related to Watson Machine Learning: cpd-platform-operator ibm-cpd-wml ibm-cpd-ccs Watson Machine Learning is made up of many moving parts across multiple namespaces. In the ibm-common-services namespace: 15 workloads / 12 pods 0.126 CPU usage / 1.11 CPU requests / 3.57 CPU limit (11% utilization) 773.8 MiB memory usage, 2.27 GiB memory requests / 5.72 GiB memory limit (33% utilization) oc -n ibm-common-services get deployments NAME READY UP-TO-DATE AVAILABLE AGE cert-manager-cainjector 1/1 1 1 4h7m cert-manager-controller 1/1 1 1 4h7m cert-manager-webhook 1/1 1 1 4h7m configmap-watcher 1/1 1 1 4h7m ibm-cert-manager-operator 1/1 1 1 4h8m ibm-common-service-operator 1/1 1 1 4h14m ibm-common-service-webhook 1/1 1 1 4h12m ibm-namespace-scope-operator 1/1 1 1 4h13m ibm-zen-operator 1/1 1 1 4h8m meta-api-deploy 1/1 1 1 4h8m operand-deployment-lifecycle-manager 1/1 1 1 4h12m secretshare 1/1 1 1 4h12m In the ibm-cpd-operators namespace: 5 workloads / 5 pods 0.011 CPU usage / 0.5 CPU requests / 2.75 CPU limit (1% utilization) 177.4 MiB memory usage, 1.14 GiB memory requests / 4.5 GiB memory limit (15% utilization) oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 3h57m ibm-common-service-operator 1/1 1 1 3h57m ibm-cpd-ccs-operator 1/1 1 1 150m ibm-cpd-wml-operator 1/1 1 1 152m ibm-namespace-scope-operator 1/1 1 1 3h57m In the ibm-cpd namespace: 50 workloads / 103 pods 1.28 CPU usage / 18.07 CPU requests / 86.15 CPU limit (7% utilization) 16.96 GiB memory usage, 41.29 GiB memory requests / 184.5 GiB memory limit (41% utilization) oc -n ibm-cpd get ccs,wmlbase,deployments,sts NAME VERSION RECONCILED STATUS AGE ccs.ccs.cpd.ibm.com/ccs-cr 4.0.9 4.0.9 Completed 149m NAME VERSION BUILD STATUS AGE wmlbase.wml.cpd.ibm.com/wml-cr 4.0.9 4.0.10-3220 Completed 151m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/asset-files-api 1/1 1 1 80m deployment.apps/ax-environments-api-deploy 1/1 1 1 66m deployment.apps/ax-environments-ui-deploy 1/1 1 1 66m deployment.apps/catalog-api 2/2 2 2 89m deployment.apps/dap-dashboards-api 1/1 1 1 80m deployment.apps/dataview-api-service 1/1 1 1 62m deployment.apps/dc-main 1/1 1 1 88m deployment.apps/event-logger-api 1/1 1 1 80m deployment.apps/ibm-nginx 3/3 3 3 3h34m deployment.apps/jobs-api 1/1 1 1 76m deployment.apps/jobs-ui 1/1 1 1 76m deployment.apps/ngp-projects-api 1/1 1 1 80m deployment.apps/portal-catalog 1/1 1 1 88m deployment.apps/portal-common-api 1/1 1 1 80m deployment.apps/portal-dashboards 1/1 1 1 80m deployment.apps/portal-job-manager 1/1 1 1 80m deployment.apps/portal-main 1/1 1 1 80m deployment.apps/portal-notifications 1/1 1 1 80m deployment.apps/portal-projects 1/1 1 1 80m deployment.apps/redis-ha-haproxy 1/1 1 1 97m deployment.apps/runtime-assemblies-operator 1/1 1 1 72m deployment.apps/runtime-manager-api 1/1 1 1 70m deployment.apps/spaces 1/1 1 1 64m deployment.apps/spawner-api 1/1 1 1 69m deployment.apps/usermgmt 3/3 3 3 3h36m deployment.apps/wdp-connect-connection 1/1 1 1 88m deployment.apps/wdp-connect-connector 1/1 1 1 88m deployment.apps/wdp-connect-flight 1/1 1 1 88m deployment.apps/wdp-dataview 1/1 1 1 62m deployment.apps/wkc-search 1/1 1 1 88m deployment.apps/wml-deployment-envoy 1/1 1 1 37m deployment.apps/wml-main 1/1 1 1 65m deployment.apps/wml-repositoryv4 1/1 1 1 18m deployment.apps/wmltraining 1/1 1 1 13m deployment.apps/wmltrainingorchestrator 1/1 1 1 8m11s deployment.apps/zen-audit 1/1 1 1 3h28m deployment.apps/zen-core 3/3 3 3 3h27m deployment.apps/zen-core-api 3/3 3 3 3h27m deployment.apps/zen-data-sorcerer 2/2 2 2 3h20m deployment.apps/zen-watchdog 1/1 1 1 3h20m deployment.apps/zen-watcher 1/1 1 1 3h27m NAME READY AGE statefulset.apps/dsx-influxdb 1/1 3h23m statefulset.apps/elasticsearch-master 3/3 94m statefulset.apps/rabbitmq-ha 3/3 142m statefulset.apps/redis-ha-server 3/3 106m statefulset.apps/wdp-couchdb 3/3 145m statefulset.apps/wml-deployment-agent 1/1 32m statefulset.apps/wml-deployment-manager 1/1 24m statefulset.apps/wml-deployments-etcd 3/3 43m statefulset.apps/zen-metastoredb 3/3 3h40m Analytics Engine \uf0c1 Subscriptions related to Analytics Engine: cpd-platform-operator analyticsengine-operator Analytics Engine is made up of many moving parts across multiple namespaces. In the ibm-common-services namespace: 15 workloads / 12 pods 0.051 CPU usage / 1.11 CPU requests / 3.57 CPU limit (5% utilization) 774.7 MiB memory usage, 2.27 GiB memory requests / 5.72 GiB memory limit (33% utilization) oc -n ibm-common-services get deployments NAME READY UP-TO-DATE AVAILABLE AGE cert-manager-cainjector 1/1 1 1 126m cert-manager-controller 1/1 1 1 126m cert-manager-webhook 1/1 1 1 126m configmap-watcher 1/1 1 1 126m ibm-cert-manager-operator 1/1 1 1 126m ibm-common-service-operator 1/1 1 1 131m ibm-common-service-webhook 1/1 1 1 130m ibm-namespace-scope-operator 1/1 1 1 131m ibm-zen-operator 1/1 1 1 126m meta-api-deploy 1/1 1 1 126m operand-deployment-lifecycle-manager 1/1 1 1 130m secretshare 1/1 1 1 130m In the ibm-cpd-operators namespace: 4 workloads / 4 pods 0.003 CPU usage / 0.4 CPU requests / 2 CPU limit (1% utilization) 131.5 MiB memory usage, 912 MiB memory requests / 3 GiB memory limit (15% utilization) oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 128m ibm-common-service-operator 1/1 1 1 128m ibm-cpd-ae-operator 1/1 1 1 65m ibm-namespace-scope-operator 1/1 1 1 128m In the ibm-cpd namespace: 16 workloads / 60 pods 0.449 CPU usage / 8.06 CPU requests / 21.15 CPU limit (5% utilization) 3.15 GiB memory usage, 14.31 GiB memory requests / 32.71 GiB memory limit (22% utilization) oc -n ibm-cpd get analyticsengine,deployments,sts NAME VERSION RECONCILED STATUS AGE analyticsengine.ae.cpd.ibm.com/analyticsengine-sample 4.0.9 4.0.9 Completed 66m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ibm-nginx 3/3 3 3 93m deployment.apps/spark-hb-control-plane 1/1 1 1 62m deployment.apps/spark-hb-create-trust-store 1/1 1 1 64m deployment.apps/spark-hb-deployer-agent 1/1 1 1 62m deployment.apps/spark-hb-helm-repo 1/1 1 1 62m deployment.apps/spark-hb-nginx 1/1 1 1 62m deployment.apps/spark-hb-register-hb-dataplane 1/1 1 1 55m deployment.apps/usermgmt 3/3 3 3 94m deployment.apps/zen-audit 1/1 1 1 89m deployment.apps/zen-core 3/3 3 3 89m deployment.apps/zen-core-api 3/3 3 3 89m deployment.apps/zen-data-sorcerer 2/2 2 2 83m deployment.apps/zen-watchdog 1/1 1 1 83m deployment.apps/zen-watcher 1/1 1 1 89m NAME READY AGE statefulset.apps/dsx-influxdb 1/1 85m statefulset.apps/zen-metastoredb 3/3 118m Watson OpenScale \uf0c1 Subscriptions related to Watson OpenScale (in the ibm-cpd-operators namespace): cpd-platform-operator ibm-cpd-wos Analytics Engine is made up of many moving parts across multiple namespaces. In the ibm-common-services namespace: 15 workloads / 12 pods 0.051 CPU usage / 1.11 CPU requests / 3.57 CPU limit (5% utilization) 774.7 MiB memory usage, 2.27 GiB memory requests / 5.72 GiB memory limit (33% utilization) oc -n ibm-common-services get deployments NAME READY UP-TO-DATE AVAILABLE AGE cert-manager-cainjector 1/1 1 1 126m cert-manager-controller 1/1 1 1 126m cert-manager-webhook 1/1 1 1 126m configmap-watcher 1/1 1 1 126m ibm-cert-manager-operator 1/1 1 1 126m ibm-common-service-operator 1/1 1 1 131m ibm-common-service-webhook 1/1 1 1 130m ibm-namespace-scope-operator 1/1 1 1 131m ibm-zen-operator 1/1 1 1 126m meta-api-deploy 1/1 1 1 126m operand-deployment-lifecycle-manager 1/1 1 1 130m secretshare 1/1 1 1 130m In the ibm-cpd-operators namespace: 4 workloads / 4 pods 0.005 CPU usage / 0.4 CPU requests / 2 CPU limit (1% utilization) 148.1 MiB memory usage, 912 MiB memory requests / 3 GiB memory limit (16% utilization) oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 145m ibm-common-service-operator 1/1 1 1 145m ibm-cpd-wos-operator 1/1 1 1 76m ibm-namespace-scope-operator 1/1 1 1 145m In the ibm-cpd namespace: 32 workloads / 63 pods 0.591 CPU usage / 16.76 CPU requests / 31.9 CPU limit (4% utilization) 7.67 GiB memory usage, 37.32 GiB memory requests / 97.89 GiB memory limit (20% utilization) oc -n ibm-cpd get woservice,deployments,sts NAME TYPE STORAGE SCALECONFIG PHASE RECONCILED STATUS woservice.wos.cpd.ibm.com/aiopenscale service ibmc-file-gold-gid small Ready 4.0.9 Completed NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/aiopenscale-ibm-aios-bias 1/1 1 1 61m deployment.apps/aiopenscale-ibm-aios-bkpicombined 1/1 1 1 61m deployment.apps/aiopenscale-ibm-aios-common-api 1/1 1 1 61m deployment.apps/aiopenscale-ibm-aios-configuration 1/1 1 1 61m deployment.apps/aiopenscale-ibm-aios-dashboard 1/1 1 1 60m deployment.apps/aiopenscale-ibm-aios-datamart 1/1 1 1 60m deployment.apps/aiopenscale-ibm-aios-drift 1/1 1 1 60m deployment.apps/aiopenscale-ibm-aios-explainability 1/1 1 1 60m deployment.apps/aiopenscale-ibm-aios-fast-path 1/1 1 1 60m deployment.apps/aiopenscale-ibm-aios-feedback 1/1 1 1 59m deployment.apps/aiopenscale-ibm-aios-ml-gateway-discovery 1/1 1 1 59m deployment.apps/aiopenscale-ibm-aios-ml-gateway-service 1/1 1 1 59m deployment.apps/aiopenscale-ibm-aios-mrm 1/1 1 1 59m deployment.apps/aiopenscale-ibm-aios-nginx 1/1 1 1 58m deployment.apps/aiopenscale-ibm-aios-notification 1/1 1 1 59m deployment.apps/aiopenscale-ibm-aios-payload-logging 1/1 1 1 58m deployment.apps/aiopenscale-ibm-aios-payload-logging-api 1/1 1 1 59m deployment.apps/aiopenscale-ibm-aios-scheduling 1/1 1 1 58m deployment.apps/ibm-nginx 3/3 3 3 125m deployment.apps/usermgmt 3/3 3 3 127m deployment.apps/zen-audit 1/1 1 1 120m deployment.apps/zen-core 3/3 3 3 120m deployment.apps/zen-core-api 3/3 3 3 120m deployment.apps/zen-data-sorcerer 2/2 2 2 114m deployment.apps/zen-watchdog 1/1 1 1 114m deployment.apps/zen-watcher 1/1 1 1 120m NAME READY AGE statefulset.apps/aiopenscale-ibm-aios-etcd 3/3 62m statefulset.apps/aiopenscale-ibm-aios-kafka 3/3 62m statefulset.apps/aiopenscale-ibm-aios-redis 3/3 62m statefulset.apps/aiopenscale-ibm-aios-zookeeper 3/3 70m statefulset.apps/dsx-influxdb 1/1 116m statefulset.apps/zen-metastoredb 3/3 130m Watson Discovery \uf0c1 Subscriptions related to Watson Discovery (in the ibm-cpd-operators namespace): cpd-platform-operator ibm-watson-discovery-operator ibm-elasticsearch-operator ibm-etcd-operator ibm-minio-operator ibm-model-train-classic-operator ibm-rabbitmq-operator ibm-watson-gateway-operator Subscriptions related to Watson Discovery (in the ibm-common-services namespace): cloud-native-postgresql Watson Discovery is made up of many moving parts across multiple namespaces. In the ibm-common-services namespace: 13 workloads / 16 pods 0.126 CPU usage / 1.11 CPU requests / 3.57 CPU limit (8% utilization) 921.9 MiB memory usage, 2.27 GiB memory requests / 5.72 GiB memory limit (40% utilization) oc -n ibm-common-services get deployments NAME READY UP-TO-DATE AVAILABLE AGE cert-manager-cainjector 1/1 1 1 3h9m cert-manager-controller 1/1 1 1 3h9m cert-manager-webhook 1/1 1 1 3h9m configmap-watcher 1/1 1 1 3h9m ibm-cert-manager-operator 1/1 1 1 3h10m ibm-common-service-operator 1/1 1 1 3h16m ibm-common-service-webhook 1/1 1 1 3h14m ibm-namespace-scope-operator 1/1 1 1 3h15m ibm-zen-operator 1/1 1 1 3h10m meta-api-deploy 1/1 1 1 3h9m operand-deployment-lifecycle-manager 1/1 1 1 3h14m postgresql-operator-controller-manager-1-15-0 1/1 1 1 134m secretshare 1/1 1 1 3h14m In the ibm-cpd-operators namespace: 10 workloads / 10 pods 0.984 CPU usage / 1.65 CPU requests / 7.3 CPU limit (60% utilization) 671.6 MiB memory usage, 2.56 GiB memory requests / 9.46 GiB memory limit (25% utilization) oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 3h12m gateway-operator 1/1 1 1 131m ibm-common-service-operator 1/1 1 1 3h12m ibm-elasticsearch-operator-ibm-es-controller-manager 1/1 1 1 131m ibm-etcd-operator 1/1 1 1 131m ibm-minio-operator 1/1 1 1 131m ibm-model-train-classic-operator 1/1 1 1 131m ibm-namespace-scope-operator 1/1 1 1 3h12m ibm-rabbitmq-operator 1/1 1 1 131m wd-discovery-operator 1/1 1 1 131m In the ibm-cpd namespace: 49 workloads / 83 pods 0.994 CPU usage / 20.06 CPU requests / 112.3 CPU limit (5% utilization) 12.2 GiB memory usage, 96.1 GiB memory requests / 195.5 GiB memory limit (12% utilization) oc -n ibm-cpd get watsondiscoveries,deployments,sts NAME VERSION READY READYREASON UPDATING UPDATINGREASON DEPLOYED VERIFIED QUIESCE DATASTOREQUIESCE AGE watsondiscovery.discovery.watson.ibm.com/wd 4.0.9 True Stable False Stable 23/23 23/23 NOT_QUIESCED NOT_QUIESCED 130m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ibm-nginx 3/3 3 3 174m deployment.apps/usermgmt 3/3 3 3 176m deployment.apps/wd-discovery-cnm-api 1/1 1 1 39m deployment.apps/wd-discovery-converter 1/1 1 1 39m deployment.apps/wd-discovery-crawler 1/1 1 1 39m deployment.apps/wd-discovery-gateway 1/1 1 1 19m deployment.apps/wd-discovery-glimpse-builder 1/1 1 1 37m deployment.apps/wd-discovery-glimpse-query 1/1 1 1 39m deployment.apps/wd-discovery-haywire 1/1 1 1 39m deployment.apps/wd-discovery-hdp-rm 1/1 1 1 39m deployment.apps/wd-discovery-ingestion-api 1/1 1 1 39m deployment.apps/wd-discovery-inlet 1/1 1 1 39m deployment.apps/wd-discovery-management 1/1 1 1 39m deployment.apps/wd-discovery-minerapp 1/1 1 1 24m deployment.apps/wd-discovery-orchestrator 1/1 1 1 39m deployment.apps/wd-discovery-outlet 1/1 1 1 39m deployment.apps/wd-discovery-po-box 1/1 1 1 122m deployment.apps/wd-discovery-project-data-prep-agent 1/1 1 1 39m deployment.apps/wd-discovery-ranker-master 1/1 1 1 37m deployment.apps/wd-discovery-ranker-monitor-agent 1/1 1 1 39m deployment.apps/wd-discovery-ranker-rest 1/1 1 1 37m deployment.apps/wd-discovery-rapi 1/1 1 1 120m deployment.apps/wd-discovery-rcm 1/1 1 1 122m deployment.apps/wd-discovery-serve-ranker 1/1 1 1 37m deployment.apps/wd-discovery-stateless-api-model-runtime 1/1 1 1 39m deployment.apps/wd-discovery-stateless-api-rest-proxy 1/1 1 1 39m deployment.apps/wd-discovery-support 0/0 0 0 39m deployment.apps/wd-discovery-tooling 1/1 1 1 24m deployment.apps/wd-discovery-training-agents 1/1 1 1 39m deployment.apps/wd-discovery-training-crud 1/1 1 1 39m deployment.apps/wd-discovery-training-rest 1/1 1 1 39m deployment.apps/wd-discovery-watson-gateway-gw-instance 1/1 1 1 15m deployment.apps/wd-discovery-wd-indexer 1/1 1 1 122m deployment.apps/wd-discovery-wksml 1/1 1 1 39m deployment.apps/zen-audit 1/1 1 1 171m deployment.apps/zen-core 3/3 3 3 171m deployment.apps/zen-core-api 3/3 3 3 171m deployment.apps/zen-data-sorcerer 2/2 2 2 165m deployment.apps/zen-watchdog 1/1 1 1 165m deployment.apps/zen-watcher 1/1 1 1 170m NAME READY AGE statefulset.apps/dsx-influxdb 1/1 167m statefulset.apps/wd-discovery-etcd 3/3 124m statefulset.apps/wd-discovery-hdp-worker 2/2 39m statefulset.apps/wd-discovery-sdu 1/1 24m statefulset.apps/wd-ibm-elasticsearch-es-server-client 1/1 121m statefulset.apps/wd-ibm-elasticsearch-es-server-data 1/1 121m statefulset.apps/wd-ibm-elasticsearch-es-server-master 1/1 121m statefulset.apps/wd-minio-discovery 4/4 125m statefulset.apps/wd-rabbitmq-discovery 1/1 125m statefulset.apps/zen-metastoredb 3/3 179m Role Variables - Installation \uf0c1 cpd_service_name \uf0c1 Name of the service to install, supported values are: wsl , wml , wd , aiopenscale and spark Required Environment Variable: CPD_SERVICE_NAME Default Value: None cpd_product_version \uf0c1 The product version (also known as operand version) of this service to install. Required Environment Variable: CPD_PRODUCT_VERSION Default Value: 4.0.9 cpd_service_storage_class \uf0c1 This is used to set spec.storageClass in all CPD v3.5 services, and many - but not all - CP4D v4.0 services. Required , unless IBMCloud storage classes are available. Environment Variable: CPD_SERVICE_STORAGE_CLASS Default Value: ibmc-file-gold-gid if the storage class is available. cpd_instance_namespace \uf0c1 Namespace where the CP4D instance is deployed. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default Value: ibm-cpd cpd_operator_namespace \uf0c1 Namespace where the CP4D instance is deployed. Optional Environment Variable: CPD_OPERATORS_NAMESPACE Default Value: ibm-cpd-operators cpd_admin_username \uf0c1 The CP4D Admin username to authenticate with CP4D APIs. If you didn't change the initial admin username after installing CP4D then you don't need to provide this. Optional Environment Variable: CP4D_ADMIN_USERNAME Default Value: admin cpd_admin_password \uf0c1 The CP4D Admin User password to call CP4D API to provision Discovery Instance. If you didn't change the initial admin password after CP4D install, you don't need to provide it. The initial admin user password for admin will be used. Optional Environment Variable: CP4D_ADMIN_PASSWORD Default Value: Looked up from the admin-user-details secret in the cpd_instance_namespace namespace Role Variables - Watson Studio \uf0c1 cpd_wsl_project_name \uf0c1 Stores the CP4D Watson Studio Project name that can be used to configure HP Utilities application in MAS. Optional, only supported when cpd_service_name = wsl Environment Variable: CPD_WSL_PROJECT_NAME Default Value: wsl-mas-${mas_instance_id}-hputilities cpd_wsl_project_description \uf0c1 Optional - Stores the CP4D Watson Studio Project description that can be used to configure HP Utilities application in MAS. Optional, only supported when cpd_service_name = wsl Environment Variable: CPD_WSL_PROJECT_DESCRIPTION Default Value: Watson Studio Project for Maximo Application Suite Role Variables - Watson Discovery \uf0c1 cpd_wd_instance_name \uf0c1 Stores the name of the CP4D Watson Discovery Instance that can be used to configure Assist application in MAS. Optional, only supported when cpd_service_name = wd Environment Variable: CPD_WD_INSTANCE_NAME Default Value: wd-mas-${mas_instance_id}-assist cpd_wd_deployment_type \uf0c1 Defines the CP4D Watson Discovery deployment type: Starter : One replica pod for each wd service/component, uses fewer resources in your cluster. Production : Multiple replica pods for each Watson Discovery service/component, recommended for production deployments to increase workload capacity however consumes more cluster resources. Optional Environment Variable: CPD_WD_DEPLOYMENT_TYPE Default Value: Starter Note: Deployment type cannot be changed in the future neither while upgrading the service. If you need to change the deployment type, you must uninstall Watson Discovery and reinstall with the desired deployment type. More information, see Upgrading Watson Discovery . Role Variables - MAS Configuration Generation \uf0c1 mas_instance_id \uf0c1 The instance ID of Maximo Application Suite that a generated configuration will target. If this or mas_config_dir are not set then the role will not generate a resource template. Optional, only supported when cpd_service_name = wsl Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \uf0c1 Local directory to save the generated resource definition. This can be used to manually configure a MAS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a resource template. Optional, only supported when cpd_service_name = wsl Environment Variable: MAS_CONFIG_DIR Default Value: None Example Playbook \uf0c1 --- - hosts: localhost any_errors_fatal: true vars: cpd_product_version: 4.5.0 cpd_service_storage_class: ibmc-file-gold-gid cpd_service_name: wsl roles: - ibm.mas_devops.cp4d_service License \uf0c1 EPL-2.0","title":"cp4d_service"},{"location":"roles/cp4d_service/#cp4d_service","text":"Install or upgrade a chosen CloudPak for Data service. Currently supported Cloud Pak for Data release versions supported are: 4.0.9 4.5.0 4.5.1 4.5.2 The role will automatically install the corresponding CPD service operator channel and custom resource version associated to the chosen Cloud Pak for Data release version. For more information about the specific CPD services channels and versions associated to a particular Cloud Pak for Data release can be found here .","title":"cp4d_service"},{"location":"roles/cp4d_service/#services-supported","text":"These services can be deployed and configured using this role: Watson Studio required by Predict and Health & Predict Utilities Watson Machine Learning required by Predict Analytics Service (Apache Spark) required by Predict Watson OpenScale an optional dependency for Predict Watson Discovery required by Assist","title":"Services Supported"},{"location":"roles/cp4d_service/#upgrade","text":"This role also supports seamlessly CPD services minor version upgrades (CPD 4.0.9 -> CPD 4.5.0), and patch version upgrades (CPD 4.5.0 -> CPD 4.5.x). All you need to do is to define cpd_product_version variable to the version you target to upgrade and run this role for a particular CPD service. It's important that before you upgrade CPD services, the CPD Control Plane/Zen is also upgraded to the same release version. For more information about IBM Cloud Pak for Data upgrade process, refer to the CPD official documentation . Application Support For more information on how Predict and HP Utilities make use of Watson Studio, refer to Predict/HP Utilities documentation Warning The reconcile of many CP4D resources will be marked as Failed multiple times during initial installation, these are misleading status updates , the install is just really slow and the operators can not properly handle this. For example, if you are watching the install of CCS you will see that each rabbitmq-ha pod takes 10-15 minutes to start up and it looks like there is a problem because the pod log will just stop at a certain point. If you see something like this as the last message in the pod log WAL: ra_log_wal init, open tbls: ra_log_open_mem_tables, closed tbls: ra_log_closed_mem_tables be assured that there's nothing wrong, it's just there's a long delay between that message and the next ( starting system coordination ) being logged.","title":"Upgrade"},{"location":"roles/cp4d_service/#watson-studio","text":"Subscriptions related to Watson Studio: cpd-platform-operator ibm-cpd-wsl ibm-cpd-ccs ibm-cpd-datarefinery ibm-cpd-ws-runtimes Watson Studio is made up of many moving parts across multiple namespaces. In the ibm-common-services namespace: 15 workloads / 12 pods 0.126 CPU usage / 1.11 CPU requests / 3.57 CPU limit (11% utilization) 773.8 MiB memory usage, 2.27 GiB memory requests / 5.72 GiB memory limit (33% utilization) oc -n ibm-common-services get deployments NAME READY UP-TO-DATE AVAILABLE AGE cert-manager-cainjector 1/1 1 1 4h57m cert-manager-controller 1/1 1 1 4h57m cert-manager-webhook 1/1 1 1 4h57m configmap-watcher 1/1 1 1 4h57m ibm-cert-manager-operator 1/1 1 1 4h58m ibm-common-service-operator 1/1 1 1 5h3m ibm-common-service-webhook 1/1 1 1 5h2m ibm-namespace-scope-operator 1/1 1 1 5h3m ibm-zen-operator 1/1 1 1 4h58m meta-api-deploy 1/1 1 1 4h57m operand-deployment-lifecycle-manager 1/1 1 1 5h2m secretshare 1/1 1 1 5h2m In the ibm-cpd-operators namespace: 7 workloads / 7 pods 0.007 CPU usage / 0.7 CPU requests / 3.75 CPU limit (1% utilization) 263.4 MiB memory usage, 1.64 GiB memory requests /6.5 GiB memory limit (15% utilization) oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 5h ibm-common-service-operator 1/1 1 1 5h ibm-cpd-ccs-operator 1/1 1 1 3h43m ibm-cpd-datarefinery-operator 1/1 1 1 134m ibm-cpd-ws-operator 1/1 1 1 3h45m ibm-cpd-ws-runtimes-operator 1/1 1 1 118m ibm-namespace-scope-operator 1/1 1 1 5h In the ibm-cpd namespace: 51 workloads / 101 pods 1.1 CPU usage / 18.72 CPU requests / 86.7 CPU limit (5% utilization) 16.46 GiB memory usage, 33.04 GiB memory requests / 175.7 GiB memory limit (50% utilization) oc -n ibm-cpd get ccs,ws,datarefinery,notebookruntimes,deployments,sts NAME VERSION RECONCILED STATUS AGE ccs.ccs.cpd.ibm.com/ccs-cr 4.0.9 4.0.9 Completed 3h42m NAME VERSION RECONCILED STATUS AGE ws.ws.cpd.ibm.com/ws-cr 4.0.9 4.0.9 Completed 3h45m NAME VERSION STATUS AGE datarefinery.datarefinery.cpd.ibm.com/datarefinery-sample 4.0.9 Completed 133m NAME VERSION RECONCILED STATUS AGE notebookruntime.ws.cpd.ibm.com/ibm-cpd-ws-runtime-py39 4.0.9 Completed 116m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/asset-files-api 1/1 1 1 159m deployment.apps/ax-cdsx-jupyter-notebooks-converter-deploy 1/1 1 1 111m deployment.apps/ax-cdsx-notebooks-job-manager-deploy 1/1 1 1 111m deployment.apps/ax-environments-api-deploy 1/1 1 1 144m deployment.apps/ax-environments-ui-deploy 1/1 1 1 144m deployment.apps/ax-wdp-notebooks-api-deploy 1/1 1 1 111m deployment.apps/ax-ws-notebooks-ui-deploy 1/1 1 1 111m deployment.apps/catalog-api 2/2 2 2 167m deployment.apps/dap-dashboards-api 1/1 1 1 159m deployment.apps/dataview-api-service 1/1 1 1 138m deployment.apps/dc-main 1/1 1 1 167m deployment.apps/event-logger-api 1/1 1 1 159m deployment.apps/ibm-0100-model-viewer-prod 1/1 1 1 110m deployment.apps/ibm-nginx 3/3 3 3 4h37m deployment.apps/jobs-api 1/1 1 1 155m deployment.apps/jobs-ui 1/1 1 1 155m deployment.apps/ngp-projects-api 1/1 1 1 159m deployment.apps/portal-catalog 1/1 1 1 166m deployment.apps/portal-common-api 1/1 1 1 159m deployment.apps/portal-dashboards 1/1 1 1 159m deployment.apps/portal-job-manager 1/1 1 1 159m deployment.apps/portal-main 1/1 1 1 159m deployment.apps/portal-ml-dl 1/1 1 1 111m deployment.apps/portal-notifications 1/1 1 1 159m deployment.apps/portal-projects 1/1 1 1 159m deployment.apps/redis-ha-haproxy 1/1 1 1 174m deployment.apps/runtime-assemblies-operator 1/1 1 1 151m deployment.apps/runtime-manager-api 1/1 1 1 147m deployment.apps/spaces 1/1 1 1 142m deployment.apps/spawner-api 1/1 1 1 146m deployment.apps/usermgmt 3/3 3 3 4h39m deployment.apps/wdp-connect-connection 1/1 1 1 167m deployment.apps/wdp-connect-connector 1/1 1 1 167m deployment.apps/wdp-connect-flight 1/1 1 1 167m deployment.apps/wdp-dataprep 1/1 1 1 126m deployment.apps/wdp-dataview 1/1 1 1 138m deployment.apps/wdp-shaper 1/1 1 1 128m deployment.apps/wkc-search 1/1 1 1 167m deployment.apps/wml-main 1/1 1 1 143m deployment.apps/zen-audit 1/1 1 1 4h33m deployment.apps/zen-core 3/3 3 3 4h32m deployment.apps/zen-core-api 3/3 3 3 4h32m deployment.apps/zen-data-sorcerer 2/2 2 2 4h25m deployment.apps/zen-watchdog 1/1 1 1 4h25m deployment.apps/zen-watcher 1/1 1 1 4h32m NAME READY AGE statefulset.apps/dsx-influxdb 1/1 4h28m statefulset.apps/elasticsearch-master 3/3 171m statefulset.apps/rabbitmq-ha 3/3 3h35m statefulset.apps/redis-ha-server 3/3 3h1m statefulset.apps/wdp-couchdb 3/3 3h38m statefulset.apps/zen-metastoredb 3/3 4h43m","title":"Watson Studio"},{"location":"roles/cp4d_service/#watson-machine-learning","text":"Subscriptions related to Watson Machine Learning: cpd-platform-operator ibm-cpd-wml ibm-cpd-ccs Watson Machine Learning is made up of many moving parts across multiple namespaces. In the ibm-common-services namespace: 15 workloads / 12 pods 0.126 CPU usage / 1.11 CPU requests / 3.57 CPU limit (11% utilization) 773.8 MiB memory usage, 2.27 GiB memory requests / 5.72 GiB memory limit (33% utilization) oc -n ibm-common-services get deployments NAME READY UP-TO-DATE AVAILABLE AGE cert-manager-cainjector 1/1 1 1 4h7m cert-manager-controller 1/1 1 1 4h7m cert-manager-webhook 1/1 1 1 4h7m configmap-watcher 1/1 1 1 4h7m ibm-cert-manager-operator 1/1 1 1 4h8m ibm-common-service-operator 1/1 1 1 4h14m ibm-common-service-webhook 1/1 1 1 4h12m ibm-namespace-scope-operator 1/1 1 1 4h13m ibm-zen-operator 1/1 1 1 4h8m meta-api-deploy 1/1 1 1 4h8m operand-deployment-lifecycle-manager 1/1 1 1 4h12m secretshare 1/1 1 1 4h12m In the ibm-cpd-operators namespace: 5 workloads / 5 pods 0.011 CPU usage / 0.5 CPU requests / 2.75 CPU limit (1% utilization) 177.4 MiB memory usage, 1.14 GiB memory requests / 4.5 GiB memory limit (15% utilization) oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 3h57m ibm-common-service-operator 1/1 1 1 3h57m ibm-cpd-ccs-operator 1/1 1 1 150m ibm-cpd-wml-operator 1/1 1 1 152m ibm-namespace-scope-operator 1/1 1 1 3h57m In the ibm-cpd namespace: 50 workloads / 103 pods 1.28 CPU usage / 18.07 CPU requests / 86.15 CPU limit (7% utilization) 16.96 GiB memory usage, 41.29 GiB memory requests / 184.5 GiB memory limit (41% utilization) oc -n ibm-cpd get ccs,wmlbase,deployments,sts NAME VERSION RECONCILED STATUS AGE ccs.ccs.cpd.ibm.com/ccs-cr 4.0.9 4.0.9 Completed 149m NAME VERSION BUILD STATUS AGE wmlbase.wml.cpd.ibm.com/wml-cr 4.0.9 4.0.10-3220 Completed 151m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/asset-files-api 1/1 1 1 80m deployment.apps/ax-environments-api-deploy 1/1 1 1 66m deployment.apps/ax-environments-ui-deploy 1/1 1 1 66m deployment.apps/catalog-api 2/2 2 2 89m deployment.apps/dap-dashboards-api 1/1 1 1 80m deployment.apps/dataview-api-service 1/1 1 1 62m deployment.apps/dc-main 1/1 1 1 88m deployment.apps/event-logger-api 1/1 1 1 80m deployment.apps/ibm-nginx 3/3 3 3 3h34m deployment.apps/jobs-api 1/1 1 1 76m deployment.apps/jobs-ui 1/1 1 1 76m deployment.apps/ngp-projects-api 1/1 1 1 80m deployment.apps/portal-catalog 1/1 1 1 88m deployment.apps/portal-common-api 1/1 1 1 80m deployment.apps/portal-dashboards 1/1 1 1 80m deployment.apps/portal-job-manager 1/1 1 1 80m deployment.apps/portal-main 1/1 1 1 80m deployment.apps/portal-notifications 1/1 1 1 80m deployment.apps/portal-projects 1/1 1 1 80m deployment.apps/redis-ha-haproxy 1/1 1 1 97m deployment.apps/runtime-assemblies-operator 1/1 1 1 72m deployment.apps/runtime-manager-api 1/1 1 1 70m deployment.apps/spaces 1/1 1 1 64m deployment.apps/spawner-api 1/1 1 1 69m deployment.apps/usermgmt 3/3 3 3 3h36m deployment.apps/wdp-connect-connection 1/1 1 1 88m deployment.apps/wdp-connect-connector 1/1 1 1 88m deployment.apps/wdp-connect-flight 1/1 1 1 88m deployment.apps/wdp-dataview 1/1 1 1 62m deployment.apps/wkc-search 1/1 1 1 88m deployment.apps/wml-deployment-envoy 1/1 1 1 37m deployment.apps/wml-main 1/1 1 1 65m deployment.apps/wml-repositoryv4 1/1 1 1 18m deployment.apps/wmltraining 1/1 1 1 13m deployment.apps/wmltrainingorchestrator 1/1 1 1 8m11s deployment.apps/zen-audit 1/1 1 1 3h28m deployment.apps/zen-core 3/3 3 3 3h27m deployment.apps/zen-core-api 3/3 3 3 3h27m deployment.apps/zen-data-sorcerer 2/2 2 2 3h20m deployment.apps/zen-watchdog 1/1 1 1 3h20m deployment.apps/zen-watcher 1/1 1 1 3h27m NAME READY AGE statefulset.apps/dsx-influxdb 1/1 3h23m statefulset.apps/elasticsearch-master 3/3 94m statefulset.apps/rabbitmq-ha 3/3 142m statefulset.apps/redis-ha-server 3/3 106m statefulset.apps/wdp-couchdb 3/3 145m statefulset.apps/wml-deployment-agent 1/1 32m statefulset.apps/wml-deployment-manager 1/1 24m statefulset.apps/wml-deployments-etcd 3/3 43m statefulset.apps/zen-metastoredb 3/3 3h40m","title":"Watson Machine Learning"},{"location":"roles/cp4d_service/#analytics-engine","text":"Subscriptions related to Analytics Engine: cpd-platform-operator analyticsengine-operator Analytics Engine is made up of many moving parts across multiple namespaces. In the ibm-common-services namespace: 15 workloads / 12 pods 0.051 CPU usage / 1.11 CPU requests / 3.57 CPU limit (5% utilization) 774.7 MiB memory usage, 2.27 GiB memory requests / 5.72 GiB memory limit (33% utilization) oc -n ibm-common-services get deployments NAME READY UP-TO-DATE AVAILABLE AGE cert-manager-cainjector 1/1 1 1 126m cert-manager-controller 1/1 1 1 126m cert-manager-webhook 1/1 1 1 126m configmap-watcher 1/1 1 1 126m ibm-cert-manager-operator 1/1 1 1 126m ibm-common-service-operator 1/1 1 1 131m ibm-common-service-webhook 1/1 1 1 130m ibm-namespace-scope-operator 1/1 1 1 131m ibm-zen-operator 1/1 1 1 126m meta-api-deploy 1/1 1 1 126m operand-deployment-lifecycle-manager 1/1 1 1 130m secretshare 1/1 1 1 130m In the ibm-cpd-operators namespace: 4 workloads / 4 pods 0.003 CPU usage / 0.4 CPU requests / 2 CPU limit (1% utilization) 131.5 MiB memory usage, 912 MiB memory requests / 3 GiB memory limit (15% utilization) oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 128m ibm-common-service-operator 1/1 1 1 128m ibm-cpd-ae-operator 1/1 1 1 65m ibm-namespace-scope-operator 1/1 1 1 128m In the ibm-cpd namespace: 16 workloads / 60 pods 0.449 CPU usage / 8.06 CPU requests / 21.15 CPU limit (5% utilization) 3.15 GiB memory usage, 14.31 GiB memory requests / 32.71 GiB memory limit (22% utilization) oc -n ibm-cpd get analyticsengine,deployments,sts NAME VERSION RECONCILED STATUS AGE analyticsengine.ae.cpd.ibm.com/analyticsengine-sample 4.0.9 4.0.9 Completed 66m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ibm-nginx 3/3 3 3 93m deployment.apps/spark-hb-control-plane 1/1 1 1 62m deployment.apps/spark-hb-create-trust-store 1/1 1 1 64m deployment.apps/spark-hb-deployer-agent 1/1 1 1 62m deployment.apps/spark-hb-helm-repo 1/1 1 1 62m deployment.apps/spark-hb-nginx 1/1 1 1 62m deployment.apps/spark-hb-register-hb-dataplane 1/1 1 1 55m deployment.apps/usermgmt 3/3 3 3 94m deployment.apps/zen-audit 1/1 1 1 89m deployment.apps/zen-core 3/3 3 3 89m deployment.apps/zen-core-api 3/3 3 3 89m deployment.apps/zen-data-sorcerer 2/2 2 2 83m deployment.apps/zen-watchdog 1/1 1 1 83m deployment.apps/zen-watcher 1/1 1 1 89m NAME READY AGE statefulset.apps/dsx-influxdb 1/1 85m statefulset.apps/zen-metastoredb 3/3 118m","title":"Analytics Engine"},{"location":"roles/cp4d_service/#watson-openscale","text":"Subscriptions related to Watson OpenScale (in the ibm-cpd-operators namespace): cpd-platform-operator ibm-cpd-wos Analytics Engine is made up of many moving parts across multiple namespaces. In the ibm-common-services namespace: 15 workloads / 12 pods 0.051 CPU usage / 1.11 CPU requests / 3.57 CPU limit (5% utilization) 774.7 MiB memory usage, 2.27 GiB memory requests / 5.72 GiB memory limit (33% utilization) oc -n ibm-common-services get deployments NAME READY UP-TO-DATE AVAILABLE AGE cert-manager-cainjector 1/1 1 1 126m cert-manager-controller 1/1 1 1 126m cert-manager-webhook 1/1 1 1 126m configmap-watcher 1/1 1 1 126m ibm-cert-manager-operator 1/1 1 1 126m ibm-common-service-operator 1/1 1 1 131m ibm-common-service-webhook 1/1 1 1 130m ibm-namespace-scope-operator 1/1 1 1 131m ibm-zen-operator 1/1 1 1 126m meta-api-deploy 1/1 1 1 126m operand-deployment-lifecycle-manager 1/1 1 1 130m secretshare 1/1 1 1 130m In the ibm-cpd-operators namespace: 4 workloads / 4 pods 0.005 CPU usage / 0.4 CPU requests / 2 CPU limit (1% utilization) 148.1 MiB memory usage, 912 MiB memory requests / 3 GiB memory limit (16% utilization) oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 145m ibm-common-service-operator 1/1 1 1 145m ibm-cpd-wos-operator 1/1 1 1 76m ibm-namespace-scope-operator 1/1 1 1 145m In the ibm-cpd namespace: 32 workloads / 63 pods 0.591 CPU usage / 16.76 CPU requests / 31.9 CPU limit (4% utilization) 7.67 GiB memory usage, 37.32 GiB memory requests / 97.89 GiB memory limit (20% utilization) oc -n ibm-cpd get woservice,deployments,sts NAME TYPE STORAGE SCALECONFIG PHASE RECONCILED STATUS woservice.wos.cpd.ibm.com/aiopenscale service ibmc-file-gold-gid small Ready 4.0.9 Completed NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/aiopenscale-ibm-aios-bias 1/1 1 1 61m deployment.apps/aiopenscale-ibm-aios-bkpicombined 1/1 1 1 61m deployment.apps/aiopenscale-ibm-aios-common-api 1/1 1 1 61m deployment.apps/aiopenscale-ibm-aios-configuration 1/1 1 1 61m deployment.apps/aiopenscale-ibm-aios-dashboard 1/1 1 1 60m deployment.apps/aiopenscale-ibm-aios-datamart 1/1 1 1 60m deployment.apps/aiopenscale-ibm-aios-drift 1/1 1 1 60m deployment.apps/aiopenscale-ibm-aios-explainability 1/1 1 1 60m deployment.apps/aiopenscale-ibm-aios-fast-path 1/1 1 1 60m deployment.apps/aiopenscale-ibm-aios-feedback 1/1 1 1 59m deployment.apps/aiopenscale-ibm-aios-ml-gateway-discovery 1/1 1 1 59m deployment.apps/aiopenscale-ibm-aios-ml-gateway-service 1/1 1 1 59m deployment.apps/aiopenscale-ibm-aios-mrm 1/1 1 1 59m deployment.apps/aiopenscale-ibm-aios-nginx 1/1 1 1 58m deployment.apps/aiopenscale-ibm-aios-notification 1/1 1 1 59m deployment.apps/aiopenscale-ibm-aios-payload-logging 1/1 1 1 58m deployment.apps/aiopenscale-ibm-aios-payload-logging-api 1/1 1 1 59m deployment.apps/aiopenscale-ibm-aios-scheduling 1/1 1 1 58m deployment.apps/ibm-nginx 3/3 3 3 125m deployment.apps/usermgmt 3/3 3 3 127m deployment.apps/zen-audit 1/1 1 1 120m deployment.apps/zen-core 3/3 3 3 120m deployment.apps/zen-core-api 3/3 3 3 120m deployment.apps/zen-data-sorcerer 2/2 2 2 114m deployment.apps/zen-watchdog 1/1 1 1 114m deployment.apps/zen-watcher 1/1 1 1 120m NAME READY AGE statefulset.apps/aiopenscale-ibm-aios-etcd 3/3 62m statefulset.apps/aiopenscale-ibm-aios-kafka 3/3 62m statefulset.apps/aiopenscale-ibm-aios-redis 3/3 62m statefulset.apps/aiopenscale-ibm-aios-zookeeper 3/3 70m statefulset.apps/dsx-influxdb 1/1 116m statefulset.apps/zen-metastoredb 3/3 130m","title":"Watson OpenScale"},{"location":"roles/cp4d_service/#watson-discovery","text":"Subscriptions related to Watson Discovery (in the ibm-cpd-operators namespace): cpd-platform-operator ibm-watson-discovery-operator ibm-elasticsearch-operator ibm-etcd-operator ibm-minio-operator ibm-model-train-classic-operator ibm-rabbitmq-operator ibm-watson-gateway-operator Subscriptions related to Watson Discovery (in the ibm-common-services namespace): cloud-native-postgresql Watson Discovery is made up of many moving parts across multiple namespaces. In the ibm-common-services namespace: 13 workloads / 16 pods 0.126 CPU usage / 1.11 CPU requests / 3.57 CPU limit (8% utilization) 921.9 MiB memory usage, 2.27 GiB memory requests / 5.72 GiB memory limit (40% utilization) oc -n ibm-common-services get deployments NAME READY UP-TO-DATE AVAILABLE AGE cert-manager-cainjector 1/1 1 1 3h9m cert-manager-controller 1/1 1 1 3h9m cert-manager-webhook 1/1 1 1 3h9m configmap-watcher 1/1 1 1 3h9m ibm-cert-manager-operator 1/1 1 1 3h10m ibm-common-service-operator 1/1 1 1 3h16m ibm-common-service-webhook 1/1 1 1 3h14m ibm-namespace-scope-operator 1/1 1 1 3h15m ibm-zen-operator 1/1 1 1 3h10m meta-api-deploy 1/1 1 1 3h9m operand-deployment-lifecycle-manager 1/1 1 1 3h14m postgresql-operator-controller-manager-1-15-0 1/1 1 1 134m secretshare 1/1 1 1 3h14m In the ibm-cpd-operators namespace: 10 workloads / 10 pods 0.984 CPU usage / 1.65 CPU requests / 7.3 CPU limit (60% utilization) 671.6 MiB memory usage, 2.56 GiB memory requests / 9.46 GiB memory limit (25% utilization) oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 3h12m gateway-operator 1/1 1 1 131m ibm-common-service-operator 1/1 1 1 3h12m ibm-elasticsearch-operator-ibm-es-controller-manager 1/1 1 1 131m ibm-etcd-operator 1/1 1 1 131m ibm-minio-operator 1/1 1 1 131m ibm-model-train-classic-operator 1/1 1 1 131m ibm-namespace-scope-operator 1/1 1 1 3h12m ibm-rabbitmq-operator 1/1 1 1 131m wd-discovery-operator 1/1 1 1 131m In the ibm-cpd namespace: 49 workloads / 83 pods 0.994 CPU usage / 20.06 CPU requests / 112.3 CPU limit (5% utilization) 12.2 GiB memory usage, 96.1 GiB memory requests / 195.5 GiB memory limit (12% utilization) oc -n ibm-cpd get watsondiscoveries,deployments,sts NAME VERSION READY READYREASON UPDATING UPDATINGREASON DEPLOYED VERIFIED QUIESCE DATASTOREQUIESCE AGE watsondiscovery.discovery.watson.ibm.com/wd 4.0.9 True Stable False Stable 23/23 23/23 NOT_QUIESCED NOT_QUIESCED 130m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ibm-nginx 3/3 3 3 174m deployment.apps/usermgmt 3/3 3 3 176m deployment.apps/wd-discovery-cnm-api 1/1 1 1 39m deployment.apps/wd-discovery-converter 1/1 1 1 39m deployment.apps/wd-discovery-crawler 1/1 1 1 39m deployment.apps/wd-discovery-gateway 1/1 1 1 19m deployment.apps/wd-discovery-glimpse-builder 1/1 1 1 37m deployment.apps/wd-discovery-glimpse-query 1/1 1 1 39m deployment.apps/wd-discovery-haywire 1/1 1 1 39m deployment.apps/wd-discovery-hdp-rm 1/1 1 1 39m deployment.apps/wd-discovery-ingestion-api 1/1 1 1 39m deployment.apps/wd-discovery-inlet 1/1 1 1 39m deployment.apps/wd-discovery-management 1/1 1 1 39m deployment.apps/wd-discovery-minerapp 1/1 1 1 24m deployment.apps/wd-discovery-orchestrator 1/1 1 1 39m deployment.apps/wd-discovery-outlet 1/1 1 1 39m deployment.apps/wd-discovery-po-box 1/1 1 1 122m deployment.apps/wd-discovery-project-data-prep-agent 1/1 1 1 39m deployment.apps/wd-discovery-ranker-master 1/1 1 1 37m deployment.apps/wd-discovery-ranker-monitor-agent 1/1 1 1 39m deployment.apps/wd-discovery-ranker-rest 1/1 1 1 37m deployment.apps/wd-discovery-rapi 1/1 1 1 120m deployment.apps/wd-discovery-rcm 1/1 1 1 122m deployment.apps/wd-discovery-serve-ranker 1/1 1 1 37m deployment.apps/wd-discovery-stateless-api-model-runtime 1/1 1 1 39m deployment.apps/wd-discovery-stateless-api-rest-proxy 1/1 1 1 39m deployment.apps/wd-discovery-support 0/0 0 0 39m deployment.apps/wd-discovery-tooling 1/1 1 1 24m deployment.apps/wd-discovery-training-agents 1/1 1 1 39m deployment.apps/wd-discovery-training-crud 1/1 1 1 39m deployment.apps/wd-discovery-training-rest 1/1 1 1 39m deployment.apps/wd-discovery-watson-gateway-gw-instance 1/1 1 1 15m deployment.apps/wd-discovery-wd-indexer 1/1 1 1 122m deployment.apps/wd-discovery-wksml 1/1 1 1 39m deployment.apps/zen-audit 1/1 1 1 171m deployment.apps/zen-core 3/3 3 3 171m deployment.apps/zen-core-api 3/3 3 3 171m deployment.apps/zen-data-sorcerer 2/2 2 2 165m deployment.apps/zen-watchdog 1/1 1 1 165m deployment.apps/zen-watcher 1/1 1 1 170m NAME READY AGE statefulset.apps/dsx-influxdb 1/1 167m statefulset.apps/wd-discovery-etcd 3/3 124m statefulset.apps/wd-discovery-hdp-worker 2/2 39m statefulset.apps/wd-discovery-sdu 1/1 24m statefulset.apps/wd-ibm-elasticsearch-es-server-client 1/1 121m statefulset.apps/wd-ibm-elasticsearch-es-server-data 1/1 121m statefulset.apps/wd-ibm-elasticsearch-es-server-master 1/1 121m statefulset.apps/wd-minio-discovery 4/4 125m statefulset.apps/wd-rabbitmq-discovery 1/1 125m statefulset.apps/zen-metastoredb 3/3 179m","title":"Watson Discovery"},{"location":"roles/cp4d_service/#role-variables-installation","text":"","title":"Role Variables - Installation"},{"location":"roles/cp4d_service/#cpd_service_name","text":"Name of the service to install, supported values are: wsl , wml , wd , aiopenscale and spark Required Environment Variable: CPD_SERVICE_NAME Default Value: None","title":"cpd_service_name"},{"location":"roles/cp4d_service/#cpd_product_version","text":"The product version (also known as operand version) of this service to install. Required Environment Variable: CPD_PRODUCT_VERSION Default Value: 4.0.9","title":"cpd_product_version"},{"location":"roles/cp4d_service/#cpd_service_storage_class","text":"This is used to set spec.storageClass in all CPD v3.5 services, and many - but not all - CP4D v4.0 services. Required , unless IBMCloud storage classes are available. Environment Variable: CPD_SERVICE_STORAGE_CLASS Default Value: ibmc-file-gold-gid if the storage class is available.","title":"cpd_service_storage_class"},{"location":"roles/cp4d_service/#cpd_instance_namespace","text":"Namespace where the CP4D instance is deployed. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default Value: ibm-cpd","title":"cpd_instance_namespace"},{"location":"roles/cp4d_service/#cpd_operator_namespace","text":"Namespace where the CP4D instance is deployed. Optional Environment Variable: CPD_OPERATORS_NAMESPACE Default Value: ibm-cpd-operators","title":"cpd_operator_namespace"},{"location":"roles/cp4d_service/#cpd_admin_username","text":"The CP4D Admin username to authenticate with CP4D APIs. If you didn't change the initial admin username after installing CP4D then you don't need to provide this. Optional Environment Variable: CP4D_ADMIN_USERNAME Default Value: admin","title":"cpd_admin_username"},{"location":"roles/cp4d_service/#cpd_admin_password","text":"The CP4D Admin User password to call CP4D API to provision Discovery Instance. If you didn't change the initial admin password after CP4D install, you don't need to provide it. The initial admin user password for admin will be used. Optional Environment Variable: CP4D_ADMIN_PASSWORD Default Value: Looked up from the admin-user-details secret in the cpd_instance_namespace namespace","title":"cpd_admin_password"},{"location":"roles/cp4d_service/#role-variables-watson-studio","text":"","title":"Role Variables - Watson Studio"},{"location":"roles/cp4d_service/#cpd_wsl_project_name","text":"Stores the CP4D Watson Studio Project name that can be used to configure HP Utilities application in MAS. Optional, only supported when cpd_service_name = wsl Environment Variable: CPD_WSL_PROJECT_NAME Default Value: wsl-mas-${mas_instance_id}-hputilities","title":"cpd_wsl_project_name"},{"location":"roles/cp4d_service/#cpd_wsl_project_description","text":"Optional - Stores the CP4D Watson Studio Project description that can be used to configure HP Utilities application in MAS. Optional, only supported when cpd_service_name = wsl Environment Variable: CPD_WSL_PROJECT_DESCRIPTION Default Value: Watson Studio Project for Maximo Application Suite","title":"cpd_wsl_project_description"},{"location":"roles/cp4d_service/#role-variables-watson-discovery","text":"","title":"Role Variables - Watson Discovery"},{"location":"roles/cp4d_service/#cpd_wd_instance_name","text":"Stores the name of the CP4D Watson Discovery Instance that can be used to configure Assist application in MAS. Optional, only supported when cpd_service_name = wd Environment Variable: CPD_WD_INSTANCE_NAME Default Value: wd-mas-${mas_instance_id}-assist","title":"cpd_wd_instance_name"},{"location":"roles/cp4d_service/#cpd_wd_deployment_type","text":"Defines the CP4D Watson Discovery deployment type: Starter : One replica pod for each wd service/component, uses fewer resources in your cluster. Production : Multiple replica pods for each Watson Discovery service/component, recommended for production deployments to increase workload capacity however consumes more cluster resources. Optional Environment Variable: CPD_WD_DEPLOYMENT_TYPE Default Value: Starter Note: Deployment type cannot be changed in the future neither while upgrading the service. If you need to change the deployment type, you must uninstall Watson Discovery and reinstall with the desired deployment type. More information, see Upgrading Watson Discovery .","title":"cpd_wd_deployment_type"},{"location":"roles/cp4d_service/#role-variables-mas-configuration-generation","text":"","title":"Role Variables - MAS Configuration Generation"},{"location":"roles/cp4d_service/#mas_instance_id","text":"The instance ID of Maximo Application Suite that a generated configuration will target. If this or mas_config_dir are not set then the role will not generate a resource template. Optional, only supported when cpd_service_name = wsl Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/cp4d_service/#mas_config_dir","text":"Local directory to save the generated resource definition. This can be used to manually configure a MAS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a resource template. Optional, only supported when cpd_service_name = wsl Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/cp4d_service/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true vars: cpd_product_version: 4.5.0 cpd_service_storage_class: ibmc-file-gold-gid cpd_service_name: wsl roles: - ibm.mas_devops.cp4d_service","title":"Example Playbook"},{"location":"roles/cp4d_service/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cp4d_upgrade/","text":"cp4d_upgrade \uf0c1 This role requires Cloud Pak for Data 3.5 to have been installed in first place using the cp4d role and that cp4d_hack_worker_nodes role have been also been executed before to prepare the cluster ahead of time by setting up a global image pull secret for CP4D 4.0 installation. This role will collect CP4D 3.5 installation information and will run the upgrade process to CP4D 4.0. Here are the list of events that will happen as part of this role: Upgrade CloudPak for Data installation from v3.5 to v4.0. More details can be found here Collect information about existing CP4D services that are installed/enabled and upgrade them all one by one. If DB2 Warehouse instances are detected, it will also upgrade each DB2 instance to the latest patch version available and compatible with CP4D 4.0. Note: When CloudPak is upgraded to version 4.0, the SSL certificates for DB2 instances will be updated and changed. As part of the DB2 Warehouse upgrade to version 4.0, this role will collect all existing JDBCCfg instances associated to your targeted MAS instance and automatically update them to have the newer SSL certificates generated for DB2 Warehouse so that your MAS applications can continue to be able to establish DB2 connection. This role considers that every database connection for the targeted MAS instance uses DB2 Warehouse configuration in the existing CP4D instance that will be upgraded, if that is not true for your case, then this role might not suite your need. For more information, please refer to Upgrading Maximo Application Suite documentation. Role Variables \uf0c1 mas_instance_id \uf0c1 Required - Defines the instance id that is used in the existing MAS installation, will be used to lookup the existing MAS JDBC configurations and make updates to the new SSL certificates generated by the CP4D upgrade. Environment Variable: MAS_INSTANCE_ID Default Value: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.cp4d_hack_worker_nodes - ibm.mas_devops.cp4d_upgrade","title":"cp4d_upgrade"},{"location":"roles/cp4d_upgrade/#cp4d_upgrade","text":"This role requires Cloud Pak for Data 3.5 to have been installed in first place using the cp4d role and that cp4d_hack_worker_nodes role have been also been executed before to prepare the cluster ahead of time by setting up a global image pull secret for CP4D 4.0 installation. This role will collect CP4D 3.5 installation information and will run the upgrade process to CP4D 4.0. Here are the list of events that will happen as part of this role: Upgrade CloudPak for Data installation from v3.5 to v4.0. More details can be found here Collect information about existing CP4D services that are installed/enabled and upgrade them all one by one. If DB2 Warehouse instances are detected, it will also upgrade each DB2 instance to the latest patch version available and compatible with CP4D 4.0. Note: When CloudPak is upgraded to version 4.0, the SSL certificates for DB2 instances will be updated and changed. As part of the DB2 Warehouse upgrade to version 4.0, this role will collect all existing JDBCCfg instances associated to your targeted MAS instance and automatically update them to have the newer SSL certificates generated for DB2 Warehouse so that your MAS applications can continue to be able to establish DB2 connection. This role considers that every database connection for the targeted MAS instance uses DB2 Warehouse configuration in the existing CP4D instance that will be upgraded, if that is not true for your case, then this role might not suite your need. For more information, please refer to Upgrading Maximo Application Suite documentation.","title":"cp4d_upgrade"},{"location":"roles/cp4d_upgrade/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cp4d_upgrade/#mas_instance_id","text":"Required - Defines the instance id that is used in the existing MAS installation, will be used to lookup the existing MAS JDBC configurations and make updates to the new SSL certificates generated by the CP4D upgrade. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/cp4d_upgrade/#example-playbook","text":"- hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.cp4d_hack_worker_nodes - ibm.mas_devops.cp4d_upgrade","title":"Example Playbook"},{"location":"roles/db2/","text":"db2 \uf0c1 This role creates a Db2 Warehouse instance using the Db2u Operator. A namespace called db2u will be created and the db2u operator will be installed into the ibm-common-services namespace to service the db2ucluster requests in db2u namespace. A private root CA certificate is created and is used to secure the TLS connections to the database. A Db2 Warehouse cluster will be created along with a public TLS encrypted route to allow external access to the cluster (access is via the ssl-server nodeport port on the -db2u-engn-svc service). Internal access is via the -db2u-engn-svc service and port 50001. Both the external route and the internal service use the same server certificate. The private root CA certificate and the server certificate are available from the db2u-ca and db2u-certificate secrets in the db2u namespace. The default user is db2inst1 and the password is available in the instancepassword secret in the same namespace. You can examine the deployed resources in the db2u namespace: oc -n db2u get db2ucluster NAME STATE MAINTENANCESTATE AGE db2u-db01 Ready None 29m It typically takes 20-30 minutes from the db2ucluster being created till it is ready. If the db2ucluster is not ready after that period then check that all the PersistentVolumeClaims in the db2u namespace are ready and that the pods in the namespace are not stuck in init state. If the c-<db2_instance_name>-db2u-0 pod is running then you can exec into the pod and check the /var/log/db2u.log for any issue. If the db2_node_label and db2_dedicated_node variables are defined then role will taint and drain the dedicated node before labeling it using database={{ db2_node_label }} . The node is then uncordoned. If the mas_instance_id and mas_config_dir are provided then the role will generate the JdbcCfg yaml that can be used to configure MAS to connect to this database. It does not apply the yaml to the cluster but does provide you with the yaml files to apply if needed. Role Variables - Installation \uf0c1 db2_instance_name \uf0c1 Name of the database instance, note that this is the instance name . Required Environment Variable: DB2_INSTANCE_NAME Default: None ibm_entitlement_key \uf0c1 Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None db2_entitlement_key \uf0c1 An IBM entitlement key specific for Db2 installation, primarily used to override ibm_entitlement_key in development. Optional Environment Variable: DB2_ENTITLEMENT_KEY Default: None db2_dbname \uf0c1 Name of the database within the instance. Optional Environment Variable: DB2_DBNAME Default: BLUDB db2_version \uf0c1 Version of the DB2U operator instance to be created. Optional Environment Variable: DB2_VERSION Default: 11.5.7.0-cn2 db2_4k_device_support \uf0c1 Whether 4K device support is turned on or not. Optional Environment Variable: DB2_4K_DEVICE_SUPPORT Default: ON db2_workload \uf0c1 The workload profile of the db2 instance, possible values are PUREDATA_OLAP or ANALYTICS . Optional Environment Variable: DB2_WORKLOAD Default: ANALYTICS db2_table_org \uf0c1 The way database tables will be organized. It can be set to either ROW or COLUMN . Optional Environment Variable: DB2_TABLE_ORG Default: ROW db2_ldap_username \uf0c1 Define the username of db2 in the local LDAP registry. If this is defined, the LDAP user will be the user identity passed into the MAS JDBC configuration. Optional Environment Variable: DB2_LDAP_USERNAME Default: None db2_ldap_password \uf0c1 Define the password of above db2 user in the local LDAP registry. Must define when db2_ldap_username is defined. Optional Environment Variable: DB2_LDAP_PASSWORD Default: None Role Variables - Storage \uf0c1 db2_meta_storage_class \uf0c1 Storage class used for metadata. This must support ReadWriteMany Required Environment Variable: DB2_META_STORAGE_CLASS Default: Defaults to ibmc-file-gold if the storage class is available in the cluster. db2_meta_storage_size \uf0c1 Size of the metadata persistent volume, in gigabytes Optional Environment Variable: DB2_META_STORAGE_SIZE Default: 20Gi db2_meta_storage_accessmode \uf0c1 The access mode for the storage. Optional Environment Variable: DB2_META_STORAGE_ACCESSMODE Default: ReadWriteMany db2_data_storage_class \uf0c1 Storage class used for user data. This must support ReadWriteOnce Required Environment Variable: DB2_DATA_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster. db2_data_storage_size \uf0c1 Size of data persistent volume. Optional Environment Variable: DB2_DATA_STORAGE_SIZE Default: 100Gi db2_data_storage_accessmode \uf0c1 The access mode for the storage. Optional Environment Variable: DB2_DATA_STORAGE_ACCESSMODE Default: ReadWriteOnce db2_backup_storage_class \uf0c1 Storage class used for backup. This must support ReadWriteMany Optional Environment Variable: DB2_BACKUP_STORAGE_CLASS Default: Defaults to ibmc-file-gold if the storage class is available in the cluster. db2_backup_storage_size \uf0c1 Size of backup persistent volume. Optional Environment Variable: DB2_BACKUP_STORAGE_SIZE Default: 100Gi db2_backup_storage_accessmode \uf0c1 The access mode for the storage. Optional Environment Variable: DB2_BACKUP_STORAGE_ACCESSMODE Default: ReadWriteMany db2_logs_storage_class \uf0c1 Storage class used for transaction logs. This must support ReadWriteOnce Optional Environment Variable: DB2_LOGS_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster. db2_logs_storage_size \uf0c1 Size of transaction logs persistent volume. Optional Environment Variable: DB2_LOGS_STORAGE_SIZE Default: 100Gi db2_logs_storage_accessmode \uf0c1 The access mode for the storage. Optional Environment Variable: DB2_LOGS_STORAGE_ACCESSMODE Default: ReadWriteOnce db2_temp_storage_class \uf0c1 Storage class used for temporary data. This must support ReadWriteOnce Optional Environment Variable: DB2_TEMP_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster. db2_temp_storage_size \uf0c1 Size of temporary persistent volume. Optional Environment Variable: DB2_TEMP_STORAGE_SIZE Default: 100Gi db2_temp_storage_accessmode \uf0c1 The access mode for the storage. Optional Environment Variable: DB2_TEMP_STORAGE_ACCESSMODE Default: ReadWriteOnce Role Variables - Resource Requests \uf0c1 db2_cpu_requests \uf0c1 Define the Kubernetes CPU request for the Db2 pod. Optional Environment Variable: DB2_CPU_REQUESTS Default: 2000m db2_cpu_limits \uf0c1 Define the Kubernetes CPU limit for the Db2 pod. Optional Environment Variable: DB2_CPU_LIMITS Default: 4000m db2_memory_requests \uf0c1 Define the Kubernetes memory request for the Db2 pod. Optional Environment Variable: DB2_MEMORY_REQUESTS Default: 6Gi db2_memory_limits \uf0c1 Define the Kubernetes memory limit for the Db2 pod. Optional Environment Variable: DB2_MEMORY_LIMITS Default: 12Gi Role Variables - Node Affinity \uf0c1 db2_node_label \uf0c1 The label used to specify node affinity and tolerations in the db2ucluster CR. Optional Environment Variable: 'DB2_NODE_LABEL Default: None db2_dedicated_node \uf0c1 The name of the worker node to apply the {{ db2_node_label }} taint and label to. Optional Environment Variable: 'DB2_DEDICATED_NODE Default: None Role Variables - MPP System \uf0c1 Warning Do not use these variables if you intend to use the Db2 instance with IBM Maximo Application Suite; no MAS application supports Db2 MPP db2_mln_count \uf0c1 The number of logical nodes (i.e. database partitions to create). Note: ensure that the application using this Db2 can support Db2 MPP (which is created when DB2_MLN_COUNT is greater than 1). Optional Environment Variable: 'DB2_MLN_COUNT Default: 1 db2_num_pods \uf0c1 The number of Db2 pods to create in the instance. Note that db2_num_pods must be less than or equal to db2_mln_count . A single db2u pod can contain multiple logical nodes. So be sure to avoid specifying a large number for db2_mln_count while specifying a small number for db2_num_pods . If in doubt, make db2_mln_count = db2_num_pods . For more information refer to the Db2 documentation . Optional Environment Variable: 'DB2_NUM_PODS Default: 1 Role Variables - MAS Configuration \uf0c1 mas_instance_id \uf0c1 Providing this and mas_config_dir will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Optional Environment Variable: MAS_INSTANCE_ID Default: None mas_config_dir \uf0c1 Providing this and mas_instance_id will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Optional Environment Variable: MAS_CONFIG_DIR Default: None mas_config_scope \uf0c1 Supported values are system , ws , app , or wsapp , this is only used when both mas_config_dir and mas_instance_id are set. Optional Environment Variable: MAS_CONFIG_SCOPE Default: system mas_workspace_id \uf0c1 This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Optional Environment Variable: MAS_WORKSPACE_ID Default: None mas_application_id \uf0c1 This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either app or wsapp Optional Environment Variable: 'MAS_APP_ID Default: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxxx # Configuration for the Db2 cluster db2_version: 11.5.7.0-cn2 db2_instance_name: db2u-db01 db2_dbname: BLUDB db2_meta_storage_class: \"ibmc-file-gold\" db2_data_storage_class: \"ibmc-block-gold\" db2_backup_storage_class: \"ibmc-file-gold\" db2_logs_storage_class: \"ibmc-block-gold\" db2_temp_storage_class: \"ibmc-block-gold\" # Create the MAS JdbcCfg & Secret resource definitions mas_instance_id: inst1 mas_config_dir: /home/david/masconfig roles: - ibm.mas_devops.db2 License \uf0c1 EPL-2.0","title":"db2"},{"location":"roles/db2/#db2","text":"This role creates a Db2 Warehouse instance using the Db2u Operator. A namespace called db2u will be created and the db2u operator will be installed into the ibm-common-services namespace to service the db2ucluster requests in db2u namespace. A private root CA certificate is created and is used to secure the TLS connections to the database. A Db2 Warehouse cluster will be created along with a public TLS encrypted route to allow external access to the cluster (access is via the ssl-server nodeport port on the -db2u-engn-svc service). Internal access is via the -db2u-engn-svc service and port 50001. Both the external route and the internal service use the same server certificate. The private root CA certificate and the server certificate are available from the db2u-ca and db2u-certificate secrets in the db2u namespace. The default user is db2inst1 and the password is available in the instancepassword secret in the same namespace. You can examine the deployed resources in the db2u namespace: oc -n db2u get db2ucluster NAME STATE MAINTENANCESTATE AGE db2u-db01 Ready None 29m It typically takes 20-30 minutes from the db2ucluster being created till it is ready. If the db2ucluster is not ready after that period then check that all the PersistentVolumeClaims in the db2u namespace are ready and that the pods in the namespace are not stuck in init state. If the c-<db2_instance_name>-db2u-0 pod is running then you can exec into the pod and check the /var/log/db2u.log for any issue. If the db2_node_label and db2_dedicated_node variables are defined then role will taint and drain the dedicated node before labeling it using database={{ db2_node_label }} . The node is then uncordoned. If the mas_instance_id and mas_config_dir are provided then the role will generate the JdbcCfg yaml that can be used to configure MAS to connect to this database. It does not apply the yaml to the cluster but does provide you with the yaml files to apply if needed.","title":"db2"},{"location":"roles/db2/#role-variables-installation","text":"","title":"Role Variables - Installation"},{"location":"roles/db2/#db2_instance_name","text":"Name of the database instance, note that this is the instance name . Required Environment Variable: DB2_INSTANCE_NAME Default: None","title":"db2_instance_name"},{"location":"roles/db2/#ibm_entitlement_key","text":"Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None","title":"ibm_entitlement_key"},{"location":"roles/db2/#db2_entitlement_key","text":"An IBM entitlement key specific for Db2 installation, primarily used to override ibm_entitlement_key in development. Optional Environment Variable: DB2_ENTITLEMENT_KEY Default: None","title":"db2_entitlement_key"},{"location":"roles/db2/#db2_dbname","text":"Name of the database within the instance. Optional Environment Variable: DB2_DBNAME Default: BLUDB","title":"db2_dbname"},{"location":"roles/db2/#db2_version","text":"Version of the DB2U operator instance to be created. Optional Environment Variable: DB2_VERSION Default: 11.5.7.0-cn2","title":"db2_version"},{"location":"roles/db2/#db2_4k_device_support","text":"Whether 4K device support is turned on or not. Optional Environment Variable: DB2_4K_DEVICE_SUPPORT Default: ON","title":"db2_4k_device_support"},{"location":"roles/db2/#db2_workload","text":"The workload profile of the db2 instance, possible values are PUREDATA_OLAP or ANALYTICS . Optional Environment Variable: DB2_WORKLOAD Default: ANALYTICS","title":"db2_workload"},{"location":"roles/db2/#db2_table_org","text":"The way database tables will be organized. It can be set to either ROW or COLUMN . Optional Environment Variable: DB2_TABLE_ORG Default: ROW","title":"db2_table_org"},{"location":"roles/db2/#db2_ldap_username","text":"Define the username of db2 in the local LDAP registry. If this is defined, the LDAP user will be the user identity passed into the MAS JDBC configuration. Optional Environment Variable: DB2_LDAP_USERNAME Default: None","title":"db2_ldap_username"},{"location":"roles/db2/#db2_ldap_password","text":"Define the password of above db2 user in the local LDAP registry. Must define when db2_ldap_username is defined. Optional Environment Variable: DB2_LDAP_PASSWORD Default: None","title":"db2_ldap_password"},{"location":"roles/db2/#role-variables-storage","text":"","title":"Role Variables - Storage"},{"location":"roles/db2/#db2_meta_storage_class","text":"Storage class used for metadata. This must support ReadWriteMany Required Environment Variable: DB2_META_STORAGE_CLASS Default: Defaults to ibmc-file-gold if the storage class is available in the cluster.","title":"db2_meta_storage_class"},{"location":"roles/db2/#db2_meta_storage_size","text":"Size of the metadata persistent volume, in gigabytes Optional Environment Variable: DB2_META_STORAGE_SIZE Default: 20Gi","title":"db2_meta_storage_size"},{"location":"roles/db2/#db2_meta_storage_accessmode","text":"The access mode for the storage. Optional Environment Variable: DB2_META_STORAGE_ACCESSMODE Default: ReadWriteMany","title":"db2_meta_storage_accessmode"},{"location":"roles/db2/#db2_data_storage_class","text":"Storage class used for user data. This must support ReadWriteOnce Required Environment Variable: DB2_DATA_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster.","title":"db2_data_storage_class"},{"location":"roles/db2/#db2_data_storage_size","text":"Size of data persistent volume. Optional Environment Variable: DB2_DATA_STORAGE_SIZE Default: 100Gi","title":"db2_data_storage_size"},{"location":"roles/db2/#db2_data_storage_accessmode","text":"The access mode for the storage. Optional Environment Variable: DB2_DATA_STORAGE_ACCESSMODE Default: ReadWriteOnce","title":"db2_data_storage_accessmode"},{"location":"roles/db2/#db2_backup_storage_class","text":"Storage class used for backup. This must support ReadWriteMany Optional Environment Variable: DB2_BACKUP_STORAGE_CLASS Default: Defaults to ibmc-file-gold if the storage class is available in the cluster.","title":"db2_backup_storage_class"},{"location":"roles/db2/#db2_backup_storage_size","text":"Size of backup persistent volume. Optional Environment Variable: DB2_BACKUP_STORAGE_SIZE Default: 100Gi","title":"db2_backup_storage_size"},{"location":"roles/db2/#db2_backup_storage_accessmode","text":"The access mode for the storage. Optional Environment Variable: DB2_BACKUP_STORAGE_ACCESSMODE Default: ReadWriteMany","title":"db2_backup_storage_accessmode"},{"location":"roles/db2/#db2_logs_storage_class","text":"Storage class used for transaction logs. This must support ReadWriteOnce Optional Environment Variable: DB2_LOGS_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster.","title":"db2_logs_storage_class"},{"location":"roles/db2/#db2_logs_storage_size","text":"Size of transaction logs persistent volume. Optional Environment Variable: DB2_LOGS_STORAGE_SIZE Default: 100Gi","title":"db2_logs_storage_size"},{"location":"roles/db2/#db2_logs_storage_accessmode","text":"The access mode for the storage. Optional Environment Variable: DB2_LOGS_STORAGE_ACCESSMODE Default: ReadWriteOnce","title":"db2_logs_storage_accessmode"},{"location":"roles/db2/#db2_temp_storage_class","text":"Storage class used for temporary data. This must support ReadWriteOnce Optional Environment Variable: DB2_TEMP_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster.","title":"db2_temp_storage_class"},{"location":"roles/db2/#db2_temp_storage_size","text":"Size of temporary persistent volume. Optional Environment Variable: DB2_TEMP_STORAGE_SIZE Default: 100Gi","title":"db2_temp_storage_size"},{"location":"roles/db2/#db2_temp_storage_accessmode","text":"The access mode for the storage. Optional Environment Variable: DB2_TEMP_STORAGE_ACCESSMODE Default: ReadWriteOnce","title":"db2_temp_storage_accessmode"},{"location":"roles/db2/#role-variables-resource-requests","text":"","title":"Role Variables - Resource Requests"},{"location":"roles/db2/#db2_cpu_requests","text":"Define the Kubernetes CPU request for the Db2 pod. Optional Environment Variable: DB2_CPU_REQUESTS Default: 2000m","title":"db2_cpu_requests"},{"location":"roles/db2/#db2_cpu_limits","text":"Define the Kubernetes CPU limit for the Db2 pod. Optional Environment Variable: DB2_CPU_LIMITS Default: 4000m","title":"db2_cpu_limits"},{"location":"roles/db2/#db2_memory_requests","text":"Define the Kubernetes memory request for the Db2 pod. Optional Environment Variable: DB2_MEMORY_REQUESTS Default: 6Gi","title":"db2_memory_requests"},{"location":"roles/db2/#db2_memory_limits","text":"Define the Kubernetes memory limit for the Db2 pod. Optional Environment Variable: DB2_MEMORY_LIMITS Default: 12Gi","title":"db2_memory_limits"},{"location":"roles/db2/#role-variables-node-affinity","text":"","title":"Role Variables - Node Affinity"},{"location":"roles/db2/#db2_node_label","text":"The label used to specify node affinity and tolerations in the db2ucluster CR. Optional Environment Variable: 'DB2_NODE_LABEL Default: None","title":"db2_node_label"},{"location":"roles/db2/#db2_dedicated_node","text":"The name of the worker node to apply the {{ db2_node_label }} taint and label to. Optional Environment Variable: 'DB2_DEDICATED_NODE Default: None","title":"db2_dedicated_node"},{"location":"roles/db2/#role-variables-mpp-system","text":"Warning Do not use these variables if you intend to use the Db2 instance with IBM Maximo Application Suite; no MAS application supports Db2 MPP","title":"Role Variables - MPP System"},{"location":"roles/db2/#db2_mln_count","text":"The number of logical nodes (i.e. database partitions to create). Note: ensure that the application using this Db2 can support Db2 MPP (which is created when DB2_MLN_COUNT is greater than 1). Optional Environment Variable: 'DB2_MLN_COUNT Default: 1","title":"db2_mln_count"},{"location":"roles/db2/#db2_num_pods","text":"The number of Db2 pods to create in the instance. Note that db2_num_pods must be less than or equal to db2_mln_count . A single db2u pod can contain multiple logical nodes. So be sure to avoid specifying a large number for db2_mln_count while specifying a small number for db2_num_pods . If in doubt, make db2_mln_count = db2_num_pods . For more information refer to the Db2 documentation . Optional Environment Variable: 'DB2_NUM_PODS Default: 1","title":"db2_num_pods"},{"location":"roles/db2/#role-variables-mas-configuration","text":"","title":"Role Variables - MAS Configuration"},{"location":"roles/db2/#mas_instance_id","text":"Providing this and mas_config_dir will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Optional Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/db2/#mas_config_dir","text":"Providing this and mas_instance_id will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Optional Environment Variable: MAS_CONFIG_DIR Default: None","title":"mas_config_dir"},{"location":"roles/db2/#mas_config_scope","text":"Supported values are system , ws , app , or wsapp , this is only used when both mas_config_dir and mas_instance_id are set. Optional Environment Variable: MAS_CONFIG_SCOPE Default: system","title":"mas_config_scope"},{"location":"roles/db2/#mas_workspace_id","text":"This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Optional Environment Variable: MAS_WORKSPACE_ID Default: None","title":"mas_workspace_id"},{"location":"roles/db2/#mas_application_id","text":"This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either app or wsapp Optional Environment Variable: 'MAS_APP_ID Default: None","title":"mas_application_id"},{"location":"roles/db2/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxxx # Configuration for the Db2 cluster db2_version: 11.5.7.0-cn2 db2_instance_name: db2u-db01 db2_dbname: BLUDB db2_meta_storage_class: \"ibmc-file-gold\" db2_data_storage_class: \"ibmc-block-gold\" db2_backup_storage_class: \"ibmc-file-gold\" db2_logs_storage_class: \"ibmc-block-gold\" db2_temp_storage_class: \"ibmc-block-gold\" # Create the MAS JdbcCfg & Secret resource definitions mas_instance_id: inst1 mas_config_dir: /home/david/masconfig roles: - ibm.mas_devops.db2","title":"Example Playbook"},{"location":"roles/db2/#license","text":"EPL-2.0","title":"License"},{"location":"roles/db2_backup/","text":"db2_backup \uf0c1 This role runs a backup procedure from Db2uCluster instance and stores the backup files in a targetted local folder. At the end of the backup process, you will find the required files to run a successful restore process in the chosen DB2_BACKUP_DIR : DB2 backup files i.e BLUDB.0.db2inst1.DBPART000.202XXXXXXXXXXX.001 DB2 keystore files (.p12 and .sth) DB2 instance master key label file (.kdb) Role Variables \uf0c1 db2_backup_dir \uf0c1 Local directory that will store the backup files taken from the DB2 Warehouse instance. Required Environment Variable: DB2_BACKUP_DIR Default: None db2_backup_instance_name \uf0c1 DB2 instance name to take the backup from. Required Environment Variable: DB2_BACKUP_INSTANCE_NAME Default: None db2_namespace \uf0c1 The namespace containing the DB instance to be backed up. Required Environment Variable: DB2_NAMESPACE Default: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: db2_namespace: \"db2u\" # Backup mydb1 and restore it to mydb2 db2_backup_dir: \"/tmp/db2backup\" db2_backup_instance_name: \"mydb1\" db2_restore_dir: \"/tmp/db2backup\" db2_restore_instance_name: \"mydb2\" roles: - ibm.mas_devops.db2_backup - ibm.mas_devops.db2_restore License \uf0c1 EPL-2.0","title":"db2_backup"},{"location":"roles/db2_backup/#db2_backup","text":"This role runs a backup procedure from Db2uCluster instance and stores the backup files in a targetted local folder. At the end of the backup process, you will find the required files to run a successful restore process in the chosen DB2_BACKUP_DIR : DB2 backup files i.e BLUDB.0.db2inst1.DBPART000.202XXXXXXXXXXX.001 DB2 keystore files (.p12 and .sth) DB2 instance master key label file (.kdb)","title":"db2_backup"},{"location":"roles/db2_backup/#role-variables","text":"","title":"Role Variables"},{"location":"roles/db2_backup/#db2_backup_dir","text":"Local directory that will store the backup files taken from the DB2 Warehouse instance. Required Environment Variable: DB2_BACKUP_DIR Default: None","title":"db2_backup_dir"},{"location":"roles/db2_backup/#db2_backup_instance_name","text":"DB2 instance name to take the backup from. Required Environment Variable: DB2_BACKUP_INSTANCE_NAME Default: None","title":"db2_backup_instance_name"},{"location":"roles/db2_backup/#db2_namespace","text":"The namespace containing the DB instance to be backed up. Required Environment Variable: DB2_NAMESPACE Default: None","title":"db2_namespace"},{"location":"roles/db2_backup/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: db2_namespace: \"db2u\" # Backup mydb1 and restore it to mydb2 db2_backup_dir: \"/tmp/db2backup\" db2_backup_instance_name: \"mydb1\" db2_restore_dir: \"/tmp/db2backup\" db2_restore_instance_name: \"mydb2\" roles: - ibm.mas_devops.db2_backup - ibm.mas_devops.db2_restore","title":"Example Playbook"},{"location":"roles/db2_backup/#license","text":"EPL-2.0","title":"License"},{"location":"roles/db2_restore/","text":"db2_restore \uf0c1 This role runs a restore procedure onto a Db2uCluster instance. In order to begin the restore process, you must have the required files to run a successful restore process in the chosen db2_restore_dir : DB2 backup files i.e BLUDB.0.db2inst1.DBPART000.202XXXXXXXXXXX.001 DB2 keystore files (.p12 and .sth) DB2 instance master key label file (.kdb) Note: These files are generated automatically if you run ibm.mas_devops.db2_backup role. If any of the above files are not found in the directory specified by db2_restore_dir the restore process will fail. Role Variables \uf0c1 db2_restore_dir \uf0c1 Local directory that stores the backup files to be used in the DB2 Warehouse restore process Required Environment Variable: DB2WH_RESTORE_DIR Default: None db2_restore_instance_name \uf0c1 DB2 Warehouse target instance to restore the backup to. Required Environment Variable: DB2_RESTORE_INSTANCE_NAME Default: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: # Backup mydb1 and restore it to mydb2 db2_backup_dir: \"/tmp/db2backup\" db2_backup_instance_name: \"mydb1\" db2_restore_dir: \"/tmp/db2backup\" db2_restore_instance_name: \"mydb2\" roles: - ibm.mas_devops.db2_backup - ibm.mas_devops.db2_restore License \uf0c1 EPL-2.0","title":"db2_restore"},{"location":"roles/db2_restore/#db2_restore","text":"This role runs a restore procedure onto a Db2uCluster instance. In order to begin the restore process, you must have the required files to run a successful restore process in the chosen db2_restore_dir : DB2 backup files i.e BLUDB.0.db2inst1.DBPART000.202XXXXXXXXXXX.001 DB2 keystore files (.p12 and .sth) DB2 instance master key label file (.kdb) Note: These files are generated automatically if you run ibm.mas_devops.db2_backup role. If any of the above files are not found in the directory specified by db2_restore_dir the restore process will fail.","title":"db2_restore"},{"location":"roles/db2_restore/#role-variables","text":"","title":"Role Variables"},{"location":"roles/db2_restore/#db2_restore_dir","text":"Local directory that stores the backup files to be used in the DB2 Warehouse restore process Required Environment Variable: DB2WH_RESTORE_DIR Default: None","title":"db2_restore_dir"},{"location":"roles/db2_restore/#db2_restore_instance_name","text":"DB2 Warehouse target instance to restore the backup to. Required Environment Variable: DB2_RESTORE_INSTANCE_NAME Default: None","title":"db2_restore_instance_name"},{"location":"roles/db2_restore/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # Backup mydb1 and restore it to mydb2 db2_backup_dir: \"/tmp/db2backup\" db2_backup_instance_name: \"mydb1\" db2_restore_dir: \"/tmp/db2backup\" db2_restore_instance_name: \"mydb2\" roles: - ibm.mas_devops.db2_backup - ibm.mas_devops.db2_restore","title":"Example Playbook"},{"location":"roles/db2_restore/#license","text":"EPL-2.0","title":"License"},{"location":"roles/gencfg_jdbc/","text":"suite_config \uf0c1 This role is used to configure db in Maximo Application Suite. Role Variables \uf0c1 mas_instance_id \uf0c1 Providing this and mas_config_dir will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Environment Variable: MAS_INSTANCE_ID Default: None mas_workspace_id \uf0c1 This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None db_instance_id \uf0c1 Defines the instance id that is used for the db configure in MAS installation Environment Variable: DB_INSTANCE_ID Default: dbinst db_username \uf0c1 Defines the username that is used for the db configure in MAS installation Environment Variable: MAS_JDBC_USER Default: None jdbc_instance_password \uf0c1 Defines the password that is used to connect to db in MAS installation Environment Variable: MAS_JDBC_PASSWORD Default: None jdbc_url \uf0c1 Defines the jdbc url that is used to connect to db in MAS installation , Append ;sslConnection=true to the URL so that it has the form jdbc:db2://hostname:port/database_name:sslConnection=true . Environment Variable: MAS_JDBC_URL Default: None db_pem_file \uf0c1 Defines the location of the pem file used for JDBC connection in MAS installation Environment Variable: MAS_JDBC_CERT_LOCAL_FILE Default: None Example Playbook \uf0c1 --- - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gencfg_jdbc License \uf0c1 EPL-2.0","title":"gencfg_jdbc"},{"location":"roles/gencfg_jdbc/#suite_config","text":"This role is used to configure db in Maximo Application Suite.","title":"suite_config"},{"location":"roles/gencfg_jdbc/#role-variables","text":"","title":"Role Variables"},{"location":"roles/gencfg_jdbc/#mas_instance_id","text":"Providing this and mas_config_dir will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/gencfg_jdbc/#mas_workspace_id","text":"This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None","title":"mas_workspace_id"},{"location":"roles/gencfg_jdbc/#db_instance_id","text":"Defines the instance id that is used for the db configure in MAS installation Environment Variable: DB_INSTANCE_ID Default: dbinst","title":"db_instance_id"},{"location":"roles/gencfg_jdbc/#db_username","text":"Defines the username that is used for the db configure in MAS installation Environment Variable: MAS_JDBC_USER Default: None","title":"db_username"},{"location":"roles/gencfg_jdbc/#jdbc_instance_password","text":"Defines the password that is used to connect to db in MAS installation Environment Variable: MAS_JDBC_PASSWORD Default: None","title":"jdbc_instance_password"},{"location":"roles/gencfg_jdbc/#jdbc_url","text":"Defines the jdbc url that is used to connect to db in MAS installation , Append ;sslConnection=true to the URL so that it has the form jdbc:db2://hostname:port/database_name:sslConnection=true . Environment Variable: MAS_JDBC_URL Default: None","title":"jdbc_url"},{"location":"roles/gencfg_jdbc/#db_pem_file","text":"Defines the location of the pem file used for JDBC connection in MAS installation Environment Variable: MAS_JDBC_CERT_LOCAL_FILE Default: None","title":"db_pem_file"},{"location":"roles/gencfg_jdbc/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gencfg_jdbc","title":"Example Playbook"},{"location":"roles/gencfg_jdbc/#license","text":"EPL-2.0","title":"License"},{"location":"roles/gencfg_watsonstudio/","text":"suite_config \uf0c1 This role is used to configure WatsonStudio in Maximo Application Suite. Role Variables \uf0c1 mas_instance_id \uf0c1 Providing this and mas_config_dir will instruct the role to generate a WatsonStudioCfg template that can be used to configure MAS to connect to WatsonStudio. Environment Variable: MAS_INSTANCE_ID Default: None mas_workspace_id \uf0c1 This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None CPD_ADMIN_USERNAME \uf0c1 Defines the username that is used for the WatsonStudio configure in MAS installation Environment Variable: CPD_ADMIN_USERNAME Default: None CPD_ADMIN_PASSWORD \uf0c1 Defines the password that is used to connect to WatsonStudio in MAS installation Environment Variable: CPD_ADMIN_PASSWORD Default: None CPD_URL \uf0c1 Defines the url that is used to connect to WatsonStudio in MAS installation Environment Variable: CPD_URL Default: None Example Playbook \uf0c1 --- - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gencfg_watsonstudio License \uf0c1 EPL-2.0","title":"gencfg_watsonstudio"},{"location":"roles/gencfg_watsonstudio/#suite_config","text":"This role is used to configure WatsonStudio in Maximo Application Suite.","title":"suite_config"},{"location":"roles/gencfg_watsonstudio/#role-variables","text":"","title":"Role Variables"},{"location":"roles/gencfg_watsonstudio/#mas_instance_id","text":"Providing this and mas_config_dir will instruct the role to generate a WatsonStudioCfg template that can be used to configure MAS to connect to WatsonStudio. Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/gencfg_watsonstudio/#mas_workspace_id","text":"This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None","title":"mas_workspace_id"},{"location":"roles/gencfg_watsonstudio/#cpd_admin_username","text":"Defines the username that is used for the WatsonStudio configure in MAS installation Environment Variable: CPD_ADMIN_USERNAME Default: None","title":"CPD_ADMIN_USERNAME"},{"location":"roles/gencfg_watsonstudio/#cpd_admin_password","text":"Defines the password that is used to connect to WatsonStudio in MAS installation Environment Variable: CPD_ADMIN_PASSWORD Default: None","title":"CPD_ADMIN_PASSWORD"},{"location":"roles/gencfg_watsonstudio/#cpd_url","text":"Defines the url that is used to connect to WatsonStudio in MAS installation Environment Variable: CPD_URL Default: None","title":"CPD_URL"},{"location":"roles/gencfg_watsonstudio/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gencfg_watsonstudio","title":"Example Playbook"},{"location":"roles/gencfg_watsonstudio/#license","text":"EPL-2.0","title":"License"},{"location":"roles/gencfg_workspace/","text":"gencfg_workspace \uf0c1 This role is used to generate a Workspace custom resource that can be applied to Maximo Application Suite manually, or using the suite_config role. The configuration will be saved to local disk in the directory specified by the mas_config_dir variable. Role Variables \uf0c1 mas_instance_id \uf0c1 Required. The MAS instance ID that the workspace will be used in Environment Variable: MAS_INSTANCE_ID Default Value: None mas_workspace_id \uf0c1 Required. The ID of the workspace Environment Variable: MAS_WORKSPACE_ID Default Value: None mas_workspace_name \uf0c1 Required. The display name for the workspace Environment Variable: MAS_WORKSPACE_NAME Default Value: None mas_config_dir \uf0c1 Required. The directory to save the configuration to. Environment Variable: MAS_CONFIG_DIR Default Value: None Example Playbook \uf0c1 --- - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_workspace_id: \"masdev\" mas_workspace_name: \"MAS Development\" mas_config_dir: \"/home/david/masconfig/inst1\" roles: - ibm.mas_devops.gencfg_workspace License \uf0c1 EPL-2.0","title":"gencfg_workspace"},{"location":"roles/gencfg_workspace/#gencfg_workspace","text":"This role is used to generate a Workspace custom resource that can be applied to Maximo Application Suite manually, or using the suite_config role. The configuration will be saved to local disk in the directory specified by the mas_config_dir variable.","title":"gencfg_workspace"},{"location":"roles/gencfg_workspace/#role-variables","text":"","title":"Role Variables"},{"location":"roles/gencfg_workspace/#mas_instance_id","text":"Required. The MAS instance ID that the workspace will be used in Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/gencfg_workspace/#mas_workspace_id","text":"Required. The ID of the workspace Environment Variable: MAS_WORKSPACE_ID Default Value: None","title":"mas_workspace_id"},{"location":"roles/gencfg_workspace/#mas_workspace_name","text":"Required. The display name for the workspace Environment Variable: MAS_WORKSPACE_NAME Default Value: None","title":"mas_workspace_name"},{"location":"roles/gencfg_workspace/#mas_config_dir","text":"Required. The directory to save the configuration to. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/gencfg_workspace/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_workspace_id: \"masdev\" mas_workspace_name: \"MAS Development\" mas_config_dir: \"/home/david/masconfig/inst1\" roles: - ibm.mas_devops.gencfg_workspace","title":"Example Playbook"},{"location":"roles/gencfg_workspace/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ibm_catalogs/","text":"ibm_catalogs \uf0c1 This role installs the IBM Maximo Operator Catalog , which is a curated Operator Catalog derived from the IBM Operator Catalog , with all content certified compatible with IBM Maximo Application Suite: Additional, for IBM employees only, the pre-release development operator catalog can be installed, this is achieved by setting both the artifactory_username and artifactory_apikey variables. Role Variables \uf0c1 mas_catalog_version \uf0c1 Version of the IBM Maximo Operator Catalog to install. Optional Environment Variable: MAS_CATALOG_VERSION Default Value: v8 artifactory_username \uf0c1 Use to enable the install of development catalog sources for pre-release installation. Optional Environment Variable: ARTIFACTORY_USERNAME Default Value: None artifactory_apikey \uf0c1 Use to enable the install of development catalog sources for pre-release installation. Optional Environment Variable: ARTIFACTORY_APIKEY Default Value: None Example Playbook \uf0c1 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost roles: - ibm.mas_devops.ibm_catalogs Run Role Playbook \uf0c1 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ROLE_NAME=ibm_catalogs ansible-playbook ibm.mas_devops.run_role License \uf0c1 EPL-2.0","title":"ibm_catalogs"},{"location":"roles/ibm_catalogs/#ibm_catalogs","text":"This role installs the IBM Maximo Operator Catalog , which is a curated Operator Catalog derived from the IBM Operator Catalog , with all content certified compatible with IBM Maximo Application Suite: Additional, for IBM employees only, the pre-release development operator catalog can be installed, this is achieved by setting both the artifactory_username and artifactory_apikey variables.","title":"ibm_catalogs"},{"location":"roles/ibm_catalogs/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ibm_catalogs/#mas_catalog_version","text":"Version of the IBM Maximo Operator Catalog to install. Optional Environment Variable: MAS_CATALOG_VERSION Default Value: v8","title":"mas_catalog_version"},{"location":"roles/ibm_catalogs/#artifactory_username","text":"Use to enable the install of development catalog sources for pre-release installation. Optional Environment Variable: ARTIFACTORY_USERNAME Default Value: None","title":"artifactory_username"},{"location":"roles/ibm_catalogs/#artifactory_apikey","text":"Use to enable the install of development catalog sources for pre-release installation. Optional Environment Variable: ARTIFACTORY_APIKEY Default Value: None","title":"artifactory_apikey"},{"location":"roles/ibm_catalogs/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost roles: - ibm.mas_devops.ibm_catalogs","title":"Example Playbook"},{"location":"roles/ibm_catalogs/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ROLE_NAME=ibm_catalogs ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/ibm_catalogs/#license","text":"EPL-2.0","title":"License"},{"location":"roles/install_operator/","text":"install_operator \uf0c1 TODO: Summarize role Role Variables \uf0c1 TODO: Finish documentation Example Playbook \uf0c1 TODO: Add example License \uf0c1 EPL-2.0","title":"install_operator"},{"location":"roles/install_operator/#install_operator","text":"TODO: Summarize role","title":"install_operator"},{"location":"roles/install_operator/#role-variables","text":"TODO: Finish documentation","title":"Role Variables"},{"location":"roles/install_operator/#example-playbook","text":"TODO: Add example","title":"Example Playbook"},{"location":"roles/install_operator/#license","text":"EPL-2.0","title":"License"},{"location":"roles/kafka/","text":"kafka \uf0c1 This role provides support to install a Kafka Cluster using Red Hat AMQ Streams and generate configuration that can be directly applied to Maximo Application Suite. The Red Hat AMQ streams component is a massively scalable, distributed, and high-performance data streaming platform based on the Apache Kafka project. It offers a distributed backbone that allows microservices and other applications to share data with high throughput and low latency. As more applications move to Kubernetes and Red Hat OpenShift, it is increasingly important to be able to run the communication infrastructure on the same platform. Red Hat OpenShift, as a highly scalable platform, is a natural fit for messaging technologies such as Kafka. The AMQ streams component makes running and managing Apache Kafka OpenShift native through the use of powerful operators that simplify the deployment, configuration, management, and use of Apache Kafka on Red Hat OpenShift. The AMQ streams component is part of the Red Hat AMQ family, which also includes the AMQ broker, a longtime innovation leader in Java\u2122 Message Service (JMS) and polyglot messaging, as well as the AMQ interconnect router, a wide-area, peer-to-peer messaging solution. Tip The role will generate a yaml file containing the definition of a Secret and KafkaCfg resource that can be used to configure the deployed cluster as the MAS system Kafka. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/kafkacfg-amqstreams-system.yaml or used in conjunction with the suite_config role. Role Variables \uf0c1 kafka_version \uf0c1 The version of Kafka to deploy by the operator. Before changing the kafka_version make the version is supported by the amq-streams operator version . Environment Variable: KAFKA_VERSION Default Value: 2.7.0 kafka_namespace \uf0c1 The namespace where the operator and Kafka cluster will be deployed. Environment Variable: KAFKA_NAMESPACE Default Value: amq-streams kafka_cluster_name \uf0c1 The name of the Kafka cluster that will be created Environment Variable: KAFKA_CLUSTER_NAME Default Value: maskafka kafka_cluster_size \uf0c1 The configuration to apply, there are two configurations available: small and large. Environment Variable: KAFKA_CLUSTER_SIZE Default Value: small kafka_storage_class \uf0c1 The name of the storage class to configure the AMQStreams operator to use for persistent storage in the Kafka cluster. Environment Variable: KAFKA_STORAGE_CLASS Default Value: lookup supported storage classes in the cluster kafka_storage_size \uf0c1 The size of the storage to configure the AMQStreams operator to use for persistent storage in the Kafka cluster. Environment Variable: KAFKA_STORAGE_SIZE Default Value: 100Gi zookeeper_storage_class \uf0c1 The name of the storage class to configure the AMQStreams operator to use for persistent storage in the Zookeeper cluster. Environment Variable: ZOOKEEPER_STORAGE_CLASS Default Value: lookup supported storage classes in the cluster zookeeper_storage_size \uf0c1 The size of the storage to configure the AMQStreams operator to use for persistent storage in the Zookeeper cluster. Environment Variable: ZOOKEEPER_STORAGE_SIZE Default Value: 10Gi kafka_user_name \uf0c1 The name of the user to setup in the cluster for MAS. Environment Variable: KAFKA_USER_NAME Default Value: masuser kafka_user_password (supported in Strimzi operator verion 0.25.0 - amq streams operator version 2.x) \uf0c1 The password of the user to setup in the cluster for MAS. Environment Variable: KAFKA_USER_PASSWORD Default Value: a randomly generated password is used if one is not specified mas_instance_id \uf0c1 The instance ID of Maximo Application Suite that the KafkaCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \uf0c1 Local directory to save the generated KafkaCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: # Set storage class suitable for use on IBM Cloud ROKS kafka_storage_class: ibmc-block-gold # Generate a KafkaCfg template mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.kafka License \uf0c1 EPL-2.0","title":"kafka"},{"location":"roles/kafka/#kafka","text":"This role provides support to install a Kafka Cluster using Red Hat AMQ Streams and generate configuration that can be directly applied to Maximo Application Suite. The Red Hat AMQ streams component is a massively scalable, distributed, and high-performance data streaming platform based on the Apache Kafka project. It offers a distributed backbone that allows microservices and other applications to share data with high throughput and low latency. As more applications move to Kubernetes and Red Hat OpenShift, it is increasingly important to be able to run the communication infrastructure on the same platform. Red Hat OpenShift, as a highly scalable platform, is a natural fit for messaging technologies such as Kafka. The AMQ streams component makes running and managing Apache Kafka OpenShift native through the use of powerful operators that simplify the deployment, configuration, management, and use of Apache Kafka on Red Hat OpenShift. The AMQ streams component is part of the Red Hat AMQ family, which also includes the AMQ broker, a longtime innovation leader in Java\u2122 Message Service (JMS) and polyglot messaging, as well as the AMQ interconnect router, a wide-area, peer-to-peer messaging solution. Tip The role will generate a yaml file containing the definition of a Secret and KafkaCfg resource that can be used to configure the deployed cluster as the MAS system Kafka. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/kafkacfg-amqstreams-system.yaml or used in conjunction with the suite_config role.","title":"kafka"},{"location":"roles/kafka/#role-variables","text":"","title":"Role Variables"},{"location":"roles/kafka/#kafka_version","text":"The version of Kafka to deploy by the operator. Before changing the kafka_version make the version is supported by the amq-streams operator version . Environment Variable: KAFKA_VERSION Default Value: 2.7.0","title":"kafka_version"},{"location":"roles/kafka/#kafka_namespace","text":"The namespace where the operator and Kafka cluster will be deployed. Environment Variable: KAFKA_NAMESPACE Default Value: amq-streams","title":"kafka_namespace"},{"location":"roles/kafka/#kafka_cluster_name","text":"The name of the Kafka cluster that will be created Environment Variable: KAFKA_CLUSTER_NAME Default Value: maskafka","title":"kafka_cluster_name"},{"location":"roles/kafka/#kafka_cluster_size","text":"The configuration to apply, there are two configurations available: small and large. Environment Variable: KAFKA_CLUSTER_SIZE Default Value: small","title":"kafka_cluster_size"},{"location":"roles/kafka/#kafka_storage_class","text":"The name of the storage class to configure the AMQStreams operator to use for persistent storage in the Kafka cluster. Environment Variable: KAFKA_STORAGE_CLASS Default Value: lookup supported storage classes in the cluster","title":"kafka_storage_class"},{"location":"roles/kafka/#kafka_storage_size","text":"The size of the storage to configure the AMQStreams operator to use for persistent storage in the Kafka cluster. Environment Variable: KAFKA_STORAGE_SIZE Default Value: 100Gi","title":"kafka_storage_size"},{"location":"roles/kafka/#zookeeper_storage_class","text":"The name of the storage class to configure the AMQStreams operator to use for persistent storage in the Zookeeper cluster. Environment Variable: ZOOKEEPER_STORAGE_CLASS Default Value: lookup supported storage classes in the cluster","title":"zookeeper_storage_class"},{"location":"roles/kafka/#zookeeper_storage_size","text":"The size of the storage to configure the AMQStreams operator to use for persistent storage in the Zookeeper cluster. Environment Variable: ZOOKEEPER_STORAGE_SIZE Default Value: 10Gi","title":"zookeeper_storage_size"},{"location":"roles/kafka/#kafka_user_name","text":"The name of the user to setup in the cluster for MAS. Environment Variable: KAFKA_USER_NAME Default Value: masuser","title":"kafka_user_name"},{"location":"roles/kafka/#kafka_user_password-supported-in-strimzi-operator-verion-0250-amq-streams-operator-version-2x","text":"The password of the user to setup in the cluster for MAS. Environment Variable: KAFKA_USER_PASSWORD Default Value: a randomly generated password is used if one is not specified","title":"kafka_user_password (supported in Strimzi operator verion 0.25.0 - amq streams operator version 2.x)"},{"location":"roles/kafka/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the KafkaCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/kafka/#mas_config_dir","text":"Local directory to save the generated KafkaCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/kafka/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # Set storage class suitable for use on IBM Cloud ROKS kafka_storage_class: ibmc-block-gold # Generate a KafkaCfg template mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.kafka","title":"Example Playbook"},{"location":"roles/kafka/#license","text":"EPL-2.0","title":"License"},{"location":"roles/mongodb/","text":"mongodb \uf0c1 MongoDb CE operator will be installed into the specified namespace, a 3 node cluster cluster will be created. The cluster will bind six PVCs, these provide persistence for the data and system logs across the three nodes. Currently there is no support built-in for customizing the cluster beyond this configuration. Tip The role will generate a yaml file containing the definition of a Secret and MongoCfg resource that can be used to configure the deployed instance as the MAS system MongoDb. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/mongocfg-mongoce-system.yaml or used in conjunction with the suite_config role. Role Variables \uf0c1 mongodb_namespace \uf0c1 The namespace where the operator and MongoDb cluster will be deployed. Environment Variable: MONGODB_NAMESPACE Default Value: mongoce mongodb_storage_class \uf0c1 Required. The name of the storage class to configure the MongoDb operator to use for persistent storage in the MongoDb cluster. Environment Variable: MONGODB_STORAGE_CLASS Default Value: None mongodb_storage_capacity_data \uf0c1 The size of the PVC that will be created for data storage in the cluster. Environment Variable: MONGODB_STORAGE_CAPACITY_DATA Default Value: 20Gi mongodb_storage_capacity_logs \uf0c1 The size of the PVC that will be created for log storage in the cluster. Environment Variable: MONGODB_STORAGE_CAPACITY_LOGS Default Value: 20Gi mongodb_cpu_limits \uf0c1 The CPU limits on the mongod container. Environment Variable: MONGODB_CPU_LIMITS Default Value: 1 mongodb_mem_limits \uf0c1 The Memory limits on the mongod container. Environment Variable: MONGODB_MEM_LIMITS Default Value: 1Gi mongodb_replicas \uf0c1 The number of the mongodb replica set members. Default is 3. Set to 1 for SNO Cluster. - Environment Variable: MONGODB_REPLICAS - Default Value: 3 mas_instance_id \uf0c1 The instance ID of Maximo Application Suite that the MongoCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a MongoCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \uf0c1 Local directory to save the generated MongoCfg resource definition. This can be used to manually configure a MAS instance to connect to the Mongo cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a MongoCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: mongodb_storage_class: ibmc-block-gold mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.mongodb License \uf0c1 EPL-2.0","title":"mongodb"},{"location":"roles/mongodb/#mongodb","text":"MongoDb CE operator will be installed into the specified namespace, a 3 node cluster cluster will be created. The cluster will bind six PVCs, these provide persistence for the data and system logs across the three nodes. Currently there is no support built-in for customizing the cluster beyond this configuration. Tip The role will generate a yaml file containing the definition of a Secret and MongoCfg resource that can be used to configure the deployed instance as the MAS system MongoDb. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/mongocfg-mongoce-system.yaml or used in conjunction with the suite_config role.","title":"mongodb"},{"location":"roles/mongodb/#role-variables","text":"","title":"Role Variables"},{"location":"roles/mongodb/#mongodb_namespace","text":"The namespace where the operator and MongoDb cluster will be deployed. Environment Variable: MONGODB_NAMESPACE Default Value: mongoce","title":"mongodb_namespace"},{"location":"roles/mongodb/#mongodb_storage_class","text":"Required. The name of the storage class to configure the MongoDb operator to use for persistent storage in the MongoDb cluster. Environment Variable: MONGODB_STORAGE_CLASS Default Value: None","title":"mongodb_storage_class"},{"location":"roles/mongodb/#mongodb_storage_capacity_data","text":"The size of the PVC that will be created for data storage in the cluster. Environment Variable: MONGODB_STORAGE_CAPACITY_DATA Default Value: 20Gi","title":"mongodb_storage_capacity_data"},{"location":"roles/mongodb/#mongodb_storage_capacity_logs","text":"The size of the PVC that will be created for log storage in the cluster. Environment Variable: MONGODB_STORAGE_CAPACITY_LOGS Default Value: 20Gi","title":"mongodb_storage_capacity_logs"},{"location":"roles/mongodb/#mongodb_cpu_limits","text":"The CPU limits on the mongod container. Environment Variable: MONGODB_CPU_LIMITS Default Value: 1","title":"mongodb_cpu_limits"},{"location":"roles/mongodb/#mongodb_mem_limits","text":"The Memory limits on the mongod container. Environment Variable: MONGODB_MEM_LIMITS Default Value: 1Gi","title":"mongodb_mem_limits"},{"location":"roles/mongodb/#mongodb_replicas","text":"The number of the mongodb replica set members. Default is 3. Set to 1 for SNO Cluster. - Environment Variable: MONGODB_REPLICAS - Default Value: 3","title":"mongodb_replicas"},{"location":"roles/mongodb/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the MongoCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a MongoCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/mongodb/#mas_config_dir","text":"Local directory to save the generated MongoCfg resource definition. This can be used to manually configure a MAS instance to connect to the Mongo cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a MongoCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/mongodb/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mongodb_storage_class: ibmc-block-gold mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.mongodb","title":"Example Playbook"},{"location":"roles/mongodb/#license","text":"EPL-2.0","title":"License"},{"location":"roles/nvidia_gpu/","text":"nvidia_gpu \uf0c1 This role installs the NVIDIA Graphical Processing Unit (GPU) operator and its prerequisite Node Feature Discovery (NFD) operator in an IBM Cloud Openshift cluster console. The role first installs the NFD operator and continues with the final step to install the NVIDIA GPU Operator. The NFD Operator is installed using the Red Hat Operators catalog source and the GPU operator is installed using the Certified Operators catalog source. Role Variables \uf0c1 nfd_namespace \uf0c1 The namespace where the node feature discovery operator will be deployed. Environment Variable: NFD_NAMESPACE Default Value: gpu-operator-resources nfd_channel \uf0c1 The channel to subscribe to for the nfd operator installation and updates. Available channels may be found in the package manifest of nfd operator in openshift. Environment Variable: NFD_CHANNEL Default Value: stable gpu_namespace \uf0c1 The namespace where the NVIDIA GPU operator will be deployed. For version 1.8.x, use of single namespace is not supported, therefore namespace is defaulted to openshift-operators . NVIDIA's suggested namespace to use for versions 1.9.0 and above is nvidia-gpu-operator Environment Variable: GPU_NAMESPACE Default Value: openshift-operators gpu_channel \uf0c1 The channel to subscribe to for the gpu operator installation and updates. Available channels may be found in the package manifest of gpu-operator-certified operator in openshift. Environment Variable: GPU_CHANNEL Default Value: v1.8 gpu_driver_version \uf0c1 The gpu driver version image that needs to be pulled from the gpu repository. It is recommended that the right version if GPU driver is used. The MVI installation documentation, the default version below should be used. Environment Variable: GPU_DRIVER_VERSION Default Value: 450.80.02 For more information on the NVIDIA GPU and NFD operators, visit https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/install-gpu-ocp.html Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gpu_install License \uf0c1 EPL-2.0","title":"nvidia_gpu"},{"location":"roles/nvidia_gpu/#nvidia_gpu","text":"This role installs the NVIDIA Graphical Processing Unit (GPU) operator and its prerequisite Node Feature Discovery (NFD) operator in an IBM Cloud Openshift cluster console. The role first installs the NFD operator and continues with the final step to install the NVIDIA GPU Operator. The NFD Operator is installed using the Red Hat Operators catalog source and the GPU operator is installed using the Certified Operators catalog source.","title":"nvidia_gpu"},{"location":"roles/nvidia_gpu/#role-variables","text":"","title":"Role Variables"},{"location":"roles/nvidia_gpu/#nfd_namespace","text":"The namespace where the node feature discovery operator will be deployed. Environment Variable: NFD_NAMESPACE Default Value: gpu-operator-resources","title":"nfd_namespace"},{"location":"roles/nvidia_gpu/#nfd_channel","text":"The channel to subscribe to for the nfd operator installation and updates. Available channels may be found in the package manifest of nfd operator in openshift. Environment Variable: NFD_CHANNEL Default Value: stable","title":"nfd_channel"},{"location":"roles/nvidia_gpu/#gpu_namespace","text":"The namespace where the NVIDIA GPU operator will be deployed. For version 1.8.x, use of single namespace is not supported, therefore namespace is defaulted to openshift-operators . NVIDIA's suggested namespace to use for versions 1.9.0 and above is nvidia-gpu-operator Environment Variable: GPU_NAMESPACE Default Value: openshift-operators","title":"gpu_namespace"},{"location":"roles/nvidia_gpu/#gpu_channel","text":"The channel to subscribe to for the gpu operator installation and updates. Available channels may be found in the package manifest of gpu-operator-certified operator in openshift. Environment Variable: GPU_CHANNEL Default Value: v1.8","title":"gpu_channel"},{"location":"roles/nvidia_gpu/#gpu_driver_version","text":"The gpu driver version image that needs to be pulled from the gpu repository. It is recommended that the right version if GPU driver is used. The MVI installation documentation, the default version below should be used. Environment Variable: GPU_DRIVER_VERSION Default Value: 450.80.02 For more information on the NVIDIA GPU and NFD operators, visit https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/install-gpu-ocp.html","title":"gpu_driver_version"},{"location":"roles/nvidia_gpu/#example-playbook","text":"- hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gpu_install","title":"Example Playbook"},{"location":"roles/nvidia_gpu/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_deprovision/","text":"ocp_deprovision \uf0c1 Deprovision OCP cluster in Fyre, IBM Cloud, & ROSA. Role Variables \uf0c1 cluster_type \uf0c1 Required. Specify the cluster type, supported values are roks and quickburn . Required Environment Variable: CLUSTER_TYPE Default Value: None cluster_name \uf0c1 Required. Specify the name of the cluster Required Environment Variable: CLUSTER_NAME Default Value: None Role Variables - ROKS \uf0c1 ibmcloud_apikey \uf0c1 The APIKey to be used by ibmcloud login comand. Required if cluster_type = roks Environment Variable: IBMCLOUD_APIKEY Default Value: None Role Variables - ROSA \uf0c1 rosa_token \uf0c1 The Token used to authenticate with the ROSA service. Required if cluster_type = rosa Environment Variable: ROSA_TOKEN Default Value: None Role Variables - FYRE \uf0c1 fyre_username \uf0c1 Username to authenticate with the Fyre API. Required if cluster_type = quickburn . Environment Variable: FYRE_USERNAME Default Value: None fyre_apikey \uf0c1 API key to authenticate with the Fyre API. Required if cluster_type = quickburn . Environment Variable: FYRE_APIKEY Default Value: None Role Variables - IPI \uf0c1 The following variables are only used when cluster_type = ipi . ipi_install_dir \uf0c1 The directory that is used to store the openshift-install executable, it's configuration, & generated log files. Optional when cluster_type = aws-ipi Environment Variable: IPI_DIR Default Value: ~/openshift-install Role Variables - AWS \uf0c1 The following variables are only used when cluster_type = ipi and ipi_platform = aws . aws_access_key_id \uf0c1 AWS access key associated with an IAM user or role. Required when cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_ACCESS_KEY_ID Default Value: None aws_secret_access_key \uf0c1 AWS secret access key associated with an IAM user or role. Make sure the access key has permissions to delete instances. Required when cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_SECRET_ACCESS_KEY Default Value: None Example Playbook \uf0c1 - hosts: localhost vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: xxxxx roles: - ibm.mas_devops.ocp_deprovision License \uf0c1 EPL-2.0","title":"ocp_deprovision"},{"location":"roles/ocp_deprovision/#ocp_deprovision","text":"Deprovision OCP cluster in Fyre, IBM Cloud, & ROSA.","title":"ocp_deprovision"},{"location":"roles/ocp_deprovision/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_deprovision/#cluster_type","text":"Required. Specify the cluster type, supported values are roks and quickburn . Required Environment Variable: CLUSTER_TYPE Default Value: None","title":"cluster_type"},{"location":"roles/ocp_deprovision/#cluster_name","text":"Required. Specify the name of the cluster Required Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/ocp_deprovision/#role-variables-roks","text":"","title":"Role Variables - ROKS"},{"location":"roles/ocp_deprovision/#ibmcloud_apikey","text":"The APIKey to be used by ibmcloud login comand. Required if cluster_type = roks Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/ocp_deprovision/#role-variables-rosa","text":"","title":"Role Variables - ROSA"},{"location":"roles/ocp_deprovision/#rosa_token","text":"The Token used to authenticate with the ROSA service. Required if cluster_type = rosa Environment Variable: ROSA_TOKEN Default Value: None","title":"rosa_token"},{"location":"roles/ocp_deprovision/#role-variables-fyre","text":"","title":"Role Variables - FYRE"},{"location":"roles/ocp_deprovision/#fyre_username","text":"Username to authenticate with the Fyre API. Required if cluster_type = quickburn . Environment Variable: FYRE_USERNAME Default Value: None","title":"fyre_username"},{"location":"roles/ocp_deprovision/#fyre_apikey","text":"API key to authenticate with the Fyre API. Required if cluster_type = quickburn . Environment Variable: FYRE_APIKEY Default Value: None","title":"fyre_apikey"},{"location":"roles/ocp_deprovision/#role-variables-ipi","text":"The following variables are only used when cluster_type = ipi .","title":"Role Variables - IPI"},{"location":"roles/ocp_deprovision/#ipi_install_dir","text":"The directory that is used to store the openshift-install executable, it's configuration, & generated log files. Optional when cluster_type = aws-ipi Environment Variable: IPI_DIR Default Value: ~/openshift-install","title":"ipi_install_dir"},{"location":"roles/ocp_deprovision/#role-variables-aws","text":"The following variables are only used when cluster_type = ipi and ipi_platform = aws .","title":"Role Variables - AWS"},{"location":"roles/ocp_deprovision/#aws_access_key_id","text":"AWS access key associated with an IAM user or role. Required when cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_ACCESS_KEY_ID Default Value: None","title":"aws_access_key_id"},{"location":"roles/ocp_deprovision/#aws_secret_access_key","text":"AWS secret access key associated with an IAM user or role. Make sure the access key has permissions to delete instances. Required when cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_SECRET_ACCESS_KEY Default Value: None","title":"aws_secret_access_key"},{"location":"roles/ocp_deprovision/#example-playbook","text":"- hosts: localhost vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: xxxxx roles: - ibm.mas_devops.ocp_deprovision","title":"Example Playbook"},{"location":"roles/ocp_deprovision/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_disable_updates/","text":"ocp_disable_updates \uf0c1 This role will disable automatic updates in the cluster. It's use is not recommended. Example Playbook \uf0c1 - hosts: localhost roles: - ibm.mas_devops.ocp_disable_updates License \uf0c1 EPL-2.0","title":"ocp_disable_updates"},{"location":"roles/ocp_disable_updates/#ocp_disable_updates","text":"This role will disable automatic updates in the cluster. It's use is not recommended.","title":"ocp_disable_updates"},{"location":"roles/ocp_disable_updates/#example-playbook","text":"- hosts: localhost roles: - ibm.mas_devops.ocp_disable_updates","title":"Example Playbook"},{"location":"roles/ocp_disable_updates/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_github_oauth/","text":"ocp_github_oauth \uf0c1 This role provides to support to configure cluster oauth using GitHub. Warning Make sure you have configured the oauth app in GitHub organization before use this role. When configuring make sure to use ibmgithub as the oauth id. Requires organization admin permission to perform this action. Role Variables \uf0c1 oauth.github_client_secret_value Secret value provided by the GitHub oauth app configuration. ouath.github_client_id_value Client ID value provided by the GitHub oauth app configuration. oauth.github_hostname can be used to target public GitHub or an enterprise account (e.g. github.ibm.com) oauth.groups List of groups to be created and its cluster role bindings oauth.groups.name Defines the name of the group oauth.groups.users List of users to be added to the group oauth.groups.groups_cluster_rolebindings List of cluster role bindings to be created for the group oauth.organizations List of GitHub organizations where the authentication will be performed Example Playbook \uf0c1 TODO: Add example License \uf0c1 EPL-2.0","title":"ocp_github_oauth"},{"location":"roles/ocp_github_oauth/#ocp_github_oauth","text":"This role provides to support to configure cluster oauth using GitHub. Warning Make sure you have configured the oauth app in GitHub organization before use this role. When configuring make sure to use ibmgithub as the oauth id. Requires organization admin permission to perform this action.","title":"ocp_github_oauth"},{"location":"roles/ocp_github_oauth/#role-variables","text":"oauth.github_client_secret_value Secret value provided by the GitHub oauth app configuration. ouath.github_client_id_value Client ID value provided by the GitHub oauth app configuration. oauth.github_hostname can be used to target public GitHub or an enterprise account (e.g. github.ibm.com) oauth.groups List of groups to be created and its cluster role bindings oauth.groups.name Defines the name of the group oauth.groups.users List of users to be added to the group oauth.groups.groups_cluster_rolebindings List of cluster role bindings to be created for the group oauth.organizations List of GitHub organizations where the authentication will be performed","title":"Role Variables"},{"location":"roles/ocp_github_oauth/#example-playbook","text":"TODO: Add example","title":"Example Playbook"},{"location":"roles/ocp_github_oauth/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_login/","text":"ocp_login \uf0c1 This role provides support to login to a cluster using the oc CLI by looking up cluster information from the infrastructure provider's APIs, it also supports setting ocp_server and ocp_token directly to support login to any Kubernetes cluster. Role Variables \uf0c1 cluster_name \uf0c1 The name of the cluster to login to. This will be used to lookup the actual login credentials of the system. Required unless ocp_server and ocp_token are set Environment Variable: CLUSTER_NAME Default: None \uf0c1 The type of cluster to login to ( roks , quickburn , or rosa ) Required unless ocp_server and ocp_token are set Environment Variable: CLUSTER_TYPE Default: None ocp_server \uf0c1 The OCP server address to perform oc login against Required unless cluster_name and cluster_type are set Environment Variable: OCP_SERVER Default: None ocp_token \uf0c1 The login token to use for oc login Required unless cluster_name and cluster_type are set Environment Variable: OCP_TOKEN Default: None Role Variables - IBMCloud ROKS \uf0c1 ibmcloud_apikey \uf0c1 APIKey to be used by ibmcloud login comand Required when cluster_type is roks Environment Variable: IBMCLOUD_APIKEY Default: None Role Variables - IBM DevIT Fyre \uf0c1 fyre_username \uf0c1 Your FYRE username Required when cluster_type is quickburn Environment Variable: FYRE_APIKEY Default: None fyre_apikey \uf0c1 Your FYRE API Key - Required when cluster_type is quickburn - Environment Variable: FYRE_APIKEY - Default: None Role Variables - AWS ROSA \uf0c1 rosa_token \uf0c1 Your ROSA secure token. Required when cluster_type is rosa Environment Variable: ROSA_TOKEN Default: None rosa_cluster_admin_password \uf0c1 The password for the cluster-admin account (created when the cluster was provisioned). Required when cluster_type is rosa Environment Variable: ROSA_CLUSTER_ADMIN_PASSWORD Default: None Example Playbooks \uf0c1 Direct Login \uf0c1 - hosts: localhost vars: ocp_server: xxxxx ocp_token: xxxxx roles: - ibm.mas_devops.ocp_login IBMCloud ROKS \uf0c1 - hosts: localhost vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: xxxxx ibmcloud_resourcegroup: mygroup roles: - ibm.mas_devops.ocp_login AWS ROSA \uf0c1 - hosts: localhost vars: cluster_name: mycluster cluster_type: rosa rosa_token: xxxxx rosa_cluster_admin_password: xxxxx roles: - ibm.mas_devops.ocp_login IBM DevIT Fyre \uf0c1 - hosts: localhost vars: cluster_name: mycluster cluster_type: quickburn fyre_username: xxxxx fyre_password: xxxxx roles: - ibm.mas_devops.ocp_login License \uf0c1 EPL-2.0","title":"ocp_login"},{"location":"roles/ocp_login/#ocp_login","text":"This role provides support to login to a cluster using the oc CLI by looking up cluster information from the infrastructure provider's APIs, it also supports setting ocp_server and ocp_token directly to support login to any Kubernetes cluster.","title":"ocp_login"},{"location":"roles/ocp_login/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_login/#cluster_name","text":"The name of the cluster to login to. This will be used to lookup the actual login credentials of the system. Required unless ocp_server and ocp_token are set Environment Variable: CLUSTER_NAME Default: None","title":"cluster_name"},{"location":"roles/ocp_login/#_1","text":"The type of cluster to login to ( roks , quickburn , or rosa ) Required unless ocp_server and ocp_token are set Environment Variable: CLUSTER_TYPE Default: None","title":""},{"location":"roles/ocp_login/#ocp_server","text":"The OCP server address to perform oc login against Required unless cluster_name and cluster_type are set Environment Variable: OCP_SERVER Default: None","title":"ocp_server"},{"location":"roles/ocp_login/#ocp_token","text":"The login token to use for oc login Required unless cluster_name and cluster_type are set Environment Variable: OCP_TOKEN Default: None","title":"ocp_token"},{"location":"roles/ocp_login/#role-variables-ibmcloud-roks","text":"","title":"Role Variables - IBMCloud ROKS"},{"location":"roles/ocp_login/#ibmcloud_apikey","text":"APIKey to be used by ibmcloud login comand Required when cluster_type is roks Environment Variable: IBMCLOUD_APIKEY Default: None","title":"ibmcloud_apikey"},{"location":"roles/ocp_login/#role-variables-ibm-devit-fyre","text":"","title":"Role Variables - IBM DevIT Fyre"},{"location":"roles/ocp_login/#fyre_username","text":"Your FYRE username Required when cluster_type is quickburn Environment Variable: FYRE_APIKEY Default: None","title":"fyre_username"},{"location":"roles/ocp_login/#fyre_apikey","text":"Your FYRE API Key - Required when cluster_type is quickburn - Environment Variable: FYRE_APIKEY - Default: None","title":"fyre_apikey"},{"location":"roles/ocp_login/#role-variables-aws-rosa","text":"","title":"Role Variables - AWS ROSA"},{"location":"roles/ocp_login/#rosa_token","text":"Your ROSA secure token. Required when cluster_type is rosa Environment Variable: ROSA_TOKEN Default: None","title":"rosa_token"},{"location":"roles/ocp_login/#rosa_cluster_admin_password","text":"The password for the cluster-admin account (created when the cluster was provisioned). Required when cluster_type is rosa Environment Variable: ROSA_CLUSTER_ADMIN_PASSWORD Default: None","title":"rosa_cluster_admin_password"},{"location":"roles/ocp_login/#example-playbooks","text":"","title":"Example Playbooks"},{"location":"roles/ocp_login/#direct-login","text":"- hosts: localhost vars: ocp_server: xxxxx ocp_token: xxxxx roles: - ibm.mas_devops.ocp_login","title":"Direct Login"},{"location":"roles/ocp_login/#ibmcloud-roks","text":"- hosts: localhost vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: xxxxx ibmcloud_resourcegroup: mygroup roles: - ibm.mas_devops.ocp_login","title":"IBMCloud ROKS"},{"location":"roles/ocp_login/#aws-rosa","text":"- hosts: localhost vars: cluster_name: mycluster cluster_type: rosa rosa_token: xxxxx rosa_cluster_admin_password: xxxxx roles: - ibm.mas_devops.ocp_login","title":"AWS ROSA"},{"location":"roles/ocp_login/#ibm-devit-fyre","text":"- hosts: localhost vars: cluster_name: mycluster cluster_type: quickburn fyre_username: xxxxx fyre_password: xxxxx roles: - ibm.mas_devops.ocp_login","title":"IBM DevIT Fyre"},{"location":"roles/ocp_login/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_provision/","text":"ocp_provision \uf0c1 Provision OCP cluster on DevIT Fyre or IBM Cloud ROKS. Role Variables \uf0c1 cluster_type \uf0c1 Required. Specify the cluster type, supported values are fyre , roks , rosa , and aws-ipi . Environment Variable: CLUSTER_TYPE Default Value: None cluster_name \uf0c1 Required. Specify the name of the cluster Environment Variable: CLUSTER_NAME Default Value: None ocp_version \uf0c1 Required. Specify the version of OCP to install. The exact format of this will vary depending on cluster_type . For ROKS clusters the format is 4.6_openshift , 4.8_openshift , for Fyre it is 4.6.16 . Environment Variable: OCP_VERSION Default Value: None Role Variables - GPU Node Support \uf0c1 ocp_provision_gpu \uf0c1 Flag that determines if GPU worker nodes should be added during cluster creation (eg. needed for MVI application). This is currently only set up for ROKS clusters. Environment Variable: OCP_PROVISION_GPU Default Value: false gpu_workerpool_name \uf0c1 The name of the gpu worker pool to added to or modify in the cluster. If already existing, use the existing name to avoid recreating another gpu worker pool unless that is the goal. Environment Variable: GPU_WORKERPOOL_NAME Default Value: gpu gpu_workers \uf0c1 The number of GPU worker nodes that will be deploy in the cluster. The node created will have mg4c.32x384.2xp100-GPU flavor. This variable depends on ocp_provision_gpu and is currently supported on ROKS clusters only. Environment Variable: GPU_WORKERS Default Value: 1 compute_node_count \uf0c1 The number of compute nodes (i.e. worker nodes) allocate to the OCP cluster. Environment Variable: COMPUTE_NODE_COUNT Default Value: 3 controlplane_node_count \uf0c1 The number of control plane nodes (i.e. master nodes) allocate to the OCP cluster. Environment Variable: CONTROLPLANE_NODE_COUNT Default Value: 3 gpu_workerpool_name \uf0c1 The name of the gpu worker pool to added to or modify in the cluster. If already existing, use the existing name to avoid recreating another gpu worker pool unless that is the goal. Environment Variable: GPU_WORKERPOOL_NAME Default Value: gpu Role Variables - ROKS \uf0c1 The following variables are only used when cluster_type = roks . ibmcloud_apikey \uf0c1 Required if cluster_type = roks . The APIKey to be used by ibmcloud login comand. Environment Variable: IBMCLOUD_APIKEY Default Value: None ibmcloud_resourcegroup \uf0c1 The resource group to create the cluster inside. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default roks_zone \uf0c1 IBM Cloud zone where the cluster should be provisioned. Environment Variable: ROKS_ZONE Default Value: dal10 roks_flavor \uf0c1 Worker node flavor Environment Variable: ROKS_FLAVOR Default Value: b3c.16x64.300gb roks_workers \uf0c1 Number of worker nodes for the roks cluster Environment Variable: ROKS_WORKERS Default Value: 3 roks_flags \uf0c1 Can be used to specify additional parameters for the cluster creation Environment Variable: ROKS_FLAGS Default Value: None Role Variables - ROSA \uf0c1 The following variables are only used when cluster_type = rosa . rosa_token \uf0c1 Token to authenticate to the ROSA service. To obtain your API token login to the OpenShift cluster manager . Required if cluster_type = rosa . Environment Variable: ROSA_TOKEN Default Value: None rosa_cluster_admin_password \uf0c1 Password to set up for the cluster-admin user account on the OCP instance. You will need this to log onto the cluster after it is provisioned. Required if cluster_type = rosa . Environment Variable: ROSA_CLUSTER_ADMIN_PASSWORD Default Value: None rosa_compute_nodes \uf0c1 Number of compute nodes to deploy in the cluster. Optional Environment Variable: ROSA_COMPUTE_NODES Default Value: 3 Role Variables - FYRE \uf0c1 The following variables are only used when cluster_type = fyre . fyre_username \uf0c1 Username to authenticate with Fyre API. Required if cluster_type = fyre . Environment Variable: FYRE_USERNAME Default Value: None fyre_apikey \uf0c1 API key to authenticate with Fyre API. Required if cluster_type = fyre . Environment Variable: FYRE_APIKEY Default Value: None fyre_quota_type \uf0c1 Type of quota to draw from when provisioning the cluster, valid options are quick_burn and product_group . Required if cluster_type = fyre . Environment Variable: FYRE_QUOTA_TYPE Default Value: quick_burn fyre_product_id \uf0c1 The Product ID that the cluster will be associated with for accounting purposes. Required if cluster_type = fyre . Environment Variable: FYRE_PRODUCT_ID Default Value: None fyre_cluster_description \uf0c1 Provide a description for the cluster. Optional Environment Variable: FYRE_CLUSTER_DESCRIPTION Default Value: None fyre_cluster_size \uf0c1 The name of one of Fyre's pre-defined cluster sizes to use for the new cluster. Required when cluster_type = fyre and fyre_quota_type = quick_burn . Environment Variable: FYRE_CLUSTER_SIZE Default Value: large fyre_worker_count \uf0c1 The number of worker nodes to provision in the cluster. Required when cluster_type = fyre and fyre_quota_type = quick_burn . Environment Variable: FYRE_WORKER_COUNT Default Value: 3 fyre_worker_cpu \uf0c1 The amount of CPU to assign to each worker node (maximum value supported by FYRE 16). Required when cluster_type = fyre and fyre_quota_type = quick_burn . Environment Variable: FYRE_WORKER_CPU Default Value: 16 fyre_worker_memory \uf0c1 The amount of memory to assign to each worker node (maximum value supported by FYRE 64). Required when cluster_type = fyre and fyre_quota_type = quick_burn . Environment Variable: FYRE_WORKER_MEMORY Default Value: 64 Role Variables - IPI \uf0c1 These variables are only used when cluster_type = ipi . Note IPI stands for Installer Provisioned Infrastructure . OpenShift offers two possible deployment methods: IPI and UPI (User Provisioned Infrastructure). The difference is the degree of automation and customization. IPI will not only deploy OpenShift but also all infrastructure components and configurations. ipi_platform \uf0c1 Platform to create the cluster on. Technically, any platform supported by openshift-install should work here, but currently we have only specifically tested on aws , which is the default value. Optional when cluster_type = ipi Environment Variable: IPI_PLATFORM Default Value: aws ipi_region \uf0c1 Platform region where OCP cluster will be created. Optional when cluster_type = ipi Environment Variable: IPI_REGION Default Value: us-east-1 ipi_base_domain \uf0c1 Specify the base domain of the cluster that will be provisioned. Required when cluster_type = aipi Environment Variable: IPI_BASE_DOMAIN Default Value: None ipi_pull_secret_file \uf0c1 Location of the file containing your Redhat OpenShift pull secret. This file can be obtained from the Red Hat Hybrid Cloud Console Required when cluster_type = ipi Environment Variable: IPI_PULL_SECRET_FILE Default Value: None ipi_dir \uf0c1 The working directory that is used to perform the installation, it will contain the openshift-install executable, it's configuration files, & any generated logs. Optional when cluster_type = ipi Environment Variable: IPI_DIR Default Value: ~/openshift-install ipi_controlplane_type \uf0c1 Control plane node type. Optional when cluster_type = ipi Environment Variable: IPI_CONTROLPLANE_TYPE Default Value: m5.4xlarge ipi_controlplane_replicas \uf0c1 The number of master nodes to provision to form the control plane of your cluster. Optional when cluster_type = ipi Environment Variable: IPI_CONTROLPLANE_REPLICAS Default Value: 3 ipi_compute_type \uf0c1 Compute node type. Optional when cluster_type = ipi Environment Variable: IPI_COMPUTE_TYPE Default Value: m5.4xlarge ipi_compute_replicas \uf0c1 The number of worker nodes to provsision in the cluster, providing your compute resource. Optional when cluster_type = ipi Environment Variable: IPI_COMPUTE_REPLICAS Default Value: 3 Role Variables - AWS \uf0c1 The following variables are only used when cluster_type = ipi and ipi_platform = aws . aws_access_key_id \uf0c1 AWS access key associated with an IAM user or role. Make sure the access key has permissions to create instances. Required when cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_ACCESS_KEY_ID Default Value: None aws_secret_access_key \uf0c1 AWS secret access key associated with an IAM user or role. Required when cluster_type = aws-ipi and ipi_platform = aws Environment Variable: AWS_SECRET_ACCESS_KEY Default Value: None Example Playbook \uf0c1 - hosts: localhost vars: cluster_type: roks cluster_name: mycluster ocp_version: 4.10 ibmcloud_apikey: xxxxx roles: - ibm.mas_devops.ocp_provision License \uf0c1 EPL-2.0","title":"ocp_provision"},{"location":"roles/ocp_provision/#ocp_provision","text":"Provision OCP cluster on DevIT Fyre or IBM Cloud ROKS.","title":"ocp_provision"},{"location":"roles/ocp_provision/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_provision/#cluster_type","text":"Required. Specify the cluster type, supported values are fyre , roks , rosa , and aws-ipi . Environment Variable: CLUSTER_TYPE Default Value: None","title":"cluster_type"},{"location":"roles/ocp_provision/#cluster_name","text":"Required. Specify the name of the cluster Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/ocp_provision/#ocp_version","text":"Required. Specify the version of OCP to install. The exact format of this will vary depending on cluster_type . For ROKS clusters the format is 4.6_openshift , 4.8_openshift , for Fyre it is 4.6.16 . Environment Variable: OCP_VERSION Default Value: None","title":"ocp_version"},{"location":"roles/ocp_provision/#role-variables-gpu-node-support","text":"","title":"Role Variables - GPU Node Support"},{"location":"roles/ocp_provision/#ocp_provision_gpu","text":"Flag that determines if GPU worker nodes should be added during cluster creation (eg. needed for MVI application). This is currently only set up for ROKS clusters. Environment Variable: OCP_PROVISION_GPU Default Value: false","title":"ocp_provision_gpu"},{"location":"roles/ocp_provision/#gpu_workerpool_name","text":"The name of the gpu worker pool to added to or modify in the cluster. If already existing, use the existing name to avoid recreating another gpu worker pool unless that is the goal. Environment Variable: GPU_WORKERPOOL_NAME Default Value: gpu","title":"gpu_workerpool_name"},{"location":"roles/ocp_provision/#gpu_workers","text":"The number of GPU worker nodes that will be deploy in the cluster. The node created will have mg4c.32x384.2xp100-GPU flavor. This variable depends on ocp_provision_gpu and is currently supported on ROKS clusters only. Environment Variable: GPU_WORKERS Default Value: 1","title":"gpu_workers"},{"location":"roles/ocp_provision/#compute_node_count","text":"The number of compute nodes (i.e. worker nodes) allocate to the OCP cluster. Environment Variable: COMPUTE_NODE_COUNT Default Value: 3","title":"compute_node_count"},{"location":"roles/ocp_provision/#controlplane_node_count","text":"The number of control plane nodes (i.e. master nodes) allocate to the OCP cluster. Environment Variable: CONTROLPLANE_NODE_COUNT Default Value: 3","title":"controlplane_node_count"},{"location":"roles/ocp_provision/#gpu_workerpool_name_1","text":"The name of the gpu worker pool to added to or modify in the cluster. If already existing, use the existing name to avoid recreating another gpu worker pool unless that is the goal. Environment Variable: GPU_WORKERPOOL_NAME Default Value: gpu","title":"gpu_workerpool_name"},{"location":"roles/ocp_provision/#role-variables-roks","text":"The following variables are only used when cluster_type = roks .","title":"Role Variables - ROKS"},{"location":"roles/ocp_provision/#ibmcloud_apikey","text":"Required if cluster_type = roks . The APIKey to be used by ibmcloud login comand. Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/ocp_provision/#ibmcloud_resourcegroup","text":"The resource group to create the cluster inside. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default","title":"ibmcloud_resourcegroup"},{"location":"roles/ocp_provision/#roks_zone","text":"IBM Cloud zone where the cluster should be provisioned. Environment Variable: ROKS_ZONE Default Value: dal10","title":"roks_zone"},{"location":"roles/ocp_provision/#roks_flavor","text":"Worker node flavor Environment Variable: ROKS_FLAVOR Default Value: b3c.16x64.300gb","title":"roks_flavor"},{"location":"roles/ocp_provision/#roks_workers","text":"Number of worker nodes for the roks cluster Environment Variable: ROKS_WORKERS Default Value: 3","title":"roks_workers"},{"location":"roles/ocp_provision/#roks_flags","text":"Can be used to specify additional parameters for the cluster creation Environment Variable: ROKS_FLAGS Default Value: None","title":"roks_flags"},{"location":"roles/ocp_provision/#role-variables-rosa","text":"The following variables are only used when cluster_type = rosa .","title":"Role Variables - ROSA"},{"location":"roles/ocp_provision/#rosa_token","text":"Token to authenticate to the ROSA service. To obtain your API token login to the OpenShift cluster manager . Required if cluster_type = rosa . Environment Variable: ROSA_TOKEN Default Value: None","title":"rosa_token"},{"location":"roles/ocp_provision/#rosa_cluster_admin_password","text":"Password to set up for the cluster-admin user account on the OCP instance. You will need this to log onto the cluster after it is provisioned. Required if cluster_type = rosa . Environment Variable: ROSA_CLUSTER_ADMIN_PASSWORD Default Value: None","title":"rosa_cluster_admin_password"},{"location":"roles/ocp_provision/#rosa_compute_nodes","text":"Number of compute nodes to deploy in the cluster. Optional Environment Variable: ROSA_COMPUTE_NODES Default Value: 3","title":"rosa_compute_nodes"},{"location":"roles/ocp_provision/#role-variables-fyre","text":"The following variables are only used when cluster_type = fyre .","title":"Role Variables - FYRE"},{"location":"roles/ocp_provision/#fyre_username","text":"Username to authenticate with Fyre API. Required if cluster_type = fyre . Environment Variable: FYRE_USERNAME Default Value: None","title":"fyre_username"},{"location":"roles/ocp_provision/#fyre_apikey","text":"API key to authenticate with Fyre API. Required if cluster_type = fyre . Environment Variable: FYRE_APIKEY Default Value: None","title":"fyre_apikey"},{"location":"roles/ocp_provision/#fyre_quota_type","text":"Type of quota to draw from when provisioning the cluster, valid options are quick_burn and product_group . Required if cluster_type = fyre . Environment Variable: FYRE_QUOTA_TYPE Default Value: quick_burn","title":"fyre_quota_type"},{"location":"roles/ocp_provision/#fyre_product_id","text":"The Product ID that the cluster will be associated with for accounting purposes. Required if cluster_type = fyre . Environment Variable: FYRE_PRODUCT_ID Default Value: None","title":"fyre_product_id"},{"location":"roles/ocp_provision/#fyre_cluster_description","text":"Provide a description for the cluster. Optional Environment Variable: FYRE_CLUSTER_DESCRIPTION Default Value: None","title":"fyre_cluster_description"},{"location":"roles/ocp_provision/#fyre_cluster_size","text":"The name of one of Fyre's pre-defined cluster sizes to use for the new cluster. Required when cluster_type = fyre and fyre_quota_type = quick_burn . Environment Variable: FYRE_CLUSTER_SIZE Default Value: large","title":"fyre_cluster_size"},{"location":"roles/ocp_provision/#fyre_worker_count","text":"The number of worker nodes to provision in the cluster. Required when cluster_type = fyre and fyre_quota_type = quick_burn . Environment Variable: FYRE_WORKER_COUNT Default Value: 3","title":"fyre_worker_count"},{"location":"roles/ocp_provision/#fyre_worker_cpu","text":"The amount of CPU to assign to each worker node (maximum value supported by FYRE 16). Required when cluster_type = fyre and fyre_quota_type = quick_burn . Environment Variable: FYRE_WORKER_CPU Default Value: 16","title":"fyre_worker_cpu"},{"location":"roles/ocp_provision/#fyre_worker_memory","text":"The amount of memory to assign to each worker node (maximum value supported by FYRE 64). Required when cluster_type = fyre and fyre_quota_type = quick_burn . Environment Variable: FYRE_WORKER_MEMORY Default Value: 64","title":"fyre_worker_memory"},{"location":"roles/ocp_provision/#role-variables-ipi","text":"These variables are only used when cluster_type = ipi . Note IPI stands for Installer Provisioned Infrastructure . OpenShift offers two possible deployment methods: IPI and UPI (User Provisioned Infrastructure). The difference is the degree of automation and customization. IPI will not only deploy OpenShift but also all infrastructure components and configurations.","title":"Role Variables - IPI"},{"location":"roles/ocp_provision/#ipi_platform","text":"Platform to create the cluster on. Technically, any platform supported by openshift-install should work here, but currently we have only specifically tested on aws , which is the default value. Optional when cluster_type = ipi Environment Variable: IPI_PLATFORM Default Value: aws","title":"ipi_platform"},{"location":"roles/ocp_provision/#ipi_region","text":"Platform region where OCP cluster will be created. Optional when cluster_type = ipi Environment Variable: IPI_REGION Default Value: us-east-1","title":"ipi_region"},{"location":"roles/ocp_provision/#ipi_base_domain","text":"Specify the base domain of the cluster that will be provisioned. Required when cluster_type = aipi Environment Variable: IPI_BASE_DOMAIN Default Value: None","title":"ipi_base_domain"},{"location":"roles/ocp_provision/#ipi_pull_secret_file","text":"Location of the file containing your Redhat OpenShift pull secret. This file can be obtained from the Red Hat Hybrid Cloud Console Required when cluster_type = ipi Environment Variable: IPI_PULL_SECRET_FILE Default Value: None","title":"ipi_pull_secret_file"},{"location":"roles/ocp_provision/#ipi_dir","text":"The working directory that is used to perform the installation, it will contain the openshift-install executable, it's configuration files, & any generated logs. Optional when cluster_type = ipi Environment Variable: IPI_DIR Default Value: ~/openshift-install","title":"ipi_dir"},{"location":"roles/ocp_provision/#ipi_controlplane_type","text":"Control plane node type. Optional when cluster_type = ipi Environment Variable: IPI_CONTROLPLANE_TYPE Default Value: m5.4xlarge","title":"ipi_controlplane_type"},{"location":"roles/ocp_provision/#ipi_controlplane_replicas","text":"The number of master nodes to provision to form the control plane of your cluster. Optional when cluster_type = ipi Environment Variable: IPI_CONTROLPLANE_REPLICAS Default Value: 3","title":"ipi_controlplane_replicas"},{"location":"roles/ocp_provision/#ipi_compute_type","text":"Compute node type. Optional when cluster_type = ipi Environment Variable: IPI_COMPUTE_TYPE Default Value: m5.4xlarge","title":"ipi_compute_type"},{"location":"roles/ocp_provision/#ipi_compute_replicas","text":"The number of worker nodes to provsision in the cluster, providing your compute resource. Optional when cluster_type = ipi Environment Variable: IPI_COMPUTE_REPLICAS Default Value: 3","title":"ipi_compute_replicas"},{"location":"roles/ocp_provision/#role-variables-aws","text":"The following variables are only used when cluster_type = ipi and ipi_platform = aws .","title":"Role Variables - AWS"},{"location":"roles/ocp_provision/#aws_access_key_id","text":"AWS access key associated with an IAM user or role. Make sure the access key has permissions to create instances. Required when cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_ACCESS_KEY_ID Default Value: None","title":"aws_access_key_id"},{"location":"roles/ocp_provision/#aws_secret_access_key","text":"AWS secret access key associated with an IAM user or role. Required when cluster_type = aws-ipi and ipi_platform = aws Environment Variable: AWS_SECRET_ACCESS_KEY Default Value: None","title":"aws_secret_access_key"},{"location":"roles/ocp_provision/#example-playbook","text":"- hosts: localhost vars: cluster_type: roks cluster_name: mycluster ocp_version: 4.10 ibmcloud_apikey: xxxxx roles: - ibm.mas_devops.ocp_provision","title":"Example Playbook"},{"location":"roles/ocp_provision/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_roks_upgrade_registry_storage/","text":"ocp_roks_upgrade_registry_storage \uf0c1 This role will use IBMCloud APIs to upgrade the capacity of the volume backing the OCP cluster's image registry. The volume will be increased from the default capacity of 100GB to 400GB. This is needed if you intend to install all of the services available in CloudPak for Data because the 100GB volume is not large enough. Role Variables \uf0c1 ibmcloud_apikey \uf0c1 The APIKey to be used to modify the storage volume associated with the image registry. Environment Variable: IBMCLOUD_APIKEY Default Value: None Example Playbook \uf0c1 - hosts: localhost roles: - ibm.mas_devops.ocp_roks_tuning License \uf0c1 EPL-2.0","title":"ocp_roks_upgrade_registry_storage"},{"location":"roles/ocp_roks_upgrade_registry_storage/#ocp_roks_upgrade_registry_storage","text":"This role will use IBMCloud APIs to upgrade the capacity of the volume backing the OCP cluster's image registry. The volume will be increased from the default capacity of 100GB to 400GB. This is needed if you intend to install all of the services available in CloudPak for Data because the 100GB volume is not large enough.","title":"ocp_roks_upgrade_registry_storage"},{"location":"roles/ocp_roks_upgrade_registry_storage/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_roks_upgrade_registry_storage/#ibmcloud_apikey","text":"The APIKey to be used to modify the storage volume associated with the image registry. Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/ocp_roks_upgrade_registry_storage/#example-playbook","text":"- hosts: localhost roles: - ibm.mas_devops.ocp_roks_tuning","title":"Example Playbook"},{"location":"roles/ocp_roks_upgrade_registry_storage/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_upgrade/","text":"ocp_upgrade \uf0c1 This role supports the upgrade of the Openshift Cluster version in IBM Cloud provider, from version 4.6 to 4.7 then 4.8 (latest patch) which is the supported version of Openshift to run MAS in 8.7 release. Role Variables \uf0c1 cluster_type \uf0c1 Required. Specify the cluster type, only IBM Cloud Openshift Clusters are supported by this role at the moment. If you provide a different cluster type than roks , this role will fail. Environment Variable: CLUSTER_TYPE Default Value: None cluster_name \uf0c1 Required. Specify the name of the cluster to be upgraded. Environment Variable: CLUSTER_NAME Default Value: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.ocp_upgrade License \uf0c1 EPL-2.0","title":"ocp_upgrade"},{"location":"roles/ocp_upgrade/#ocp_upgrade","text":"This role supports the upgrade of the Openshift Cluster version in IBM Cloud provider, from version 4.6 to 4.7 then 4.8 (latest patch) which is the supported version of Openshift to run MAS in 8.7 release.","title":"ocp_upgrade"},{"location":"roles/ocp_upgrade/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_upgrade/#cluster_type","text":"Required. Specify the cluster type, only IBM Cloud Openshift Clusters are supported by this role at the moment. If you provide a different cluster type than roks , this role will fail. Environment Variable: CLUSTER_TYPE Default Value: None","title":"cluster_type"},{"location":"roles/ocp_upgrade/#cluster_name","text":"Required. Specify the name of the cluster to be upgraded. Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/ocp_upgrade/#example-playbook","text":"- hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.ocp_upgrade","title":"Example Playbook"},{"location":"roles/ocp_upgrade/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_verify/","text":"ocp_verify \uf0c1 This role will verify that the target OCP cluster is ready to be setup for MAS. For example, in IBMCloud ROKS we have seen delays of over an hour before the Red Hat Operator catalog is ready to use. This will cause attempts to install anything from that CatalogSource to fail as the timeouts built into the roles in this collection are designed to catch problems with an install, rather than a half-provisioned cluster that is not properly ready to use yet. Role Variables \uf0c1 cluster_name \uf0c1 Specify the name of the cluster, in some cluster setups this name is required to determine the name of the default router certificate. Optional Environment Variable: CLUSTER_NAME Default Value: None cluster ingres tls secret name \uf0c1 Specify the name of the cluster's ingres tls secret which contains the default router certificate. Optional Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default Value: router-certs-default Example Playbook \uf0c1 - hosts: localhost roles: - ibm.mas_devops.ocp_verify License \uf0c1 EPL-2.0","title":"ocp_verify"},{"location":"roles/ocp_verify/#ocp_verify","text":"This role will verify that the target OCP cluster is ready to be setup for MAS. For example, in IBMCloud ROKS we have seen delays of over an hour before the Red Hat Operator catalog is ready to use. This will cause attempts to install anything from that CatalogSource to fail as the timeouts built into the roles in this collection are designed to catch problems with an install, rather than a half-provisioned cluster that is not properly ready to use yet.","title":"ocp_verify"},{"location":"roles/ocp_verify/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_verify/#cluster_name","text":"Specify the name of the cluster, in some cluster setups this name is required to determine the name of the default router certificate. Optional Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/ocp_verify/#cluster-ingres-tls-secret-name","text":"Specify the name of the cluster's ingres tls secret which contains the default router certificate. Optional Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default Value: router-certs-default","title":"cluster ingres tls secret name"},{"location":"roles/ocp_verify/#example-playbook","text":"- hosts: localhost roles: - ibm.mas_devops.ocp_verify","title":"Example Playbook"},{"location":"roles/ocp_verify/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocs/","text":"ocs \uf0c1 This role provides support to install Openshift Container Storage. This role is not used by default when setting up IBM Cloud ROKS clusters because they are automatically provisioned with their own storage plugin already. Unfortunately, starting fromOCP 4.8 IBM/Red Hat have decided to stop supporting OCS on IBMCloud ROKS. So this role is of limited value to users of ROKS going forward. If you attempt to install OpenShift Container Storage on ROKS via a Subscription channel you will be met by the following error as the admission webhook has been coded to prevent use of the OCS operator on IBM Cloud ROKS: Failed to apply object: \"admission webhook \"validate.managed.openshift.io\" denied the request: Installing OpenShift Data Foundation on IBM Cloud by using OperatorHub is not supported. You can install OpenShift Data Foundation by using the IBM Cloud add-on. For more information, see https://cloud.ibm.com/docs/openshift?topic=openshift-ocs-storage-prep. Role Variables \uf0c1 ocp_release \uf0c1 Set this to e.g. 4.6 , 4.7 , 4.8 . We need to know what the release level is to know what channel to target the operator subscriptions at. Environment Variable: OCP_RELEASE Default Value: 4.10 Example Playbook \uf0c1 --- - hosts: localhost any_errors_fatal: true vars: ocp_release: \"4.8\" roles: - ibm.mas_devops.ocs License \uf0c1 EPL-2.0","title":"ocs"},{"location":"roles/ocs/#ocs","text":"This role provides support to install Openshift Container Storage. This role is not used by default when setting up IBM Cloud ROKS clusters because they are automatically provisioned with their own storage plugin already. Unfortunately, starting fromOCP 4.8 IBM/Red Hat have decided to stop supporting OCS on IBMCloud ROKS. So this role is of limited value to users of ROKS going forward. If you attempt to install OpenShift Container Storage on ROKS via a Subscription channel you will be met by the following error as the admission webhook has been coded to prevent use of the OCS operator on IBM Cloud ROKS: Failed to apply object: \"admission webhook \"validate.managed.openshift.io\" denied the request: Installing OpenShift Data Foundation on IBM Cloud by using OperatorHub is not supported. You can install OpenShift Data Foundation by using the IBM Cloud add-on. For more information, see https://cloud.ibm.com/docs/openshift?topic=openshift-ocs-storage-prep.","title":"ocs"},{"location":"roles/ocs/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocs/#ocp_release","text":"Set this to e.g. 4.6 , 4.7 , 4.8 . We need to know what the release level is to know what channel to target the operator subscriptions at. Environment Variable: OCP_RELEASE Default Value: 4.10","title":"ocp_release"},{"location":"roles/ocs/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true vars: ocp_release: \"4.8\" roles: - ibm.mas_devops.ocs","title":"Example Playbook"},{"location":"roles/ocs/#license","text":"EPL-2.0","title":"License"},{"location":"roles/sbo/","text":"sbo \uf0c1 This role provides support to install RedHat Service Binding Operator. The role declares a dependency on ocp_verify to ensure that the RedHat Operator Catalog is installed and ready before we try to install the Service Binding Operator from that catalog. For MAS 8.6 or earlier (or when running MAS 8.7 on OCP 4.6) Service Binding Operator v0.8 will be installed from the preview channel. It is important not to upgrade to later preview builds as they are incompatible with MAS due to breaking API changes in SBO. For MAS 8.7 the stable channel will be used instead, with automatic updates enabled. In both cases, the operator will be installed in the openshift-operators namespace with cluster scope. For MAS 8.8 the Service Binding Operator is no longer required, calling this role with mas_channel set to 8.8 (or later) will result in no action taken. This is also the default if no value is provided for mas_channel . Role Variables \uf0c1 mas_channel \uf0c1 Used to determine whether to install SBO stable channel and the IBM badged cert-manager. Optional, but if this is not set SBO will not be installed Environment Variable: MAS_CHANNEL Default Value: None Example Playbook \uf0c1 - hosts: localhost roles: - ibm.mas_devops.sbo License \uf0c1 EPL-2.0","title":"sbo"},{"location":"roles/sbo/#sbo","text":"This role provides support to install RedHat Service Binding Operator. The role declares a dependency on ocp_verify to ensure that the RedHat Operator Catalog is installed and ready before we try to install the Service Binding Operator from that catalog. For MAS 8.6 or earlier (or when running MAS 8.7 on OCP 4.6) Service Binding Operator v0.8 will be installed from the preview channel. It is important not to upgrade to later preview builds as they are incompatible with MAS due to breaking API changes in SBO. For MAS 8.7 the stable channel will be used instead, with automatic updates enabled. In both cases, the operator will be installed in the openshift-operators namespace with cluster scope. For MAS 8.8 the Service Binding Operator is no longer required, calling this role with mas_channel set to 8.8 (or later) will result in no action taken. This is also the default if no value is provided for mas_channel .","title":"sbo"},{"location":"roles/sbo/#role-variables","text":"","title":"Role Variables"},{"location":"roles/sbo/#mas_channel","text":"Used to determine whether to install SBO stable channel and the IBM badged cert-manager. Optional, but if this is not set SBO will not be installed Environment Variable: MAS_CHANNEL Default Value: None","title":"mas_channel"},{"location":"roles/sbo/#example-playbook","text":"- hosts: localhost roles: - ibm.mas_devops.sbo","title":"Example Playbook"},{"location":"roles/sbo/#license","text":"EPL-2.0","title":"License"},{"location":"roles/sbo_upgrade/","text":"sbo_upgrade \uf0c1 This role will upgrade Service Binding Operator version 0.8v (preview channel) to 1.0.x (stable channel), which is the supported SBO version for MAS 8.7+ For more information, please refer to Upgrading Maximo Application Suite documentation. Role Variables \uf0c1 mas_instance_id \uf0c1 Required - Defines the instance id that is used in the existing MAS installation, will be used to lookup the existing Service Binding resources associated to your MAS instance, to ensure it is ready to support SBO version 1.0.x. Environment Variable: MAS_INSTANCE_ID Default Value: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.sbo_upgrade","title":"sbo_upgrade"},{"location":"roles/sbo_upgrade/#sbo_upgrade","text":"This role will upgrade Service Binding Operator version 0.8v (preview channel) to 1.0.x (stable channel), which is the supported SBO version for MAS 8.7+ For more information, please refer to Upgrading Maximo Application Suite documentation.","title":"sbo_upgrade"},{"location":"roles/sbo_upgrade/#role-variables","text":"","title":"Role Variables"},{"location":"roles/sbo_upgrade/#mas_instance_id","text":"Required - Defines the instance id that is used in the existing MAS installation, will be used to lookup the existing Service Binding resources associated to your MAS instance, to ensure it is ready to support SBO version 1.0.x. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/sbo_upgrade/#example-playbook","text":"- hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.sbo_upgrade","title":"Example Playbook"},{"location":"roles/sls/","text":"sls \uf0c1 Install IBM Suite License Service and generate a configuration that can be directly applied to IBM Maximo Application Suite. The role assumes that you have already installed the Certificate Manager in the target cluster. This action is performed by the cert_manager role if you want to use this collection to install the cert-manager operator. Role Variables - Installation \uf0c1 If sls_url is set then the role will skip the installation of an SLS instance and simply generate the SLSCfg resource for the SLS instance defined. ibm_entitlement_key \uf0c1 Provide your IBM entitlement key . Required unless sls_url is provided Environment Variable: IBM_ENTITLEMENT_KEY Default: None sls_entitlement_key \uf0c1 An IBM entitlement key specific for SLS installation, primarily used to override ibm_entitlement_key in development. Optional Environment Variable: SLS_ENTITLEMENT_KEY Default: None sls_catalog_source \uf0c1 Defines the OLM catalog to be used to install SLS. Set to ibm-sls-operators if you want to deploy pre-release development builds of SLS or leave as the default ibm-operator-catalog for the released versions. Optional Environment Variable: SLS_CATALOG_SOURCE Default: ibm-operator-catalog sls_channel \uf0c1 The SLS OLM subscription channel to be installed. Optional Environment Variable: SLS_CHANNEL Default: 3.x sls_namespace \uf0c1 Define the namespace where sls must be installed. Optional Environment Variable: SLS_NAMESPACE Default: ibm-sls sls_icr_cp \uf0c1 The container registry source for all container images deployed by the SLS operator. Override to use development images Optional Environment Variable: SLS_ICR_CP Default: cp.icr.io/cp sls_icr_cpopen \uf0c1 The container registry source for the SLS operator container image. Override to use development image Optional Environment Variable: SLS_ICR_CPOPEN Default: icr.io/cpopen sls_instance_name \uf0c1 Defines the instance ID to be used for SLS installation. Optional Environment Variable: SLS_INSTANCE_NAME Default: sls sls_entitlement_username \uf0c1 Username for entitled registry. This username will be used to create the image pull secret. Optional Environment Variable: SLS_ENTITLEMENT_USERNAME Default: cp Role Variables - Configuration \uf0c1 sls_domain \uf0c1 SLS can be configured to be externally accessible through a route by setting the domain. Set the domain if SLS is used by application suites that are installed in separate OpenShift clusters. Optional Environment Variable: SLS_DOMAIN Default: None sls_auth_enforce \uf0c1 Determines whether authorization is enforced. If set to true, clients must use mTLS with certificates generated from the client registration flow for SLS API calls. Optional Environment Variable: SLS_AUTH_ENFORCE Default: True sls_compliance_enforce \uf0c1 Determines whether compliance is enforced. If there are not enough tokens to support the request. If compliance is not enforced, license checkout requests will be allowed even if there are not enough tokens to support the request. Optional Environment Variable: SLS_COMPLIANCE_ENFORCE Default: True sls_registration_open \uf0c1 Determines whether registration is open. If set to true, clients will be able to register themselves with SLS and use SLS APIs. Optional Environment Variable: SLS_REGISTRATION_OPEN Default: True sls_mongodb_cfg_file \uf0c1 Location of a MAS MongoCfg defintion (as generated by the mongodb role). If provided the role will use the information in that config file to configure SLS. Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS. Environment Variable: SLS_MONGODB_CFG_FILE Default: None sls_mongodb.hosts \uf0c1 Defines list of host and port pair for MongoDb to be used with SLS. Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS. Environment Variable: None Default: None sls_mongodb.username \uf0c1 Defines the MongoDB Username. Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS. Environment Variable: None Default: None sls_mongodb.password \uf0c1 Defines the MongoDb Password. Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS. Environment Variable: None Default: None Role Variables - Bootstrap \uf0c1 bootstrap.license_id \uf0c1 Defines the License Id to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on Optional Environment Variable: SLS_LICENSE_ID Default: None bootstrap.registration_key \uf0c1 Defines the Registration Key to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on Optional Environment Variable: SLS_REGISTRATION_KEY Default: None bootstrap.license_file \uf0c1 Defines the License File to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on. Note: this variable used to be called bootstrap.entitlement_file and defaulted to {{mas_config_dir}}/entitlement.lic , this is no longer the case and SLS_LICENSE_FILE has to be set in order to bootstrap. Optional Environment Variable: SLS_LICENSE_FILE Default: None Role Variables - SLSCfg \uf0c1 mas_instance_id \uf0c1 The instance ID of Maximo Application Suite that the SlsCfg configuration will target. Optional, if this or mas_config_dir are not set then the role will not generate a SlsCfg template Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \uf0c1 Local directory to save the generated SlsCfg resource definition. This can be used to manually configure a MAS instance to connect to SLS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a SlsCfg template. Optional, if this or mas_config_dir are not set then the role will not generate a SlsCfg template Environment Variable: MAS_CONFIG_DIR Default Value: None sls_url \uf0c1 The URL of the LicenseService to be called when the Maximo Application Suite is registered with SLS. Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_URL Default Value: None sls_tls_crt \uf0c1 The TLS CA certiticate of the LicenseService to be used when the Maximo Application Suite is registered with SLS. Takes precedence over sls_tls_crt_local_file_path . Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_TLS_CERT Default Value: None sls_tls_crt_local_file_path \uf0c1 The path on the local system to a file containing the TLS CA certiticate of the LicenseService to be used when the Maximo Application Suite is registered with SLS. This variable is only used if sls_tls_crt has not been set. Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_TLS_CERT_LOCAL_FILE Default Value: None sls_registration_key \uf0c1 The Registration key of the LicenseService instance to be used when the Maximo Application Suite is registered with SLS. Optional Environment Variable: SLS_REGISTRATION_KEY Default Value: None Example Playbook \uf0c1 Install and generate a configuration \uf0c1 - hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxx mas_instance_id: inst1 mas_config_dir: /home/me/masconfig sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" bootstrap: license_id: \"aa78dd65ef10\" license_file: \"/etc/mas/entitlement.lic\" registration_key: xxxx roles: - ibm.mas_devops.sls Generate a configuration for an existing installation \uf0c1 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 mas_config_dir: /home/me/masconfig sls_tls_crt_local_file_path: \"/home/me/sls.crt\" slscfg_url: \"https://xxx\" slscfg_registration_key: \"xxx\" roles: - ibm.mas_devops.sls License \uf0c1 EPL-2.0","title":"sls"},{"location":"roles/sls/#sls","text":"Install IBM Suite License Service and generate a configuration that can be directly applied to IBM Maximo Application Suite. The role assumes that you have already installed the Certificate Manager in the target cluster. This action is performed by the cert_manager role if you want to use this collection to install the cert-manager operator.","title":"sls"},{"location":"roles/sls/#role-variables-installation","text":"If sls_url is set then the role will skip the installation of an SLS instance and simply generate the SLSCfg resource for the SLS instance defined.","title":"Role Variables - Installation"},{"location":"roles/sls/#ibm_entitlement_key","text":"Provide your IBM entitlement key . Required unless sls_url is provided Environment Variable: IBM_ENTITLEMENT_KEY Default: None","title":"ibm_entitlement_key"},{"location":"roles/sls/#sls_entitlement_key","text":"An IBM entitlement key specific for SLS installation, primarily used to override ibm_entitlement_key in development. Optional Environment Variable: SLS_ENTITLEMENT_KEY Default: None","title":"sls_entitlement_key"},{"location":"roles/sls/#sls_catalog_source","text":"Defines the OLM catalog to be used to install SLS. Set to ibm-sls-operators if you want to deploy pre-release development builds of SLS or leave as the default ibm-operator-catalog for the released versions. Optional Environment Variable: SLS_CATALOG_SOURCE Default: ibm-operator-catalog","title":"sls_catalog_source"},{"location":"roles/sls/#sls_channel","text":"The SLS OLM subscription channel to be installed. Optional Environment Variable: SLS_CHANNEL Default: 3.x","title":"sls_channel"},{"location":"roles/sls/#sls_namespace","text":"Define the namespace where sls must be installed. Optional Environment Variable: SLS_NAMESPACE Default: ibm-sls","title":"sls_namespace"},{"location":"roles/sls/#sls_icr_cp","text":"The container registry source for all container images deployed by the SLS operator. Override to use development images Optional Environment Variable: SLS_ICR_CP Default: cp.icr.io/cp","title":"sls_icr_cp"},{"location":"roles/sls/#sls_icr_cpopen","text":"The container registry source for the SLS operator container image. Override to use development image Optional Environment Variable: SLS_ICR_CPOPEN Default: icr.io/cpopen","title":"sls_icr_cpopen"},{"location":"roles/sls/#sls_instance_name","text":"Defines the instance ID to be used for SLS installation. Optional Environment Variable: SLS_INSTANCE_NAME Default: sls","title":"sls_instance_name"},{"location":"roles/sls/#sls_entitlement_username","text":"Username for entitled registry. This username will be used to create the image pull secret. Optional Environment Variable: SLS_ENTITLEMENT_USERNAME Default: cp","title":"sls_entitlement_username"},{"location":"roles/sls/#role-variables-configuration","text":"","title":"Role Variables - Configuration"},{"location":"roles/sls/#sls_domain","text":"SLS can be configured to be externally accessible through a route by setting the domain. Set the domain if SLS is used by application suites that are installed in separate OpenShift clusters. Optional Environment Variable: SLS_DOMAIN Default: None","title":"sls_domain"},{"location":"roles/sls/#sls_auth_enforce","text":"Determines whether authorization is enforced. If set to true, clients must use mTLS with certificates generated from the client registration flow for SLS API calls. Optional Environment Variable: SLS_AUTH_ENFORCE Default: True","title":"sls_auth_enforce"},{"location":"roles/sls/#sls_compliance_enforce","text":"Determines whether compliance is enforced. If there are not enough tokens to support the request. If compliance is not enforced, license checkout requests will be allowed even if there are not enough tokens to support the request. Optional Environment Variable: SLS_COMPLIANCE_ENFORCE Default: True","title":"sls_compliance_enforce"},{"location":"roles/sls/#sls_registration_open","text":"Determines whether registration is open. If set to true, clients will be able to register themselves with SLS and use SLS APIs. Optional Environment Variable: SLS_REGISTRATION_OPEN Default: True","title":"sls_registration_open"},{"location":"roles/sls/#sls_mongodb_cfg_file","text":"Location of a MAS MongoCfg defintion (as generated by the mongodb role). If provided the role will use the information in that config file to configure SLS. Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS. Environment Variable: SLS_MONGODB_CFG_FILE Default: None","title":"sls_mongodb_cfg_file"},{"location":"roles/sls/#sls_mongodbhosts","text":"Defines list of host and port pair for MongoDb to be used with SLS. Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS. Environment Variable: None Default: None","title":"sls_mongodb.hosts"},{"location":"roles/sls/#sls_mongodbusername","text":"Defines the MongoDB Username. Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS. Environment Variable: None Default: None","title":"sls_mongodb.username"},{"location":"roles/sls/#sls_mongodbpassword","text":"Defines the MongoDb Password. Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS. Environment Variable: None Default: None","title":"sls_mongodb.password"},{"location":"roles/sls/#role-variables-bootstrap","text":"","title":"Role Variables - Bootstrap"},{"location":"roles/sls/#bootstraplicense_id","text":"Defines the License Id to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on Optional Environment Variable: SLS_LICENSE_ID Default: None","title":"bootstrap.license_id"},{"location":"roles/sls/#bootstrapregistration_key","text":"Defines the Registration Key to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on Optional Environment Variable: SLS_REGISTRATION_KEY Default: None","title":"bootstrap.registration_key"},{"location":"roles/sls/#bootstraplicense_file","text":"Defines the License File to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on. Note: this variable used to be called bootstrap.entitlement_file and defaulted to {{mas_config_dir}}/entitlement.lic , this is no longer the case and SLS_LICENSE_FILE has to be set in order to bootstrap. Optional Environment Variable: SLS_LICENSE_FILE Default: None","title":"bootstrap.license_file"},{"location":"roles/sls/#role-variables-slscfg","text":"","title":"Role Variables - SLSCfg"},{"location":"roles/sls/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the SlsCfg configuration will target. Optional, if this or mas_config_dir are not set then the role will not generate a SlsCfg template Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/sls/#mas_config_dir","text":"Local directory to save the generated SlsCfg resource definition. This can be used to manually configure a MAS instance to connect to SLS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a SlsCfg template. Optional, if this or mas_config_dir are not set then the role will not generate a SlsCfg template Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/sls/#sls_url","text":"The URL of the LicenseService to be called when the Maximo Application Suite is registered with SLS. Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_URL Default Value: None","title":"sls_url"},{"location":"roles/sls/#sls_tls_crt","text":"The TLS CA certiticate of the LicenseService to be used when the Maximo Application Suite is registered with SLS. Takes precedence over sls_tls_crt_local_file_path . Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_TLS_CERT Default Value: None","title":"sls_tls_crt"},{"location":"roles/sls/#sls_tls_crt_local_file_path","text":"The path on the local system to a file containing the TLS CA certiticate of the LicenseService to be used when the Maximo Application Suite is registered with SLS. This variable is only used if sls_tls_crt has not been set. Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_TLS_CERT_LOCAL_FILE Default Value: None","title":"sls_tls_crt_local_file_path"},{"location":"roles/sls/#sls_registration_key","text":"The Registration key of the LicenseService instance to be used when the Maximo Application Suite is registered with SLS. Optional Environment Variable: SLS_REGISTRATION_KEY Default Value: None","title":"sls_registration_key"},{"location":"roles/sls/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/sls/#install-and-generate-a-configuration","text":"- hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxx mas_instance_id: inst1 mas_config_dir: /home/me/masconfig sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" bootstrap: license_id: \"aa78dd65ef10\" license_file: \"/etc/mas/entitlement.lic\" registration_key: xxxx roles: - ibm.mas_devops.sls","title":"Install and generate a configuration"},{"location":"roles/sls/#generate-a-configuration-for-an-existing-installation","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 mas_config_dir: /home/me/masconfig sls_tls_crt_local_file_path: \"/home/me/sls.crt\" slscfg_url: \"https://xxx\" slscfg_registration_key: \"xxx\" roles: - ibm.mas_devops.sls","title":"Generate a configuration for an existing installation"},{"location":"roles/sls/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_config/","text":"suite_app_config \uf0c1 This role is used to configure specific components of the application workspace after the application has been installed in the Maximo Application Suite. Role Variables \uf0c1 mas_instance_id \uf0c1 Defines the instance id that was used for the MAS installation mas_app_id \uf0c1 Defines the kind of application that is intended for installation such as assist , health , iot , manage , monitor , mso , predict , or safety mas_workspace_id \uf0c1 MAS application workspace to use to configure app components mas_config_dir \uf0c1 Optional. Local directory where generated resource definitions are saved into. It is used by current role to retrieve the id of the analytics project eventually created by cp4d_service role and then configure it into Health & Predict - Utilities resource. Optional, only supported when mas_app_id = hputilities and cpd_ws_project_name is informed. Environment Variable: MAS_CONFIG_DIR Default Value: None mas_appws_components \uf0c1 Defines the app components and versions to configure in the application workspace. Takes the form of key=value pairs seperated by a comma i.e. To install health within Manage set base=latest,health=latest Environment Variable: MAS_APPWS_COMPONENTS Default: For Manage the default is: base=latest For Health (standalone) the default is: health=latest mas_app_ws_spec \uf0c1 Optional. The application workspace deployment spec used to configure various aspects of the application workspace configuration. Note that use of this will override anything set in mas_appws_components Environment Variable: MAS_APP_WS_SPEC Default: defaults are specified in vars/defaultspecs/{{mas_app_id}}.yml Role Variables - Health & Predict Utilities \uf0c1 cpd_ws_project_id \uf0c1 Optional. It is the id of the analytics project created in Watson Studio and used to configure hputilities application. Only supported when mas_app_id = hputilities Environment Variable: CPD_WS_PROJECT_ID Default: None cpd_ws_project_name \uf0c1 Optional. It specifies the name of the file in mas_config_dir where the id of the analytics project is saved. Only supported when mas_app_id = hputilities and mas_config_dir is informed. Environment Variable: CPD_WS_PROJECT_NAME Default Value: wsl-mas-${mas_instance_id}-hputilities Role Variables - Manage \uf0c1 mas_app_settings_aio_flag \uf0c1 Optional. Flag indicating if Asset Investment Optimization (AIO) resource must be loaded or not. It can be loaded only when Optimizer application is installed. Only supported when Optimizer application is installed. Environment Variable: MAS_APP_SETTINGS_AIO_FLAG Default: true mas_app_settings_db2_schema \uf0c1 Optional. Name of the schema where Manage database lives in. Environment Variable: MAS_APP_SETTINGS_DB2_SCHEMA') | Default: maximo mas_app_settings_demodata \uf0c1 Optional. Flag indicating if manage demodata should be loaded or not. Environment Variable: MAS_APP_SETTINGS_DEMODATA Default: false (do not load demodata) mas_app_settings_base_language \uf0c1 Optional. Provide the base language for Manage application. For a full list of supported languages for Manage application and its corresponding language codes, please refer to Language Support documentation. Environment Variable: MAS_APP_SETTINGS_BASE_LANG Default: EN (English) mas_app_settings_secondary_languages \uf0c1 Optional. Provide a list of additional secondary languages for Manage application. Note: The more languages you add, the longer Manage will take to install and activate. Export the MAS_APP_SETTINGS_SECONDARY_LANGS variable with the language codes as comma-separated values. For a full list of supported languages for Manage application and its corresponding language codes, please refer to Language Support documentation. Environment Variable: MAS_APP_SETTINGS_SECONDARY_LANGS Default: None For example, use the following to enable Manage application with Arabic, Deutsch and Japanese as secondary languages: export MAS_APP_SETTINGS_SECONDARY_LANGS='AR,DE,JA' mas_app_settings_server_bundles_size \uf0c1 Optional. Provides different flavors of server bundle configuration to handle workload for Manage application. For more details about Manage application server bundle configuration, refer to Setting the server bundles for Manage application . Currently supported server bundle sizes are: dev - Deploys Manage with the default server bundle configuration. i.e just 1 bundle pod handling all Manage application workload. small - Deploys Manage with the most common deployment configuration. i.e 4 bundle pods, each one handling workload for each main capabilities: mea , cron , report and ui jms - Can be used for Manage 8.4 and above. Same server bundle configuration as small and includes jms bundle pod. Environment Variable: MAS_APP_SETTINGS_SERVER_BUNDLES_SIZE Default: dev Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS workspace configuration mas_workspace_id: \"{{ lookup('env', 'MAS_WORKSPACE_ID') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" mas_app_ws_spec: bindings: jdbc: \"{{ mas_appws_jdbc_binding | default( 'system' , true) }}\" roles: - ibm.mas_devops.suite_app_config License \uf0c1 EPL-2.0","title":"suite_app_config"},{"location":"roles/suite_app_config/#suite_app_config","text":"This role is used to configure specific components of the application workspace after the application has been installed in the Maximo Application Suite.","title":"suite_app_config"},{"location":"roles/suite_app_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_app_config/#mas_instance_id","text":"Defines the instance id that was used for the MAS installation","title":"mas_instance_id"},{"location":"roles/suite_app_config/#mas_app_id","text":"Defines the kind of application that is intended for installation such as assist , health , iot , manage , monitor , mso , predict , or safety","title":"mas_app_id"},{"location":"roles/suite_app_config/#mas_workspace_id","text":"MAS application workspace to use to configure app components","title":"mas_workspace_id"},{"location":"roles/suite_app_config/#mas_config_dir","text":"Optional. Local directory where generated resource definitions are saved into. It is used by current role to retrieve the id of the analytics project eventually created by cp4d_service role and then configure it into Health & Predict - Utilities resource. Optional, only supported when mas_app_id = hputilities and cpd_ws_project_name is informed. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/suite_app_config/#mas_appws_components","text":"Defines the app components and versions to configure in the application workspace. Takes the form of key=value pairs seperated by a comma i.e. To install health within Manage set base=latest,health=latest Environment Variable: MAS_APPWS_COMPONENTS Default: For Manage the default is: base=latest For Health (standalone) the default is: health=latest","title":"mas_appws_components"},{"location":"roles/suite_app_config/#mas_app_ws_spec","text":"Optional. The application workspace deployment spec used to configure various aspects of the application workspace configuration. Note that use of this will override anything set in mas_appws_components Environment Variable: MAS_APP_WS_SPEC Default: defaults are specified in vars/defaultspecs/{{mas_app_id}}.yml","title":"mas_app_ws_spec"},{"location":"roles/suite_app_config/#role-variables-health-predict-utilities","text":"","title":"Role Variables - Health &amp; Predict Utilities"},{"location":"roles/suite_app_config/#cpd_ws_project_id","text":"Optional. It is the id of the analytics project created in Watson Studio and used to configure hputilities application. Only supported when mas_app_id = hputilities Environment Variable: CPD_WS_PROJECT_ID Default: None","title":"cpd_ws_project_id"},{"location":"roles/suite_app_config/#cpd_ws_project_name","text":"Optional. It specifies the name of the file in mas_config_dir where the id of the analytics project is saved. Only supported when mas_app_id = hputilities and mas_config_dir is informed. Environment Variable: CPD_WS_PROJECT_NAME Default Value: wsl-mas-${mas_instance_id}-hputilities","title":"cpd_ws_project_name"},{"location":"roles/suite_app_config/#role-variables-manage","text":"","title":"Role Variables - Manage"},{"location":"roles/suite_app_config/#mas_app_settings_aio_flag","text":"Optional. Flag indicating if Asset Investment Optimization (AIO) resource must be loaded or not. It can be loaded only when Optimizer application is installed. Only supported when Optimizer application is installed. Environment Variable: MAS_APP_SETTINGS_AIO_FLAG Default: true","title":"mas_app_settings_aio_flag"},{"location":"roles/suite_app_config/#mas_app_settings_db2_schema","text":"Optional. Name of the schema where Manage database lives in. Environment Variable: MAS_APP_SETTINGS_DB2_SCHEMA') | Default: maximo","title":"mas_app_settings_db2_schema"},{"location":"roles/suite_app_config/#mas_app_settings_demodata","text":"Optional. Flag indicating if manage demodata should be loaded or not. Environment Variable: MAS_APP_SETTINGS_DEMODATA Default: false (do not load demodata)","title":"mas_app_settings_demodata"},{"location":"roles/suite_app_config/#mas_app_settings_base_language","text":"Optional. Provide the base language for Manage application. For a full list of supported languages for Manage application and its corresponding language codes, please refer to Language Support documentation. Environment Variable: MAS_APP_SETTINGS_BASE_LANG Default: EN (English)","title":"mas_app_settings_base_language"},{"location":"roles/suite_app_config/#mas_app_settings_secondary_languages","text":"Optional. Provide a list of additional secondary languages for Manage application. Note: The more languages you add, the longer Manage will take to install and activate. Export the MAS_APP_SETTINGS_SECONDARY_LANGS variable with the language codes as comma-separated values. For a full list of supported languages for Manage application and its corresponding language codes, please refer to Language Support documentation. Environment Variable: MAS_APP_SETTINGS_SECONDARY_LANGS Default: None For example, use the following to enable Manage application with Arabic, Deutsch and Japanese as secondary languages: export MAS_APP_SETTINGS_SECONDARY_LANGS='AR,DE,JA'","title":"mas_app_settings_secondary_languages"},{"location":"roles/suite_app_config/#mas_app_settings_server_bundles_size","text":"Optional. Provides different flavors of server bundle configuration to handle workload for Manage application. For more details about Manage application server bundle configuration, refer to Setting the server bundles for Manage application . Currently supported server bundle sizes are: dev - Deploys Manage with the default server bundle configuration. i.e just 1 bundle pod handling all Manage application workload. small - Deploys Manage with the most common deployment configuration. i.e 4 bundle pods, each one handling workload for each main capabilities: mea , cron , report and ui jms - Can be used for Manage 8.4 and above. Same server bundle configuration as small and includes jms bundle pod. Environment Variable: MAS_APP_SETTINGS_SERVER_BUNDLES_SIZE Default: dev","title":"mas_app_settings_server_bundles_size"},{"location":"roles/suite_app_config/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS workspace configuration mas_workspace_id: \"{{ lookup('env', 'MAS_WORKSPACE_ID') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" mas_app_ws_spec: bindings: jdbc: \"{{ mas_appws_jdbc_binding | default( 'system' , true) }}\" roles: - ibm.mas_devops.suite_app_config","title":"Example Playbook"},{"location":"roles/suite_app_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_install/","text":"suite_app_install \uf0c1 This role is used to install a specified application in Maximo Application Suite. Role Variables \uf0c1 mas_app_catalog_source Defines the catalog to be used to install the MAS app. You can set it to ibm-operator-catalog for release install or ibm-mas-{mas_app_id}-operators for development, where {mas_app_id} will be manage for the Manage and Health app installation, for example. artifactory_username Required when using this role for development versions of the MAS application artifactory_apikey Required when using this role for development versions of the MAS application mas_app_channel Defines which channel of the MAS application to subscribe to. Set to 8.x when installing released version mas_instance_id Defines the instance id that was used for the MAS installation mas_icr_cp Defines the entitled registry from the images should be pulled from. Set this to cp.icr.io/cp when installing release version of MAS or wiotp-docker-local.artifactory.swg-devops.com for dev mas_entitlement_username Username for entitled registry. This username will be used to create the image pull secret. Set to cp when installing release or use your w3Id for dev mas_entitlement_key API Key for entitled registry. This password will be used to create the image pull secret. Set to with IBM entitlement key when installing release or use your artifactory apikey for dev. mas_app_id Defines the kind of application that is intended for installation such as assist , health , iot , manage , monitor , mso , predict , or safety mas_app_upgrade_strategy Defines the Upgrade strategy for the MAS Application Operator. Default is set to Automatic mas_app_plan \uf0c1 Optional. Defines what plan will be used in application install. Environment Variable: MAS_APP_PLAN Default: Application-specific, see details below. Application Support: Optimizer v8.2+: full and limited are supported, defaults to full mas_app_spec \uf0c1 Optional. The application deployment spec used to configure different aspects of the application deployment configuration. Environment Variable: None Default: defaults are specified in vars/defaultspecs/{{mas_app_id}}.yml Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: # Choose which catalog source to use for the MAS install, default to the IBM operator catalog mas_app_catalog_source: \"{{ lookup('env', 'MAS_APP_CATALOG_SOURCE') | default('ibm-operator-catalog', true) }}\" # Which MAS channel to subscribe to mas_app_channel: \"{{ lookup('env', 'MAS_APP_CHANNEL') | default('8.x', true) }}\" # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS configuration - IBM container registry configuration mas_icr_cp: \"{{ lookup('env', 'MAS_ICR_CP') | default('cp.icr.io/cp', true) }}\" # MAS configuration - Entitlement mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') | default('cp', true) }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" # Determine MAS Operator Upgrade Strategy Manual | Automatic mas_app_upgrade_strategy: \"{{ lookup('env', 'MAS_APP_UPGRADE_STRATEGY') | default('Manual', true) }}\" # Application Configuration - Spec mas_app_spec: bindings: jdbc: system mongo: system kafka: system settings: messagesight: storage: class: block1000p size: 100Gi deployment: size: medium # Application Configuration - Install Plan mas_app_plan: \"{{ lookup('env', 'MAS_APP_PLAN') | default('full', true) }}\" roles: - ibm.mas_devops.suite_app_install License \uf0c1 EPL-2.0","title":"suite_app_install"},{"location":"roles/suite_app_install/#suite_app_install","text":"This role is used to install a specified application in Maximo Application Suite.","title":"suite_app_install"},{"location":"roles/suite_app_install/#role-variables","text":"mas_app_catalog_source Defines the catalog to be used to install the MAS app. You can set it to ibm-operator-catalog for release install or ibm-mas-{mas_app_id}-operators for development, where {mas_app_id} will be manage for the Manage and Health app installation, for example. artifactory_username Required when using this role for development versions of the MAS application artifactory_apikey Required when using this role for development versions of the MAS application mas_app_channel Defines which channel of the MAS application to subscribe to. Set to 8.x when installing released version mas_instance_id Defines the instance id that was used for the MAS installation mas_icr_cp Defines the entitled registry from the images should be pulled from. Set this to cp.icr.io/cp when installing release version of MAS or wiotp-docker-local.artifactory.swg-devops.com for dev mas_entitlement_username Username for entitled registry. This username will be used to create the image pull secret. Set to cp when installing release or use your w3Id for dev mas_entitlement_key API Key for entitled registry. This password will be used to create the image pull secret. Set to with IBM entitlement key when installing release or use your artifactory apikey for dev. mas_app_id Defines the kind of application that is intended for installation such as assist , health , iot , manage , monitor , mso , predict , or safety mas_app_upgrade_strategy Defines the Upgrade strategy for the MAS Application Operator. Default is set to Automatic","title":"Role Variables"},{"location":"roles/suite_app_install/#mas_app_plan","text":"Optional. Defines what plan will be used in application install. Environment Variable: MAS_APP_PLAN Default: Application-specific, see details below. Application Support: Optimizer v8.2+: full and limited are supported, defaults to full","title":"mas_app_plan"},{"location":"roles/suite_app_install/#mas_app_spec","text":"Optional. The application deployment spec used to configure different aspects of the application deployment configuration. Environment Variable: None Default: defaults are specified in vars/defaultspecs/{{mas_app_id}}.yml","title":"mas_app_spec"},{"location":"roles/suite_app_install/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # Choose which catalog source to use for the MAS install, default to the IBM operator catalog mas_app_catalog_source: \"{{ lookup('env', 'MAS_APP_CATALOG_SOURCE') | default('ibm-operator-catalog', true) }}\" # Which MAS channel to subscribe to mas_app_channel: \"{{ lookup('env', 'MAS_APP_CHANNEL') | default('8.x', true) }}\" # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS configuration - IBM container registry configuration mas_icr_cp: \"{{ lookup('env', 'MAS_ICR_CP') | default('cp.icr.io/cp', true) }}\" # MAS configuration - Entitlement mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') | default('cp', true) }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" # Determine MAS Operator Upgrade Strategy Manual | Automatic mas_app_upgrade_strategy: \"{{ lookup('env', 'MAS_APP_UPGRADE_STRATEGY') | default('Manual', true) }}\" # Application Configuration - Spec mas_app_spec: bindings: jdbc: system mongo: system kafka: system settings: messagesight: storage: class: block1000p size: 100Gi deployment: size: medium # Application Configuration - Install Plan mas_app_plan: \"{{ lookup('env', 'MAS_APP_PLAN') | default('full', true) }}\" roles: - ibm.mas_devops.suite_app_install","title":"Example Playbook"},{"location":"roles/suite_app_install/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_upgrade/","text":"suite_app_upgrade \uf0c1 This role will upgrade the subscription channel for an an installed MAS application after validating: That the application is installed and in a healthy state That the new version of the application can be upgraded to from the existing version That the new version of the application is compatible with the running MAS core platform Role Variables \uf0c1 mas_instance_id \uf0c1 Set the instance ID for the MAS installation where you wish to upgrade the application. Required Environment Variable: MAS_INSTANCE_ID Default Value: None mas_app_channel \uf0c1 Select the subscription channel you wish to upgrade to. Built-in validation will ensure that the upgrade will only proceed if a supportable upgrade path is chosen. Required Environment Variable: MAS_APP_CHANNEL Default Value: None mas_upgrade_dryrun \uf0c1 When set to true will ensure that the role only preforms upgrade validation checks and does not make any changes to the target installation. Optional Environment Variable: MAS_UPGRADE_DRYRUN Default: False Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: iot mas_app_channel: 8.5.x roles: - ibm.mas_devops.suite_app_upgrade","title":"suite_app_upgrade"},{"location":"roles/suite_app_upgrade/#suite_app_upgrade","text":"This role will upgrade the subscription channel for an an installed MAS application after validating: That the application is installed and in a healthy state That the new version of the application can be upgraded to from the existing version That the new version of the application is compatible with the running MAS core platform","title":"suite_app_upgrade"},{"location":"roles/suite_app_upgrade/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_app_upgrade/#mas_instance_id","text":"Set the instance ID for the MAS installation where you wish to upgrade the application. Required Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_app_upgrade/#mas_app_channel","text":"Select the subscription channel you wish to upgrade to. Built-in validation will ensure that the upgrade will only proceed if a supportable upgrade path is chosen. Required Environment Variable: MAS_APP_CHANNEL Default Value: None","title":"mas_app_channel"},{"location":"roles/suite_app_upgrade/#mas_upgrade_dryrun","text":"When set to true will ensure that the role only preforms upgrade validation checks and does not make any changes to the target installation. Optional Environment Variable: MAS_UPGRADE_DRYRUN Default: False","title":"mas_upgrade_dryrun"},{"location":"roles/suite_app_upgrade/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: iot mas_app_channel: 8.5.x roles: - ibm.mas_devops.suite_app_upgrade","title":"Example Playbook"},{"location":"roles/suite_config/","text":"suite_config \uf0c1 TODO: Summarize role Role Variables \uf0c1 TODO: Finish documentation Example Playbook \uf0c1 TODO: Add example License \uf0c1 EPL-2.0","title":"suite_config"},{"location":"roles/suite_config/#suite_config","text":"TODO: Summarize role","title":"suite_config"},{"location":"roles/suite_config/#role-variables","text":"TODO: Finish documentation","title":"Role Variables"},{"location":"roles/suite_config/#example-playbook","text":"TODO: Add example","title":"Example Playbook"},{"location":"roles/suite_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_db2_setup_for_manage/","text":"suite_db2_setup_for_manage \uf0c1 This role shouldn't need to exist, it should be part of the Manage operator, but is not so we have to do it as a seperate step in the install flow for now. The role will perform some initial setup on the Db2 instance that is needed to prepare it for use with the Manage application and supports both CP4D version 3.5 and 4.0. The role will copy a bash script (setupdb.sh) into the Db2 pod and execute it inside the container, this script will perform a number of configuration changes to the database as well as configuring the tablespaces for Maximo Manage because the operator is not yet able to do this itself. Role Variables \uf0c1 db2_instance_name \uf0c1 The name of the db2 instance to execute the setup in. Required Environment Variable: DB2_INSTANCE_NAME Default Value: None db2_namespace \uf0c1 The namespace where the Db2 instance is running. Optional Environment Variable: DB2_NAMESPACE Default Value: db2u db2_username \uf0c1 The username that will be used to connect to the database specified by db2_dbname . Optional Environment Variable: None Default Value: db2inst1 db2_dbname \uf0c1 The name of the database in the instance to connect to when executing the setup script. Optional Environment Variable: None Default Value: BLUDB db2_schema \uf0c1 The name of the Manage schema where the hack should be targeted in. Optional Environment Variable: None Default Value: maximo db2_tablespace_data_size \uf0c1 The size of the tablespace data in the database. Optional Environment Variable: DB2_TABLESPACE_DATA_SIZE Default Value: 5000 M db2_tablespace_index_size \uf0c1 The size of the tablespace indexes in the database. Optional Environment Variable: DB2_TABLESPACE_INDEX_SIZE Default Value: 5000 M Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: db2_instancename: mydb2 roles: - ibm.mas_devops.suite_db2_setup_for_manage License \uf0c1 EPL-2.0","title":"suite_db2_setup_for_manage"},{"location":"roles/suite_db2_setup_for_manage/#suite_db2_setup_for_manage","text":"This role shouldn't need to exist, it should be part of the Manage operator, but is not so we have to do it as a seperate step in the install flow for now. The role will perform some initial setup on the Db2 instance that is needed to prepare it for use with the Manage application and supports both CP4D version 3.5 and 4.0. The role will copy a bash script (setupdb.sh) into the Db2 pod and execute it inside the container, this script will perform a number of configuration changes to the database as well as configuring the tablespaces for Maximo Manage because the operator is not yet able to do this itself.","title":"suite_db2_setup_for_manage"},{"location":"roles/suite_db2_setup_for_manage/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_db2_setup_for_manage/#db2_instance_name","text":"The name of the db2 instance to execute the setup in. Required Environment Variable: DB2_INSTANCE_NAME Default Value: None","title":"db2_instance_name"},{"location":"roles/suite_db2_setup_for_manage/#db2_namespace","text":"The namespace where the Db2 instance is running. Optional Environment Variable: DB2_NAMESPACE Default Value: db2u","title":"db2_namespace"},{"location":"roles/suite_db2_setup_for_manage/#db2_username","text":"The username that will be used to connect to the database specified by db2_dbname . Optional Environment Variable: None Default Value: db2inst1","title":"db2_username"},{"location":"roles/suite_db2_setup_for_manage/#db2_dbname","text":"The name of the database in the instance to connect to when executing the setup script. Optional Environment Variable: None Default Value: BLUDB","title":"db2_dbname"},{"location":"roles/suite_db2_setup_for_manage/#db2_schema","text":"The name of the Manage schema where the hack should be targeted in. Optional Environment Variable: None Default Value: maximo","title":"db2_schema"},{"location":"roles/suite_db2_setup_for_manage/#db2_tablespace_data_size","text":"The size of the tablespace data in the database. Optional Environment Variable: DB2_TABLESPACE_DATA_SIZE Default Value: 5000 M","title":"db2_tablespace_data_size"},{"location":"roles/suite_db2_setup_for_manage/#db2_tablespace_index_size","text":"The size of the tablespace indexes in the database. Optional Environment Variable: DB2_TABLESPACE_INDEX_SIZE Default Value: 5000 M","title":"db2_tablespace_index_size"},{"location":"roles/suite_db2_setup_for_manage/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: db2_instancename: mydb2 roles: - ibm.mas_devops.suite_db2_setup_for_manage","title":"Example Playbook"},{"location":"roles/suite_db2_setup_for_manage/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_dns/","text":"suite_dns \uf0c1 This role will manage MAS and DNS provider integration. IBM Cloud Internet Services is the only supported DNS provider currently. It will also create a secure route (https://cp4d. ) to the CP4D web client using the custom domain used in this role. DNS management \uf0c1 There are two different ways this role controls DNS entries in the provider: Top Level DNS entries \uf0c1 This mode will create the entries directly under your DNS zone. Use this when the DNS zone matches the MAS domain exactly. If your MAS installation will be using the domain mymas.mycompany.com and you have a DNS zone for mymas.mycompany.com then you will be creating top-level DNS entries for MAS, e.g. admin , home , & api . Subdomain DNS entries \uf0c1 This mode will create DNS entries in the zone under a subdomain. Use this when your DNS zone will be used for more than just one MAS instance. If your MAS installation will be using the domain mymas.mycompany.com and you have a DNS zone for mycompany.com then you will be creating subdomain DNS entries for MAS, e.g. admin.mymas , home.mymas , & api.mymas . CIS and Cloudflare integrations support both mode of DNS management. A single optional variable is required to enable subdomain DNS management, in the examples above you would set these to mymas : cis_subdomain cloudflare_subdomain Let's Encrypt Integration \uf0c1 Both the CIS and Cloudflare options also enable integration with Let's Encrypt for automatic certificate management via IBM Certificate Manager. Each will create a new ClusterIssuer which can be used when installing Maximo Application Suite: Cloudflare Let's Encrypt ClusterIssuer: {{ mas_instance_id }}-cloudflare-le-prod IBM Cloud Internet Services Let's Encrypt ClusterIssuer: {{ mas_instance_id }}-cis-le-prod If you want to use Let's Encrypt certificates in your MAS installation you will need to configure the mas_cluster_issuer variable in the suite_install role, setting it to the name of the ClusterIssuer as documented above. Note There are issues with how cert-manager works with LetsEncrypt staging servers. It creates a secret for the certificate that doesn't contain the LetsEncrypt CA, but the staging service does not use a well known cert so we end up with MAS unable to trust the certificates generated by LetsEncrypt staging. At present there is no workaround for this, so do not use the LetsEncrypt staging certificate issuer. Role Variables - General \uf0c1 dns_provider \uf0c1 Required Environment Variable: DNS_PROVIDER Default: None mas_instance_id: \uf0c1 Required Environment Variable: MAS_INSTANCE_ID Default: None mas_domain \uf0c1 Required Environment Variable: MAS_DOMAIN Default: None ocp_ingress \uf0c1 Optional Environment Variable: OCP_INGRESS Default: None Role Variables - Cloudflare DNS Integration \uf0c1 cloudflare_email \uf0c1 Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_EMAIL Default: None cloudflare_apitoken \uf0c1 To generate an API token follow the Cloudflare documentation . Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_APITOKEN Default: None cloudflare_zone \uf0c1 This is your domain name that is managed by Cloudflare (e.g. mydomain.com ). Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_ZONE Default: None cloudflare_subdomain \uf0c1 Set this to the name of the subdomain under your Cloudflare domain where you would like to manage the MAS DNS entires if you don't want MAS to use the top-level domain itself. In other words cloudflare_subdomain.cloudflare_zone should equal mas_domain . Optional Environment Variable: CLOUDFLARE_SUBDOMAIN Default: None Role Variables - IBM Cloud Internet Services DNS Integration \uf0c1 cis_email \uf0c1 Required if dns_provider is set to cis Environment Variable: CIS_EMAIL Default: None cis_apikey \uf0c1 Required if dns_provider is set to cis Environment Variable: CIS_APIKEY Default: None cis_crn \uf0c1 Required if dns_provider is set to cis Environment Variable: CIS_CRN Default: None cis_subdomain \uf0c1 Optional Environment Variable: CIS_SUBDOMAIN Default: None Example Playbook \uf0c1 --- - hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 mas_domain: mydomain.com cis_crn: xxx cis_apikey: xxx cis_email: xxx roles: - ibm.mas_devops.suite_dns License \uf0c1 EPL-2.0","title":"suite_dns"},{"location":"roles/suite_dns/#suite_dns","text":"This role will manage MAS and DNS provider integration. IBM Cloud Internet Services is the only supported DNS provider currently. It will also create a secure route (https://cp4d. ) to the CP4D web client using the custom domain used in this role.","title":"suite_dns"},{"location":"roles/suite_dns/#dns-management","text":"There are two different ways this role controls DNS entries in the provider:","title":"DNS management"},{"location":"roles/suite_dns/#top-level-dns-entries","text":"This mode will create the entries directly under your DNS zone. Use this when the DNS zone matches the MAS domain exactly. If your MAS installation will be using the domain mymas.mycompany.com and you have a DNS zone for mymas.mycompany.com then you will be creating top-level DNS entries for MAS, e.g. admin , home , & api .","title":"Top Level DNS entries"},{"location":"roles/suite_dns/#subdomain-dns-entries","text":"This mode will create DNS entries in the zone under a subdomain. Use this when your DNS zone will be used for more than just one MAS instance. If your MAS installation will be using the domain mymas.mycompany.com and you have a DNS zone for mycompany.com then you will be creating subdomain DNS entries for MAS, e.g. admin.mymas , home.mymas , & api.mymas . CIS and Cloudflare integrations support both mode of DNS management. A single optional variable is required to enable subdomain DNS management, in the examples above you would set these to mymas : cis_subdomain cloudflare_subdomain","title":"Subdomain DNS entries"},{"location":"roles/suite_dns/#lets-encrypt-integration","text":"Both the CIS and Cloudflare options also enable integration with Let's Encrypt for automatic certificate management via IBM Certificate Manager. Each will create a new ClusterIssuer which can be used when installing Maximo Application Suite: Cloudflare Let's Encrypt ClusterIssuer: {{ mas_instance_id }}-cloudflare-le-prod IBM Cloud Internet Services Let's Encrypt ClusterIssuer: {{ mas_instance_id }}-cis-le-prod If you want to use Let's Encrypt certificates in your MAS installation you will need to configure the mas_cluster_issuer variable in the suite_install role, setting it to the name of the ClusterIssuer as documented above. Note There are issues with how cert-manager works with LetsEncrypt staging servers. It creates a secret for the certificate that doesn't contain the LetsEncrypt CA, but the staging service does not use a well known cert so we end up with MAS unable to trust the certificates generated by LetsEncrypt staging. At present there is no workaround for this, so do not use the LetsEncrypt staging certificate issuer.","title":"Let's Encrypt Integration"},{"location":"roles/suite_dns/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/suite_dns/#dns_provider","text":"Required Environment Variable: DNS_PROVIDER Default: None","title":"dns_provider"},{"location":"roles/suite_dns/#mas_instance_id","text":"Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id:"},{"location":"roles/suite_dns/#mas_domain","text":"Required Environment Variable: MAS_DOMAIN Default: None","title":"mas_domain"},{"location":"roles/suite_dns/#ocp_ingress","text":"Optional Environment Variable: OCP_INGRESS Default: None","title":"ocp_ingress"},{"location":"roles/suite_dns/#role-variables-cloudflare-dns-integration","text":"","title":"Role Variables - Cloudflare DNS Integration"},{"location":"roles/suite_dns/#cloudflare_email","text":"Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_EMAIL Default: None","title":"cloudflare_email"},{"location":"roles/suite_dns/#cloudflare_apitoken","text":"To generate an API token follow the Cloudflare documentation . Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_APITOKEN Default: None","title":"cloudflare_apitoken"},{"location":"roles/suite_dns/#cloudflare_zone","text":"This is your domain name that is managed by Cloudflare (e.g. mydomain.com ). Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_ZONE Default: None","title":"cloudflare_zone"},{"location":"roles/suite_dns/#cloudflare_subdomain","text":"Set this to the name of the subdomain under your Cloudflare domain where you would like to manage the MAS DNS entires if you don't want MAS to use the top-level domain itself. In other words cloudflare_subdomain.cloudflare_zone should equal mas_domain . Optional Environment Variable: CLOUDFLARE_SUBDOMAIN Default: None","title":"cloudflare_subdomain"},{"location":"roles/suite_dns/#role-variables-ibm-cloud-internet-services-dns-integration","text":"","title":"Role Variables - IBM Cloud Internet Services DNS Integration"},{"location":"roles/suite_dns/#cis_email","text":"Required if dns_provider is set to cis Environment Variable: CIS_EMAIL Default: None","title":"cis_email"},{"location":"roles/suite_dns/#cis_apikey","text":"Required if dns_provider is set to cis Environment Variable: CIS_APIKEY Default: None","title":"cis_apikey"},{"location":"roles/suite_dns/#cis_crn","text":"Required if dns_provider is set to cis Environment Variable: CIS_CRN Default: None","title":"cis_crn"},{"location":"roles/suite_dns/#cis_subdomain","text":"Optional Environment Variable: CIS_SUBDOMAIN Default: None","title":"cis_subdomain"},{"location":"roles/suite_dns/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 mas_domain: mydomain.com cis_crn: xxx cis_apikey: xxx cis_email: xxx roles: - ibm.mas_devops.suite_dns","title":"Example Playbook"},{"location":"roles/suite_dns/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_install/","text":"suite_install \uf0c1 This role install Maximo Application Suite. It internally resolve the namespace based on the mas_instance_id as mas-{mas_instance_id}-core . By default this role install MAS Operator using Manual Upgrade Strategy. Set MAS_UPGRADE_STRATEGY environment variable to Automatic to override it. In the Manual upgrade mode, IBM Common Services operators requested by MAS will inherit the upgrade strategy from MAS and their pending install plans approved. Role Variables \uf0c1 mas_catalog_source Defines the catalog to be used to install MAS. You can set it to ibm-operator-catalog for both release as well as for development install artifactory_username Required when using this role for development versions of MAS artifactory_apikey Required when using this role for development versions of MAS mas_channel Defines which channel of MAS to subscribe to mas_domain Opitional fact, if not provided the role will use the default cluster subdomain mas_instance_id Defines the instance id to be used for MAS installation mas_icr_cp Defines the entitled registry from the images should be pulled from. Set this to cp.icr.io/cp when installing release version of MAS or wiotp-docker-local.artifactory.swg-devops.com for dev mas_icr_cpopen Defines the registry for non entitled images, such as operators. Set this to icr.io/cpopen when installing release version of MAS or wiotp-docker-local.artifactory.swg-devops.com for dev mas_entitlement_username Username for entitled registry. This username will be used to create the image pull secret. Set to cp when installing release or use your w3Id for dev. mas_entitlement_key API Key for entitled registry. This password will be used to create the image pull secret. Set to with IBM entitlement key when installing release or use your artifactory apikey for dev. mas_config_dir Directory containing configuration files ( *.yaml and *.yml ) to be applied to the MAS installation. Intended for creating the various MAS custom resources to configure the suite post-install, but can be used to apply any kubernetes resource you need to customize any aspect of your cluster. certManager.namespace The namespace containing the cert-manager to be used by MAS mas_upgrade_strategy The Upgrade strategy for MAS Operator. Default is set to Automatic mas_annotations Optional variable having all the annotations that need to be added to the Suite CR Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_config_dir: \"/home/david/masconfig\" mas_entitlement_key: \"{{ lookup('env', 'IBM_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify Tekton Task \uf0c1 Start a run of the mas-devops-suite-install Task as below, you must have already prepared the namespace: cat <<EOF | oc create -f - apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: generateName: mas-devops-suite-install- spec: taskRef: kind: Task name: mas-devops-suite-install params: - name: mas_instance_id value: inst1 - name: mas_channel value: 8.7.x - name: mas_entitlement_key value: xxx resources: {} serviceAccountName: pipeline timeout: 48h0m0s EOF License \uf0c1 EPL-2.0","title":"suite_install"},{"location":"roles/suite_install/#suite_install","text":"This role install Maximo Application Suite. It internally resolve the namespace based on the mas_instance_id as mas-{mas_instance_id}-core . By default this role install MAS Operator using Manual Upgrade Strategy. Set MAS_UPGRADE_STRATEGY environment variable to Automatic to override it. In the Manual upgrade mode, IBM Common Services operators requested by MAS will inherit the upgrade strategy from MAS and their pending install plans approved.","title":"suite_install"},{"location":"roles/suite_install/#role-variables","text":"mas_catalog_source Defines the catalog to be used to install MAS. You can set it to ibm-operator-catalog for both release as well as for development install artifactory_username Required when using this role for development versions of MAS artifactory_apikey Required when using this role for development versions of MAS mas_channel Defines which channel of MAS to subscribe to mas_domain Opitional fact, if not provided the role will use the default cluster subdomain mas_instance_id Defines the instance id to be used for MAS installation mas_icr_cp Defines the entitled registry from the images should be pulled from. Set this to cp.icr.io/cp when installing release version of MAS or wiotp-docker-local.artifactory.swg-devops.com for dev mas_icr_cpopen Defines the registry for non entitled images, such as operators. Set this to icr.io/cpopen when installing release version of MAS or wiotp-docker-local.artifactory.swg-devops.com for dev mas_entitlement_username Username for entitled registry. This username will be used to create the image pull secret. Set to cp when installing release or use your w3Id for dev. mas_entitlement_key API Key for entitled registry. This password will be used to create the image pull secret. Set to with IBM entitlement key when installing release or use your artifactory apikey for dev. mas_config_dir Directory containing configuration files ( *.yaml and *.yml ) to be applied to the MAS installation. Intended for creating the various MAS custom resources to configure the suite post-install, but can be used to apply any kubernetes resource you need to customize any aspect of your cluster. certManager.namespace The namespace containing the cert-manager to be used by MAS mas_upgrade_strategy The Upgrade strategy for MAS Operator. Default is set to Automatic mas_annotations Optional variable having all the annotations that need to be added to the Suite CR","title":"Role Variables"},{"location":"roles/suite_install/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_config_dir: \"/home/david/masconfig\" mas_entitlement_key: \"{{ lookup('env', 'IBM_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify","title":"Example Playbook"},{"location":"roles/suite_install/#tekton-task","text":"Start a run of the mas-devops-suite-install Task as below, you must have already prepared the namespace: cat <<EOF | oc create -f - apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: generateName: mas-devops-suite-install- spec: taskRef: kind: Task name: mas-devops-suite-install params: - name: mas_instance_id value: inst1 - name: mas_channel value: 8.7.x - name: mas_entitlement_key value: xxx resources: {} serviceAccountName: pipeline timeout: 48h0m0s EOF","title":"Tekton Task"},{"location":"roles/suite_install/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_attachments_config/","text":"suite_manage_attachments_config \uf0c1 This role extends support for Configuring IBM Cloud Object Storage or PVC File Storages for Manage application attachments. The default for Manage attachments configuration is to use your cluster's default file storage system as persistent storage. Although, you can optionally define IBM Cloud Object Storage. You can run cos role to provision an IBM Cloud Object Storage or you can provide existing IBM Cloud Object Storage information to use it as storage for Manage application attachments Role Variables \uf0c1 mas_manage_attachments_provider \uf0c1 Required. Defines the storage provider type to be used to store Manage application's attachments. Environment Variable: MAS_MANAGE_ATTACHMENTS_PROVIDER Default Value: filestorage . Optionally set this variable to cos if you're planning to use IBM Cloud Object Storage instead of File Storage persistent volumes. cos_instance_name \uf0c1 Required. Only used if storage provider is cos . IBM Cloud Object Storage instance name to be used to store Manage application attachments Environment Variable: COS_INSTANCE_NAME Default Value: None. If you do not have an existing IBM Cloud Object Storage instance, you can use cos role to provision one. ibmcloud_resourcegroup \uf0c1 Optional. Only used if storage provider is cos . Provide the name of the resource group that hosts your IBM Cloud Object Storage instance. If you do not provide it, the role will try to find the IBM Cloud Object Storage instance in Default resource group. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default ibmcloud_apikey \uf0c1 Required. Only used if storage provider is cos . Provide your IBM Cloud API Key. Environment Variable: IBMCLOUD_APIKEY Default Value: None mas_instance_id \uf0c1 Required. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_workspace_id \uf0c1 Required. The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_WORKSPACE_ID Default Value: None db2_instance_name \uf0c1 Required. The DB2 Warehouse instance name that stores your Manage application tables and data. This will be used to lookup for Manage application database and update it with the IBM Object Storage configuration. Environment Variable: DB2_INSTANCE_NAME # e.g. db2u-iot or db2wh-1658148844550964 Default Value: None db2_namespace \uf0c1 Optional. The namespace in your cluster that hosts the DB2 Warehouse instance name. This will be used to lookup for Manage application database and update it with the IBM Object Storage configuration. If you do not provide it, the role will try to find the Db2 Warehouse in db2u namespace. Environment Variable: DB2_NAMESPACE # e.g. db2u Default Value: db2u Example Playbook \uf0c1 The following sample can be used to configure COS for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage cos_instance_name: cos-masinst1 ibmcloud_apikey: xxxx mas_manage_attachments_provider: cos roles: - ibm.mas_devops.suite_manage_attachments_config The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage cos_type: ibm cos_instance_name: cos-masinst1 ibmcloud_apikey: xxxx mas_manage_attachments_provider: cos roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_attachments_config The following sample playbook can be used to deploy Manage with default persistent storage for Manage attachments (PVC mount path /DOCLINKS ), and configure Manage system properties with the corresponding attachments settings. - hosts: localhost any_errors_fatal: true vars: mas_app_id: manage mas_app_channel: 8.4.x mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage mas_app_settings_persistent_volumes_flag: true mas_app_settings_attachments_mount_path: /DOCLINKS mas_manage_attachments_provider: filestorage roles: - ibm.mas_devops.db2 - ibm.mas_devops.suite_db2_setup_for_manage - ibm.mas_devops.suite_config - ibm.mas_devops.suite_app_install - ibm.mas_devops.suite_app_config - ibm.mas_devops.suite_manage_attachments_config License \uf0c1 EPL-2.0","title":"suite_manage_attachments_config"},{"location":"roles/suite_manage_attachments_config/#suite_manage_attachments_config","text":"This role extends support for Configuring IBM Cloud Object Storage or PVC File Storages for Manage application attachments. The default for Manage attachments configuration is to use your cluster's default file storage system as persistent storage. Although, you can optionally define IBM Cloud Object Storage. You can run cos role to provision an IBM Cloud Object Storage or you can provide existing IBM Cloud Object Storage information to use it as storage for Manage application attachments","title":"suite_manage_attachments_config"},{"location":"roles/suite_manage_attachments_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_attachments_config/#mas_manage_attachments_provider","text":"Required. Defines the storage provider type to be used to store Manage application's attachments. Environment Variable: MAS_MANAGE_ATTACHMENTS_PROVIDER Default Value: filestorage . Optionally set this variable to cos if you're planning to use IBM Cloud Object Storage instead of File Storage persistent volumes.","title":"mas_manage_attachments_provider"},{"location":"roles/suite_manage_attachments_config/#cos_instance_name","text":"Required. Only used if storage provider is cos . IBM Cloud Object Storage instance name to be used to store Manage application attachments Environment Variable: COS_INSTANCE_NAME Default Value: None. If you do not have an existing IBM Cloud Object Storage instance, you can use cos role to provision one.","title":"cos_instance_name"},{"location":"roles/suite_manage_attachments_config/#ibmcloud_resourcegroup","text":"Optional. Only used if storage provider is cos . Provide the name of the resource group that hosts your IBM Cloud Object Storage instance. If you do not provide it, the role will try to find the IBM Cloud Object Storage instance in Default resource group. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default","title":"ibmcloud_resourcegroup"},{"location":"roles/suite_manage_attachments_config/#ibmcloud_apikey","text":"Required. Only used if storage provider is cos . Provide your IBM Cloud API Key. Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/suite_manage_attachments_config/#mas_instance_id","text":"Required. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_manage_attachments_config/#mas_workspace_id","text":"Required. The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_WORKSPACE_ID Default Value: None","title":"mas_workspace_id"},{"location":"roles/suite_manage_attachments_config/#db2_instance_name","text":"Required. The DB2 Warehouse instance name that stores your Manage application tables and data. This will be used to lookup for Manage application database and update it with the IBM Object Storage configuration. Environment Variable: DB2_INSTANCE_NAME # e.g. db2u-iot or db2wh-1658148844550964 Default Value: None","title":"db2_instance_name"},{"location":"roles/suite_manage_attachments_config/#db2_namespace","text":"Optional. The namespace in your cluster that hosts the DB2 Warehouse instance name. This will be used to lookup for Manage application database and update it with the IBM Object Storage configuration. If you do not provide it, the role will try to find the Db2 Warehouse in db2u namespace. Environment Variable: DB2_NAMESPACE # e.g. db2u Default Value: db2u","title":"db2_namespace"},{"location":"roles/suite_manage_attachments_config/#example-playbook","text":"The following sample can be used to configure COS for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage cos_instance_name: cos-masinst1 ibmcloud_apikey: xxxx mas_manage_attachments_provider: cos roles: - ibm.mas_devops.suite_manage_attachments_config The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage cos_type: ibm cos_instance_name: cos-masinst1 ibmcloud_apikey: xxxx mas_manage_attachments_provider: cos roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_attachments_config The following sample playbook can be used to deploy Manage with default persistent storage for Manage attachments (PVC mount path /DOCLINKS ), and configure Manage system properties with the corresponding attachments settings. - hosts: localhost any_errors_fatal: true vars: mas_app_id: manage mas_app_channel: 8.4.x mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage mas_app_settings_persistent_volumes_flag: true mas_app_settings_attachments_mount_path: /DOCLINKS mas_manage_attachments_provider: filestorage roles: - ibm.mas_devops.db2 - ibm.mas_devops.suite_db2_setup_for_manage - ibm.mas_devops.suite_config - ibm.mas_devops.suite_app_install - ibm.mas_devops.suite_app_config - ibm.mas_devops.suite_manage_attachments_config","title":"Example Playbook"},{"location":"roles/suite_manage_attachments_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_bim_config/","text":"suite_manage_bim_config \uf0c1 This role extends support for configuring existing PVC mounted path for BIM (Building Information Models) in Manage application. In order for this task to run successfully your Manage application must have been configured with a proper persistent volume and mounted path. You can run suite_app_config with mas_app_settings_persistent_volumes_flag: true while installing mas_app_id: manage to have a default persistent storage configured as part of Manage deployment that can be used in this role to setup BIM. For more details on how to configure persistent storage for Manage refer to Configuring persistent volume claims . Role Variables \uf0c1 mas_app_settings_bim_mount_path \uf0c1 Required. Defines the persistent volume mount path to be used while configuring Manage BIM folders. If you used suite_app_config role to configure the persistent volumes while deploying Manage application, the default BIM persistent volume mount path will be the same. Environment Variable: MAS_APP_SETTINGS_BIM_MOUNT_PATH Default Value: /bim . mas_instance_id \uf0c1 Required. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None db2_instance_name \uf0c1 Required. The DB2 Warehouse instance name that stores your Manage application tables and data. This will be used to lookup for Manage application database and update it with the BIM system properties. Environment Variable: DB2_INSTANCE_NAME # e.g. db2u-manage Default Value: None db2_namespace \uf0c1 Optional. The namespace in your cluster that hosts the DB2 Warehouse instance name. This will be used to lookup for Manage application database and update it with the with the BIM system properties. If you do not provide it, the role will try to find the Db2 Warehouse in db2u namespace. Environment Variable: DB2_NAMESPACE # e.g. db2u Default Value: db2u Example Playbook \uf0c1 The following sample can be used to configure BIM for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 db2_instance_name: db2w-manage mas_app_settings_bim_mount_path: /bim roles: - ibm.mas_devops.suite_manage_bim_config The following sample playbook can be used to deploy Manage with default persistent storage for BIM (PVC mount path /bim ), and configure Manage system properties with the corresponding BIM settings. - hosts: localhost any_errors_fatal: true vars: mas_app_id: manage mas_app_channel: 8.4.x mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage mas_app_settings_persistent_volumes_flag: true mas_app_settings_bim_mount_path: /bim roles: - ibm.mas_devops.db2 - ibm.mas_devops.suite_db2_setup_for_manage - ibm.mas_devops.suite_config - ibm.mas_devops.suite_app_install - ibm.mas_devops.suite_app_config - ibm.mas_devops.suite_manage_bim_config","title":"Suite manage bim config"},{"location":"roles/suite_manage_bim_config/#suite_manage_bim_config","text":"This role extends support for configuring existing PVC mounted path for BIM (Building Information Models) in Manage application. In order for this task to run successfully your Manage application must have been configured with a proper persistent volume and mounted path. You can run suite_app_config with mas_app_settings_persistent_volumes_flag: true while installing mas_app_id: manage to have a default persistent storage configured as part of Manage deployment that can be used in this role to setup BIM. For more details on how to configure persistent storage for Manage refer to Configuring persistent volume claims .","title":"suite_manage_bim_config"},{"location":"roles/suite_manage_bim_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_bim_config/#mas_app_settings_bim_mount_path","text":"Required. Defines the persistent volume mount path to be used while configuring Manage BIM folders. If you used suite_app_config role to configure the persistent volumes while deploying Manage application, the default BIM persistent volume mount path will be the same. Environment Variable: MAS_APP_SETTINGS_BIM_MOUNT_PATH Default Value: /bim .","title":"mas_app_settings_bim_mount_path"},{"location":"roles/suite_manage_bim_config/#mas_instance_id","text":"Required. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_manage_bim_config/#db2_instance_name","text":"Required. The DB2 Warehouse instance name that stores your Manage application tables and data. This will be used to lookup for Manage application database and update it with the BIM system properties. Environment Variable: DB2_INSTANCE_NAME # e.g. db2u-manage Default Value: None","title":"db2_instance_name"},{"location":"roles/suite_manage_bim_config/#db2_namespace","text":"Optional. The namespace in your cluster that hosts the DB2 Warehouse instance name. This will be used to lookup for Manage application database and update it with the with the BIM system properties. If you do not provide it, the role will try to find the Db2 Warehouse in db2u namespace. Environment Variable: DB2_NAMESPACE # e.g. db2u Default Value: db2u","title":"db2_namespace"},{"location":"roles/suite_manage_bim_config/#example-playbook","text":"The following sample can be used to configure BIM for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 db2_instance_name: db2w-manage mas_app_settings_bim_mount_path: /bim roles: - ibm.mas_devops.suite_manage_bim_config The following sample playbook can be used to deploy Manage with default persistent storage for BIM (PVC mount path /bim ), and configure Manage system properties with the corresponding BIM settings. - hosts: localhost any_errors_fatal: true vars: mas_app_id: manage mas_app_channel: 8.4.x mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage mas_app_settings_persistent_volumes_flag: true mas_app_settings_bim_mount_path: /bim roles: - ibm.mas_devops.db2 - ibm.mas_devops.suite_db2_setup_for_manage - ibm.mas_devops.suite_config - ibm.mas_devops.suite_app_install - ibm.mas_devops.suite_app_config - ibm.mas_devops.suite_manage_bim_config","title":"Example Playbook"},{"location":"roles/suite_mustgather/","text":"suite_mustgather \uf0c1 Run IBM AI Applications' Must Gather tool against a MAS instance Role Variables \uf0c1 mas_instance_id \uf0c1 Required. MAS instance ID to run against. Environment Variable: MAS_INSTANCE_ID Default Value: None base_output_dir \uf0c1 Required. Location on the local disk to save the must-gather file. Environment Variable: BASE_OUTPUT_DIR Default Value: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 base_output_dir: ~/mas/mustgather roles: - ibm.mas_devops.suite_mustgather License \uf0c1 EPL-2.0","title":"suite_mustgather"},{"location":"roles/suite_mustgather/#suite_mustgather","text":"Run IBM AI Applications' Must Gather tool against a MAS instance","title":"suite_mustgather"},{"location":"roles/suite_mustgather/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_mustgather/#mas_instance_id","text":"Required. MAS instance ID to run against. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_mustgather/#base_output_dir","text":"Required. Location on the local disk to save the must-gather file. Environment Variable: BASE_OUTPUT_DIR Default Value: None","title":"base_output_dir"},{"location":"roles/suite_mustgather/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 base_output_dir: ~/mas/mustgather roles: - ibm.mas_devops.suite_mustgather","title":"Example Playbook"},{"location":"roles/suite_mustgather/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_mustgather_download/","text":"suite_mustgather_download \uf0c1 Downloads the result from IBM AI Applications' Must Gather tool that is run against a MAS instance. This role is to be used when the suite_mustgather role is run from within a pipeline, and the user needs to download the result to a local machine. If you run the suite_mustgather role locally then there is no need to run this role as the output is already available to you locally. Role Variables \uf0c1 local_output_dir \uf0c1 Required. Location on the local disk to save the must-gather file. Environment Variable: LOCAL_OUTPUT_DIR Default Value: None mustgather_namespace \uf0c1 Required. The openshift namespace that the pipeline, that ran the suite_mustgather task, was executed in. Environment Variable: MUSTGATHER_NAMESPACE Default Value: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: mustgather_namespace: mas-samples-pipelines local_output_dir: ~/mas/mustgather roles: - ibm.mas_devops.suite_mustgather_download License \uf0c1 EPL-2.0","title":"suite_mustgather_download"},{"location":"roles/suite_mustgather_download/#suite_mustgather_download","text":"Downloads the result from IBM AI Applications' Must Gather tool that is run against a MAS instance. This role is to be used when the suite_mustgather role is run from within a pipeline, and the user needs to download the result to a local machine. If you run the suite_mustgather role locally then there is no need to run this role as the output is already available to you locally.","title":"suite_mustgather_download"},{"location":"roles/suite_mustgather_download/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_mustgather_download/#local_output_dir","text":"Required. Location on the local disk to save the must-gather file. Environment Variable: LOCAL_OUTPUT_DIR Default Value: None","title":"local_output_dir"},{"location":"roles/suite_mustgather_download/#mustgather_namespace","text":"Required. The openshift namespace that the pipeline, that ran the suite_mustgather task, was executed in. Environment Variable: MUSTGATHER_NAMESPACE Default Value: None","title":"mustgather_namespace"},{"location":"roles/suite_mustgather_download/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mustgather_namespace: mas-samples-pipelines local_output_dir: ~/mas/mustgather roles: - ibm.mas_devops.suite_mustgather_download","title":"Example Playbook"},{"location":"roles/suite_mustgather_download/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_upgrade/","text":"suite_upgrade \uf0c1 This role validates if a given MAS installation is ready for the core platform to be upgraded to a specific subscription channel, and (as long as dry run mode is not enabled) will execute the upgrade. It will validate that the current subscription channel is able to be upgraded to the target channel. It will validate that all installed applications have already been upgraded to versions compatible with the new version of the Core Platform. It will upgrade the MAS core platform to the desired channel (as long as dry run is not enabled). It will validate that the core platform has been successfully reconciled at the upgraded version. It will not validate that all core services successfully deploy after the reconcile (but we will be working on this limitation). Role Variables \uf0c1 mas_instance_id \uf0c1 The ID of the MAS instance to upgrade. Required Environment Variable: MAS_INSTANCE_ID Default: None mas_channel \uf0c1 The name of the MAS subscription channel that you want to upgrade to, if not provided the correct version to upgrade to will be automatically selected based on the current version of MAS installed. Optional Environment Variable: MAS_CHANNEL Default: None mas_upgrade_dryrun \uf0c1 When set to true will ensure that the role only preforms upgrade validation checks and does not make any changes to the target installation. Optional Environment Variable: MAS_UPGRADE_DRYRUN Default: False Example Playbook \uf0c1 Automatic Target Selection \uf0c1 Running this playbook will upgrade MAS to the next release. If you run this playbook when you are already on the latest release then it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_upgrade_dryrun: False roles: - ibm.mas_devops.suite_upgrade_check Explicit Upgrade Target \uf0c1 Running this playbook will attempt to upgrade MAS to the specified release. If the specified release cannot be upgraded to from the installed version of MAS then no action will be taken. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_channel: 8.8.x mas_upgrade_dryrun: False roles: - ibm.mas_devops.suite_upgrade_check","title":"suite_upgrade"},{"location":"roles/suite_upgrade/#suite_upgrade","text":"This role validates if a given MAS installation is ready for the core platform to be upgraded to a specific subscription channel, and (as long as dry run mode is not enabled) will execute the upgrade. It will validate that the current subscription channel is able to be upgraded to the target channel. It will validate that all installed applications have already been upgraded to versions compatible with the new version of the Core Platform. It will upgrade the MAS core platform to the desired channel (as long as dry run is not enabled). It will validate that the core platform has been successfully reconciled at the upgraded version. It will not validate that all core services successfully deploy after the reconcile (but we will be working on this limitation).","title":"suite_upgrade"},{"location":"roles/suite_upgrade/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_upgrade/#mas_instance_id","text":"The ID of the MAS instance to upgrade. Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/suite_upgrade/#mas_channel","text":"The name of the MAS subscription channel that you want to upgrade to, if not provided the correct version to upgrade to will be automatically selected based on the current version of MAS installed. Optional Environment Variable: MAS_CHANNEL Default: None","title":"mas_channel"},{"location":"roles/suite_upgrade/#mas_upgrade_dryrun","text":"When set to true will ensure that the role only preforms upgrade validation checks and does not make any changes to the target installation. Optional Environment Variable: MAS_UPGRADE_DRYRUN Default: False","title":"mas_upgrade_dryrun"},{"location":"roles/suite_upgrade/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_upgrade/#automatic-target-selection","text":"Running this playbook will upgrade MAS to the next release. If you run this playbook when you are already on the latest release then it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_upgrade_dryrun: False roles: - ibm.mas_devops.suite_upgrade_check","title":"Automatic Target Selection"},{"location":"roles/suite_upgrade/#explicit-upgrade-target","text":"Running this playbook will attempt to upgrade MAS to the specified release. If the specified release cannot be upgraded to from the installed version of MAS then no action will be taken. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_channel: 8.8.x mas_upgrade_dryrun: False roles: - ibm.mas_devops.suite_upgrade_check","title":"Explicit Upgrade Target"},{"location":"roles/suite_verify/","text":"suite_verify \uf0c1 Verify a MAS installation is ready to use. This role will also print out the Admin Dashboard URL and the username and password of the superuser. If you want to disable these credentials being written to the output set the mas_hide_superuser_credentials to True . Role Variables \uf0c1 mas_instance_id \uf0c1 Required. The instance ID of the Maximo Application Suite installation to verify. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_hide_superuser_credentials \uf0c1 Set this to True if you want to disable the display of the superuser credentials as part of the verify. When this is enabled the debug will only identify the name of the secret containing the credentials rather than displaying the actual values. Environment Variable: MAS_HIDE_SUPERUSER_CREDENTIALS Default Value: False Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_hide_superuser_credentials: True roles: - ibm.mas_devops.suite_verify License \uf0c1 EPL-2.0","title":"suite_verify"},{"location":"roles/suite_verify/#suite_verify","text":"Verify a MAS installation is ready to use. This role will also print out the Admin Dashboard URL and the username and password of the superuser. If you want to disable these credentials being written to the output set the mas_hide_superuser_credentials to True .","title":"suite_verify"},{"location":"roles/suite_verify/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_verify/#mas_instance_id","text":"Required. The instance ID of the Maximo Application Suite installation to verify. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_verify/#mas_hide_superuser_credentials","text":"Set this to True if you want to disable the display of the superuser credentials as part of the verify. When this is enabled the debug will only identify the name of the secret containing the credentials rather than displaying the actual values. Environment Variable: MAS_HIDE_SUPERUSER_CREDENTIALS Default Value: False","title":"mas_hide_superuser_credentials"},{"location":"roles/suite_verify/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_hide_superuser_credentials: True roles: - ibm.mas_devops.suite_verify","title":"Example Playbook"},{"location":"roles/suite_verify/#license","text":"EPL-2.0","title":"License"},{"location":"roles/uds/","text":"uds \uf0c1 Installs IBM User Data Services as part of IBM Foundational Services in the ibm-common-services namespace. If mas_instance_id and the others associated parameters are provided then the role will also generate a configuration file that can be directly applied to IBM Maximo Application Suite. Role Variables - Installation \uf0c1 cluster_name \uf0c1 Required only for ROSA cluster. This variable is required to extract the UDS certificates. For other clusters this variable is not used. Environment Variable: CLUSTER_NAME Default Value: None uds_storage_class \uf0c1 Required. Storage class where UDS will be installed. On IBM Cloud RedHat Openshift Kubernetes Service (ROKS) ibmc-block-bronze is the recommended value. Environment Variable: UDS_STORAGE_CLASS Default Value: None uds_event_scheduler_frequency \uf0c1 Defines the frequency that BAS will collect event data. The value can be set following a cron tab format. Environment Variable: UDS_EVENT_SCHEDULER_FREQUENCY Default Value: @daily cluster ingres tls secret name \uf0c1 Specify the name of the cluster's ingres tls secret which contains the default router certificate. Optional Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default Value: router-certs-default Role Variables - BASCfg Generation \uf0c1 mas_instance_id \uf0c1 The instance ID of Maximo Application Suite that the BasCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a BasCfg template. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \uf0c1 Local directory to save the generated BasCfg resource definition. This can be used to manually configure a MAS instance to connect to BAS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a BasCfg template. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None mas_segment_key \uf0c1 Override the biult-in segment key used by MAS when communicating with User Data Services. This variable is only used for the generation of the BASCfg template, and in 99% of use cases you will not need to set this. Optional Environment Variable: MAS_SEGMENT_KEY Default Value: None uds_contact.email \uf0c1 Sets the Contact e-mail address used by the MAS instance's UDS configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: UDS_CONTACT_EMAIL Default Value: None uds_contact.first_name \uf0c1 Sets the Contact first name used by the MAS instance's UDS configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: UDS_CONTACT_FIRSTNAME Default Value: None uds_contact.last_name \uf0c1 Sets the Contact last name used by the MAS instance's UDS configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: UDS_CONTACT_LASTNAME Default Value: None uds_endpoint_url \uf0c1 Sets the UDS endpoint url used by the MAS instance's UDS configuration. Optional, used to instruct the role to set up MAS for an existing UDS instance. Environment Variable: UDS_ENDPOINT_URL Default Value: None uds_tls_crt \uf0c1 Sets the UDS TLS CA or Server Certificate used by the MAS instance's UDS configuration. Optional, used to instruct the role to set up MAS for an existing UDS instance. Environment Variable: UDS_TLS_CERT Default Value: None uds_tls_crt_local_file_path \uf0c1 The path on the local system to a file containing the TLS CA certiticate of the AnalyticsProxy to be used when the Maximo Application Suite is registered with UDS. This variable is only used if uds_tls_crt has not been set. Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: UDS_TLS_CERT_LOCAL_FILE Default Value: None uds_api_key \uf0c1 Sets the UDS api key used by the MAS instance's UDS configuration. Optional, used to instruct the role to set up MAS for an existing UDS instance. Environment Variable: UDS_API_KEY Default Value: None Example Playbook \uf0c1 Install in-cluster and generate MAS configuration \uf0c1 - hosts: localhost any_errors_fatal: true vars: uds_storage_class: ibmc-block-bronze mas_instance_id: masinst1 mas_config_dir: ~/masconfig uds_contact: email: 'john@email.com' first_name: 'john' last_name: 'winter' roles: - ibm.mas_devops.uds Generate MAS configuration for existing installation \uf0c1 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig uds_endpoint_url: \"https://xxx\" uds_api_key: \"xxx\" uds_tls_crt_local_file_path: \"/path/to/uds.crt\" uds_contact: email: 'john@email.com' first_name: 'john' last_name: 'winter' roles: - ibm.mas_devops.uds License \uf0c1 EPL-2.0","title":"uds"},{"location":"roles/uds/#uds","text":"Installs IBM User Data Services as part of IBM Foundational Services in the ibm-common-services namespace. If mas_instance_id and the others associated parameters are provided then the role will also generate a configuration file that can be directly applied to IBM Maximo Application Suite.","title":"uds"},{"location":"roles/uds/#role-variables-installation","text":"","title":"Role Variables - Installation"},{"location":"roles/uds/#cluster_name","text":"Required only for ROSA cluster. This variable is required to extract the UDS certificates. For other clusters this variable is not used. Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/uds/#uds_storage_class","text":"Required. Storage class where UDS will be installed. On IBM Cloud RedHat Openshift Kubernetes Service (ROKS) ibmc-block-bronze is the recommended value. Environment Variable: UDS_STORAGE_CLASS Default Value: None","title":"uds_storage_class"},{"location":"roles/uds/#uds_event_scheduler_frequency","text":"Defines the frequency that BAS will collect event data. The value can be set following a cron tab format. Environment Variable: UDS_EVENT_SCHEDULER_FREQUENCY Default Value: @daily","title":"uds_event_scheduler_frequency"},{"location":"roles/uds/#cluster-ingres-tls-secret-name","text":"Specify the name of the cluster's ingres tls secret which contains the default router certificate. Optional Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default Value: router-certs-default","title":"cluster ingres tls secret name"},{"location":"roles/uds/#role-variables-bascfg-generation","text":"","title":"Role Variables - BASCfg Generation"},{"location":"roles/uds/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the BasCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a BasCfg template. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/uds/#mas_config_dir","text":"Local directory to save the generated BasCfg resource definition. This can be used to manually configure a MAS instance to connect to BAS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a BasCfg template. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/uds/#mas_segment_key","text":"Override the biult-in segment key used by MAS when communicating with User Data Services. This variable is only used for the generation of the BASCfg template, and in 99% of use cases you will not need to set this. Optional Environment Variable: MAS_SEGMENT_KEY Default Value: None","title":"mas_segment_key"},{"location":"roles/uds/#uds_contactemail","text":"Sets the Contact e-mail address used by the MAS instance's UDS configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: UDS_CONTACT_EMAIL Default Value: None","title":"uds_contact.email"},{"location":"roles/uds/#uds_contactfirst_name","text":"Sets the Contact first name used by the MAS instance's UDS configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: UDS_CONTACT_FIRSTNAME Default Value: None","title":"uds_contact.first_name"},{"location":"roles/uds/#uds_contactlast_name","text":"Sets the Contact last name used by the MAS instance's UDS configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: UDS_CONTACT_LASTNAME Default Value: None","title":"uds_contact.last_name"},{"location":"roles/uds/#uds_endpoint_url","text":"Sets the UDS endpoint url used by the MAS instance's UDS configuration. Optional, used to instruct the role to set up MAS for an existing UDS instance. Environment Variable: UDS_ENDPOINT_URL Default Value: None","title":"uds_endpoint_url"},{"location":"roles/uds/#uds_tls_crt","text":"Sets the UDS TLS CA or Server Certificate used by the MAS instance's UDS configuration. Optional, used to instruct the role to set up MAS for an existing UDS instance. Environment Variable: UDS_TLS_CERT Default Value: None","title":"uds_tls_crt"},{"location":"roles/uds/#uds_tls_crt_local_file_path","text":"The path on the local system to a file containing the TLS CA certiticate of the AnalyticsProxy to be used when the Maximo Application Suite is registered with UDS. This variable is only used if uds_tls_crt has not been set. Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: UDS_TLS_CERT_LOCAL_FILE Default Value: None","title":"uds_tls_crt_local_file_path"},{"location":"roles/uds/#uds_api_key","text":"Sets the UDS api key used by the MAS instance's UDS configuration. Optional, used to instruct the role to set up MAS for an existing UDS instance. Environment Variable: UDS_API_KEY Default Value: None","title":"uds_api_key"},{"location":"roles/uds/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/uds/#install-in-cluster-and-generate-mas-configuration","text":"- hosts: localhost any_errors_fatal: true vars: uds_storage_class: ibmc-block-bronze mas_instance_id: masinst1 mas_config_dir: ~/masconfig uds_contact: email: 'john@email.com' first_name: 'john' last_name: 'winter' roles: - ibm.mas_devops.uds","title":"Install in-cluster and generate MAS configuration"},{"location":"roles/uds/#generate-mas-configuration-for-existing-installation","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig uds_endpoint_url: \"https://xxx\" uds_api_key: \"xxx\" uds_tls_crt_local_file_path: \"/path/to/uds.crt\" uds_contact: email: 'john@email.com' first_name: 'john' last_name: 'winter' roles: - ibm.mas_devops.uds","title":"Generate MAS configuration for existing installation"},{"location":"roles/uds/#license","text":"EPL-2.0","title":"License"}]}